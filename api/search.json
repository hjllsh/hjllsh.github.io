[{"id":"11fe232dbef934e03d9537a3a12675e0","title":"MySQL实战","content":"不建议使用长事务假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录\n\n当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-view A，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。\n即使现在有另外一个事务正在将4改成5，这个事务跟read-view A、B、C对应的事务是不会冲突的。\n回滚日志总不能一直保留吧，什么时候删除呢？\n答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的read-view的时候。\n长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。\n事务的启动方式长事务有这些潜在风险，当然是建议尽量避免。其实很多时候并不是有意使用长事务，通常是由于误用所致。MySQL的事务启动方式有以下几种：\n\n显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。\nset autocommit&#x3D;0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。\n\n有些客户端连接框架会默认连接成功后先执行一个set autocommit&#x3D;0的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。\n因此，建议总是使用set autocommit&#x3D;1, 通过显式语句的方式来启动事务。\n对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你有这个顾虑，我建议你使用commit work and chain语法。在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。\n可以在information_schema库的innodb_trx这个表中查询长事务\n比如下面这个语句，用于查找持续时间超过60s的事务。\nselect * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60\n\n索引索引的常见模型索引的出现是为了提高查询效率，但是实现索引的方式却有很多种。常见、也比较简单的数据结构索引，它们分别是哈希表、有序数组和搜索树\n哈希表是一种以键-值（key-value）存储数据的结构，不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况，处理方法是，拉出一个链表。哈希表这种结构适用于只有等值查询的场景。\n有序数组，在等值查询和范围查询场景中的性能就都非常优秀，用二分法就可以快速得到，这个时间复杂度是O(log(N))。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。有序数组索引只适用于静态存储引擎，\n二叉搜索树，每个节点的左儿子小于父节点，父节点又小于右儿子，时间复杂度是O(log(N))，为了维持O(log(N))的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是O(log(N))。\nInnoDB 的索引模型在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。\n基于主键索引和普通索引的查询有什么区别？\n\n如果语句是select * from T where ID&#x3D;500，即主键查询方式，则只需要搜索ID这棵B+树；\n如果语句是select * from T where k&#x3D;5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为回表。\n\n也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n索引维护\nB+树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行ID值为700，则只需要在R5的记录后面插入一个新记录。如果新插入的ID值为400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。\n而更糟的情况是，如果R5所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。\n除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约50%。\n当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。\n分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。\n自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的：NOT NULL PRIMARY KEY AUTO_INCREMENT。\n插入新记录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。\n而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。\n除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。\n显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。\n适合用业务字段直接做主键的场景：典型的KV场景\n\n只有一个索引；\n该索引必须是唯一索引。\n\n由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题\n有没有可能经过索引优化，避免回表过程呢？覆盖索引如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。\n在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？\n如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，身份证号、姓名的联合索引可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。\n最左前缀原则要按照市民的身份证号去查他的家庭地址，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？\nB+树这种索引结构，可以利用索引的“最左前缀”，来定位记录\n\n索引项是按照索引定义里面出现的字段顺序排序的\n当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到ID4，然后向后遍历得到所有需要的结果。\n如果你要查的是所有名字第一个字是“张”的人，你的SQL语句的条件是”where name like ‘张%’”。这时，你也能够用上这个索引，查找到第一个符合条件的记录是ID3，然后向后遍历，直到不满足条件为止。\n最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。\n在建立联合索引的时候，如何安排索引内的字段顺序？\n第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。\n如果既有联合查询，又有基于a、b各自的查询呢？查询条件里面只有b的语句，是无法使用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护(a,b)、(b) 这两个索引。这时候，我们要考虑的原则就是空间了。比如name字段是比age字段大的，建议创建一个（name,age)的联合索引和一个(age)的单字段索引。\n索引下推以联合索引（name, age）为例，检索出表中“名字第一个字是张，而且年龄是10岁的所有男孩\nmysql> select * from tuser where name like '张%' and age=10 and ismale=1;\n\n这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录ID3，然后判断其他条件是否满足\n在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。\n而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。\n\n在(name,age)索引里面没有age的where条件，这个过程InnoDB并不会去看age的值，只是按顺序把“name第一个字是’张’”的记录一条条取出来回表。因此，需要回表4次。\n\nInnoDB在(name,age)索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过。只需要对ID4、ID5这两条记录回表取数据判断，就只需要回表2次。\n主键索引也是可以使用多个字段的CREATE TABLE `geek` (\n  `a` int(11) NOT NULL,\n  `b` int(11) NOT NULL,\n  `c` int(11) NOT NULL,\n  `d` int(11) NOT NULL,\n  PRIMARY KEY (`a`,`b`),\n  KEY `c` (`c`),\n  KEY `ca` (`c`,`a`),\n  KEY `cb` (`c`,`b`)\n) ENGINE=InnoDB;\n\n主键包含了a、b这两个字段，那意味着单独在字段c上创建一个索引，就已经包含了三个字段了。为什么要创建“ca”，“cb”这两个索引？\n看这样的两种语句：\nselect * from geek where c= N order by a limit 1;\nselect * from geek where c= N order by b limit 1;\n\n表记录–a–|–b–|–c–|–d–1 2 3 d1 3 2 d1 4 3 d2 1 3 d2 2 2 d2 3 4 d主键 a，b的聚簇索引组织顺序相当于 order by a,b ，也就是先按a排序，再按b排序，c无序。\n索引 ca 的组织是先按c排序，再按a排序，同时记录主键–c–|–a–|–主键部分b– （注意，这里不是ab，而是只有b）2 1 32 2 23 1 23 1 43 2 14 2 3这个跟索引c的数据是一模一样的。\n索引 cb 的组织是先按c排序，在按b排序，同时记录主键–c–|–b–|–主键部分a– （同上）2 2 22 3 13 1 23 2 13 4 14 3 2\n所以，结论是ca可以去掉，cb需要保留。\n","slug":"MySQL-实战(1)","date":"2023-06-14T07:25:58.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"9f1c39845db12e0e40a2bf8f9721b61a","title":"Mybatis(一)","content":"MybatisMyBatis 是一款优秀的持久层框架，一个半 ORM（对象关系映射）框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几 乎所有的 JDBC 代码和手动设置参数以及 获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生类型、接口和 Java 的 POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。\n\n优点 \n\n与传统的数据库访问技术相比，ORM有以下优点： 基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL 写在XML里，解除sql与程序代码的耦合，便于统 一管理；提供XML标签，支持编写动态 SQL语句，并可重用 \n与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数 据库MyBatis都支持， 能够与Spring很好的集成\n\n\n缺点 \n\nSQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求 SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库\n\n\n\nORM是什么?ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数 据与简单Java对象（POJO）的映射关系的技术。简 单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中。\n为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？全自动：Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时， 可以根据对象关系模型直接获取，所以它是全自动的。\n而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半 自动ORM映射工具。\n传统JDBC开发存在哪些问题？MyBatis是如何解决这些问题的？\n频繁创建数据库连接对象、释放，容易造成系统资源浪费，影响系统性能。可以使用连接池 解决这个问题。但是使用jdbc需要自己实现连接池。 解决：在mybatis-config.xml中配置数据链接池，使用连接池管理数据库连接。\nsql语句定义、参数设置、结果集处理存在硬编码。实际项目中sql语句变化的可能性较大， 一旦发生变化，需要修改java代码，系统需要重 新编译，重新发布，不好维护。解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离，动态sql灵活。\n使用preparedStatement向占有位符号传参数存在硬编码，因为sql语句的where条件不一 定，可能多也可能少，修改sql还要修改代码，系统不易维护。 解决： Mybatis自动将java对象映射至sql语句。\n结果集处理存在重复代码，处理麻烦。如果可以映射成Java对象会比较方便。解决：Mybatis自动将sql执行结果映射至java对象。\n\nHibernate 和 MyBatis 的区别\n相同点\n\n都是对jdbc的封装，都是持久层的框架，都用于dao层的开发。\n\n\n不同点\n\n映射关系 \nMyBatis 是一个半自动映射的框架，配置Java对象与sql语句执行结果的对应关系，多表关联关系配置简单\nHibernate 是一个全表映射的框架，配置Java对象与数据库表的对应关系，多表关联关系配置复杂\n\n\n\n\nSQL优化和移植性 \n\nHibernate 对SQL语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性， 此外还提供 HQL（Hibernate Query Language） 操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但SQL语句优化困难。\nMyBatis 需要手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。 开发工作量相对大些。直接使用SQL语句操作数据库，不支持数据库无关性，但sql语句优化容易。\n\n\n开发难易程度和学习成本\n\nHibernate是重量级框架,学习使用门槛高,适合于需求相对稳定,中小型的项目,比如:办公自动化系统 \nMyBatis 是轻量级框架,学习使用门槛低,适合于需求变化频繁,大型的项目,\n\n\n\n总结: MyBatis 是一个小巧、方便、高效、简单、直接、半自动化的持久层框架， Hibernate 是一个强大、方便、高效、复杂、间接、全自动化的持久层框架。\nMyBatis的解析和运行原理MyBatis编程步骤:\n\n创建SqlSessionFactory \n通过SqlSessionFactory创建SqlSession\n通过sqlsession执行数据库操作 \n调用session.commit()提交事务 \n调用session.close()关闭会话\n\nMyBatis的工作原理\n1）读取 MyBatis 配置文件：mybatis-config.xml 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息。\n2）加载映射文件：映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句， 需要在MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表。\n 3）构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory。 \n4）创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所 有方法。 \n5）Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。 \n6）MappedStatement 对象：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于 存储要映射的 SQL 语句的 id、参数等信 息。\n 7）输入参数映射：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过程。 \n8）输出结果映射：输出结果类型可以是 Map、List 等集合类型，也可以是基本数据类型 和 POJO 类型。输出结果映射过程类似于 JDBC 对 结果集的解析过程。\nMyBatis的功能架构API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层 一接收到调用请求就会调用数据处理层来完成具 体的数据处理。 \n数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的 目的是根据调用的请求完成一次数据库操作。 \n基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这 些都是共用的东西，将他们抽取出来作为最基础 的组件。为上层的数据处理层提供最基础的 支撑。\nMyBatis的初始化，会从mybatis-config.xml配置文件，解析构造成 Configuration这个类，\n(1)加载配置：配置来源于两个地方，一处是配置文件，一处是Java代码的注解，将SQL的 配置信息加载成为一个个MappedStatement对象 （包括了传入参数映射配置、执行的SQL 语句、结果映射配置），存储在内存中。 \n(2)SQL解析：当API接口层接收到调用请求时，会接收到传入SQL的ID和传入对象（可以是 Map、JavaBean或者基本数据类型），Mybatis 会根据SQL的ID找到对应的 MappedStatement，然后根据传入参数对象对MappedStatement进行解析，解析后可以 得到最终要执行的 SQL语句和参数。\n (3)SQL执行：将最终得到的SQL和参数拿到数据库进行执行，得到操作数据库的结果。\n (4)结果映射：将操作数据库的结果按照映射的配置进行转换，可以转换成HashMap、 JavaBean或者基本数据类型，并将最终结果返回。\n","slug":"Mybatis(1)","date":"2023-06-14T02:03:59.000Z","categories_index":"","tags_index":"Mybatis","author_index":"大宝贝的程序员"},{"id":"1442087dfdabdc07946399fb7094e2fa","title":"MySQL日志系统","content":"MySQL的基本架构比如，你有个最简单的表，表里只有一个ID字段，在执行下面这个查询语句时：\nmysql> select * from T where ID=10；\n\nMySQL内部的执行过程MySQL的基本架构示意图\n\n大体来说，MySQL可以分为Server层和存储引擎层两部分。\nServer层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。不同的存储引擎共用一个Server层。\n连接器第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：\nmysql -h$ip -P$port -u$user -p\n\n连接命令中的mysql是客户端工具，用来跟服务端建立连接。在完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。\n\n如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。\n如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。\n\n这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。\n但是全部使用长连接后，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。\n考虑以下两种方案\n\n定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。\n如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\n\n查询缓存连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步：查询缓存。\nMySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。\n但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。\n查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。\nMySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定\nmysql> select SQL_CACHE * from T where ID=10；\n\nMySQL 8.0版本直接将查询缓存的整块功能删掉了。\n分析器如果没有命中查询缓存，就开始执行语句。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。\nMySQL从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。\n做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。\n如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒\n优化器经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。\n优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的join：\nmysql> select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;\n\n\n既可以先从表t1里面取出c&#x3D;10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。\n也可以先从表t2里面取出d&#x3D;20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。\n\n这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。\n执行器开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误。\nmysql> select * from T where ID=10;\n\nERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T'\n\n如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。\n这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的：\n\n调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；\n调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。\n执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。\n\n至此，这个语句就执行完成了。\n对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。\n你会在数据库的慢查询日志中看到一个rows_examined的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟rows_examined并不是完全相同的。\n日志系统MySQL可以恢复到半个月内任意一秒的状态，这是怎么做到的？\n我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键ID和一个整型字段c：\nmysql> create table T(ID int primary key, c int);\n\n如果要将ID&#x3D;2这一行的值加1，SQL语句就会这么写：\nmysql> update T set c=c+1 where ID=2;\n\nSQL语句基本的执行链路，更新语句也是同样会走一遍\n与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）。\n重要的日志模块：redo log在MySQL里有这个问题，如果每一次的更新操作都需要写进磁盘，磁盘要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。MySQL里使用了WAL技术来解决，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。\n具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。\n如果更新操作很频繁，redo log日志是会写满的，InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。\n\nwrite pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。\nwrite pos和checkpoint之间的是空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。\n重要的日志模块：binlogMySQL整体来看，其实就有两块：一块是Server层，它主要做的是MySQL功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。\n为什么会有两份日志呢？\n因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统也就是redo log来实现crash-safe能力。\n这两种日志有以下三点不同。\n\nredo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。\nredo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID&#x3D;2这一行的c字段加1 ”。\nredo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n\n再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程。\n\n执行器先找引擎取ID&#x3D;2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID&#x3D;2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。\n执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。\n引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。\n执行器生成这个操作的binlog，并把binlog写入磁盘。\n执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。\n\n这里我给出这个update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。\n\n将redo log的写入拆成了两个步骤：prepare和commit，这就是”两阶段提交”。这是为了让两份日志之间的逻辑一致。\n两阶段提交为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。\n假设当前ID&#x3D;2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？\n先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。\n先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同。\n可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。\n小结：\nredo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数我建议你设置成1，这样可以保证MySQL异常重启之后数据不丢失。\nsync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数我也建议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。\n","slug":"MySQL日志系统","date":"2023-06-11T04:33:43.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"32a652994e7e566383677d79f35d7c59","title":"Lambda表达式","content":"函数式编程思想函数式编程思想类似于我们数学中的函数，它主要关注的是对数据进行了什么操作。\n优点：\n\n代码简洁，开发快速\n接近自然语言，易于理解\n易于“并发编程”\n\nLambda表达式Lambda是JDK8中一个语法糖，它可以对某些匿名内部类的写法进行简化。它是函数式编程思想的一个重要体现，让我们不用关注是什么对象，而是关注我们对数据进行了什么操作。\n基本格式：（参数列表）-&gt; {代码}\n例如：我们在创建线程并启动时可以使用匿名内部类的写法\nnew Thread(new Runnable() &#123;\n    @Override\n    public void run() &#123;\n        System.out.println(\"线程中run方法被执行\");\n    &#125;\n&#125;).start();\n\nLambda表达式只关注有什么参数\nnew Thread(() -> &#123;\n        System.out.println(\"线程中run方法被执行\");\n&#125;).start();\n\n\n匿名内部类 优化成Lambda表达式\npublic static void main(String[] args) &#123;\n    int sum = calculateNum(new IntBinaryOperator() &#123;\n        @Override\n        public int applyAsInt(int left, int right) &#123;\n            return left + right;\n        &#125;\n    &#125;);\n    System.out.println(sum);\n&#125;\n\n// IntBinaryOperator为一个接口\npublic static int calculateNum(IntBinaryOperator operator) &#123;\n    int a = 10;\n    int b = 20;\n    return operator.applyAsInt(a, b);\n&#125;\n\n\n优化后：\npublic static void main(String[] args) &#123;\n    int sum = calculateNum((int left, int right) -> &#123;\n        return left + right;\n    &#125;);\n    System.out.println(sum);\n&#125;\n//IntBinaryOperator为一个接口\npublic static int calculateNum(IntBinaryOperator operator) &#123;\n    int a = 10;\n    int b = 20;\n    return operator.applyAsInt(a, b);\n&#125;\n\n\n带返回值的函数方法\npublic static void main(String[] args) &#123;\n    String str = typeConver(new Function&lt;String, String>() &#123;\n        public String apply(String s) &#123;\n            return s + \" hxp\";\n        &#125;\n    &#125;);\n    System.out.println(str);\n&#125;\n\npublic static &lt;R> R typeConver(Function&lt;String,R> function) &#123;\n    String str = \"hello\";\n    R result = function.apply(str);\n    return result;\n&#125;\n\n\n优化\npublic static void main(String[] args) &#123;\n    String str = typeConver((String s) -> &#123;\n        return s + \" hxp\";\n    &#125;);\n    System.out.println(str);\n&#125;\n\npublic static &lt;R> R typeConver(Function&lt;String,R> function) &#123;\n    String str = \"hello\";\n    R result = function.apply(str);\n    return result;\n&#125;\n\n\n再省略\npublic static void main(String[] args) &#123;\n    String str = typeConver(s -> s + \" hxp\");\n    System.out.println(str);\n&#125;\n\n使用Lambda表达式替换匿名内部类，只需要保留匿名内部类的方法参数和方法体，然后在中间加一个箭头即可\n省略规则\n\n参数类型可以省略\n方法体只有一句代码时大括号、return和唯一一句代码的分号可以省略\n方法只有一个参数时，小括号可以省略\n以上规则都可以省略不记，使用idea的快捷键 alt+enter\n\nStream流Java8的Stream使用的是函数式编程模式，如同它的名字一样，它可以被用来对集合或数组进行链状流式的操作，可以更方便的让我们对集合或数组操作。\n案例准备：\n@Data\n@NoArgsConstructor\n@AllArgsConstructor\n@EqualsAndHashCode  // 去重\npublic class Author &#123;\n    private Long id;\n    private String name;\n    private Integer age;\n    private String intro;\n    private List&lt;Book> books;\n&#125;\n\n\n@Data\n@AllArgsConstructor\n@NoArgsConstructor\n@EqualsAndHashCode\npublic class Book &#123;\n    private Long id;\n    private String name;\n    private String category;\n    private Integer score;\n    private String intro;\n&#125;\n\npublic class StreamDemo &#123;\n\n    private static List&lt;Author> getAuthors() &#123;\n        Author author = new Author(1L, \"蒙多\", 17, \"一个祖安人\", null);\n        Author author2 = new Author(2L, \"亚拉索\", 18, \"艾欧尼亚\", null);\n        Author author3 = new Author(3L, \"易大师\", 19, \"黑色玫瑰\", null);\n        Author author4 = new Author(3L, \"易大师\", 19, \"黑色玫瑰\", null);\n\n        List&lt;Book> book1 = new ArrayList&lt;>();\n        List&lt;Book> book2 = new ArrayList&lt;>();\n        List&lt;Book> book3 = new ArrayList&lt;>();\n        List&lt;Book> book4 = new ArrayList&lt;>();\n\n        book1.add(new Book(1L,\"*\",\"哲学,爱情\", 80, \"*\"));\n        book1.add(new Book(2L,\"**\",\"爱情,个人成长\", 80, \"**\"));\n\n        book2.add(new Book(3L,\"***\",\"爱情,传记\", 70, \"***\"));\n        book2.add(new Book(3L,\"****\",\"个人成长,传记\", 70, \"****\"));\n        book2.add(new Book(4L,\"*****\",\"哲学\", 70, \"*****\"));\n\n        book3.add(new Book(5L,\"******\",\"个人成长\", 60, \"******\"));\n        book3.add(new Book(6L,\"*******\",\"传记\", 60, \"*******\"));\n        book3.add(new Book(6L,\"********\",\"爱情\", 60, \"********\"));\n\n        book4.add(new Book(5L,\"******\",\"个人成长\", 60, \"******\"));\n        book4.add(new Book(6L,\"*******\",\"个人成长,传记,爱情\", 60, \"*******\"));\n        book4.add(new Book(6L,\"********\",\"哲学,爱情,个人成长\", 60, \"********\"));\n\n\n        author.setBooks(book1);\n        author2.setBooks(book2);\n        author3.setBooks(book3);\n        author4.setBooks(book4);\n\n        List&lt;Author> authors = new ArrayList&lt;>(Arrays.asList(author,author2,author3,author4));\n        return authors;\n    &#125;\n&#125;\n\n\n案例：获取到作家的集合，打印出年龄小于18的作家的名字\npublic static void main(String[] args) &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()    //将集合转换成流\n            .distinct() //去重\n            .filter(new Predicate&lt;Author>() &#123;\n                @Override\n                public boolean test(Author author) &#123;\n                    return author.getAge() &lt; 18;    //获取年龄小于18的对象\n                &#125;\n            &#125;)\n            .forEach(new Consumer&lt;Author>() &#123;\n                @Override\n                public void accept(Author author) &#123;\n                    System.out.println(author.getName());   //打印对象的名字\n                &#125;\n            &#125;);\n&#125;\n\n\n\n常用操作创建流单例集合：集合对象.stream()\n数组：Arrays.stream(数组) 或 Stream.of(数组) 来创建\n双例集合：转换成单例集合后再创建\nMap&lt;String, String> map = new HashMap&lt;>();\nStream&lt;Map.Entry&lt;String, String>> stream = map.entrySet().stream();\n\n中间操作1、filter对流中的元素进行条件过滤，符合过滤条件的才能继续留在流中。\npublic static void test() &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()\n            .filter(author -> author.getAge() > 18) //中间操作\n            .forEach(author -> System.out.println(author)); //终结操作\n&#125;\n\n2、map可以把流中的元素进行计算或转换。\n转换:\npublic static void test() &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()\n            .map(new Function&lt;Author, String>() &#123;\n                //泛型中，第一个参数为方法的参数类型（流中的类型），第二个参数为方法的返回值类型\n                @Override\n                public String apply(Author author) &#123;\n                    return author.getName();\n                &#125;\n            &#125;)\n            .forEach(new Consumer&lt;String>() &#123;\n                @Override\n                public void accept(String name) &#123;\n                    System.out.println(name);\n                &#125;\n            &#125;);\n&#125;\n\n简化\npublic static void test() &#123;\n    List&lt;Author> authors = getAuthors();\n    //泛型中，第一个参数为方法的参数类型，第二个参数为方法的返回值类型\n    authors.stream()\n            .map(author -> author.getName())\n            .forEach(name -> System.out.println(name));\n&#125;\n\n\n计算：\npublic static void test() &#123;\n    List&lt;Author> authors = getAuthors();\n    //泛型中，第一个参数为方法的参数类型，第二个参数为方法的返回值类型\n    authors.stream()\n            .map(author -> author.getAge())\n            .map(age -> age+10)\n            .forEach(age -> System.out.println(age));\n&#125;\n\n\n\n3、distinct去除流中的重复元素。\n注意：distinct方法是依赖Object的equals方法来判断是否是相同对象，所以需要重写equals方法。\n4、sorted对流中的元素进行排序。\n方式一：调用sorted()空参方法在比较的实体类上要实现Comparable接口，实现Comparator接口，不然会报类型不匹配的异常。\n\npublic static void test2() &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()\n            .distinct()\n            .sorted()\n            .forEach(author -> System.out.println(author.getAge()));\n&#125;\n\n方式二：在sorted()方法中实现Comparator接口\npublic static void test3() &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()\n            .distinct()\n            .sorted(new Comparator&lt;Author>() &#123;\n                @Override\n                public int compare(Author o1, Author o2) &#123;\n                    return o1.getAge()-o2.getAge();\n                &#125;\n            &#125;)\n            .forEach(author -> System.out.println(author.getAge()));\n&#125;\n\n简化后：\npublic static void test2() &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()\n            .distinct()\n            .sorted((o1, o2) -> o1.getAge()-o2.getAge())\n            .forEach(author -> System.out.println(author.getAge()));\n&#125;\n\n\n5、limit可以设置流的最大长度，超出的部分将被抛弃。\n//对流中的元素按照年龄进行降序排序，并且要求不能有重复元素，打印其中年龄最大的两个作家。\npublic static void test2() &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()\n            .distinct()\n            .sorted((o1, o2) -> o2.getAge()-o1.getAge())\n            .limit(2)\n            .forEach(author -> System.out.println(author.getName()));\n&#125;\n\n6、skip跳过流中的前n个元素，返回剩下的元素。\n7、flatMapmap只能把一个对象转换成另一个对象来作为流中的元素。而flatMap可以把一个对象转换成多个对象作为流中的元素。\n案例1：打印所有书籍的名字，要求对重复的元素进行去重。map方式：Author对象的books属性是集合类型，使用原来map转换对象，要使用嵌套循环进行打印。\npublic static void test2() &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()\n            .map(author -> author.getBooks())\n            .forEach(books -> &#123;\n                for (Book book : books) &#123;\n                    System.out.println(book.getName());\n                &#125;\n            &#125;);\n&#125;\n\nflatMap方式：\npublic static void test3() &#123;\n    List&lt;Author> authors = getAuthors();\n    authors.stream()\n            .flatMap(author -> author.getBooks().stream())\n            .distinct()\n            .flatMap(book -> Arrays.stream(book.getCategory().split(\",\")))\n            .distinct()\n            .forEach(category -> System.out.println(category));\n&#125;\n\n\n终结操作1、forEach对流中的元素进行遍历操作，我们通过传入的参数去指定对遍历到的元素进行什么具体操作。\n2、count获取当前流中元素的个数。\n//打印这些作家的所出书籍的数量\npublic static void test4() &#123;\n    List&lt;Author> authors = getAuthors();\n    long count = authors.stream()\n            .flatMap(author -> author.getBooks().stream())\n            .distinct()\n            .count();\n    System.out.println(count);\n&#125;\n\n3、max&amp;min获取流中的最值\n//分别获取这些作家所出书籍的最高分和最低分\npublic static void test4() &#123;\n    List&lt;Author> authors = getAuthors();\n    Optional&lt;Integer> max = authors.stream()\n            .flatMap(author -> author.getBooks().stream())\n            .map(book -> book.getScore())\n            .max((o1, o2) -> o1 - o2);\n    Optional&lt;Integer> min = authors.stream()\n            .flatMap(author -> author.getBooks().stream())\n            .map(book -> book.getScore())\n            .min(((o1, o2) -> o1 - o2));\n    System.out.println(max.get());\n    System.out.println(min.get());\n&#125;\n\n4、collect把当前流转换成一个集合。\nlist集合\n//获取一个存放所有作者名字的list集合\npublic static void test5() &#123;\n    List&lt;Author> authors = getAuthors();\n    List&lt;String> nameList = authors.stream()\n            .map(author -> author.getName())\n            .collect(Collectors.toList());\n    System.out.println(nameList);\n&#125;\n\nset集合\n//获取一个存放所有作者名字的set集合\npublic static void test5() &#123;\n    List&lt;Author> authors = getAuthors();\n    Set&lt;String> nameList = authors.stream()\n            .map(author -> author.getName())\n            .collect(Collectors.toSet());\n    System.out.println(nameList);\n&#125;\n\nmap集合\n//获取一个Map集合，map的key为作者名，value为List&lt;Book>\npublic static void test5() &#123;\n    List&lt;Author> authors = getAuthors();\n    Map&lt;String, List&lt;Book>> map = authors.stream()\n            .distinct()\n            .collect(Collectors.toMap(author -> author.getName(), author -> author.getBooks()));\n    System.out.println(map);\n&#125;\n\n\n\n\n\n\n\n\n\n\n要注意转换过程中集合类型的特殊性，比如set集合和map集合的key不允许相同\n5、查找与匹配\n（1）anyMatch：判断是否有任意符合匹配条件的元素，结果为boolean类型。\n//判断是否有作家年龄在18以上\npublic static void test6() &#123;\n    List&lt;Author> authors = getAuthors();\n    boolean flag = authors.stream()\n            .anyMatch(author -> author.getAge() > 18);\n    System.out.println(flag);\n&#125;\n\n\n（2）allMatch：判断是否都符合条件，如果都符合返回true，否则返回false\n//判断是否有作家年龄在18以上\npublic static void test6() &#123;\n    List&lt;Author> authors = getAuthors();\n    boolean flag = authors.stream()\n            .anyMatch(author -> author.getAge() > 18);\n    System.out.println(flag);\n&#125;\n\n\n（3）noneMatch：判断流中的元素是否都不符合匹配条件，如果都不符合结果为true，否则结果为false。\n（4）findAny：获取流中的任意一个元素。该方法没有办法保证获取到的一定是流中的第一个元素。\n//获取任意一个年龄大于18的作家，如果存在就输出他的名字\npublic static void test7() &#123;\n    List&lt;Author> authors = getAuthors();\n    Optional&lt;Author> any = authors.stream()\n            .filter(author -> author.getAge() > 18)\n            .findAny();\n    //如果这个Optional中有元素，则执行方法，没有就不执行\n    any.ifPresent(author -> System.out.println(author.getName()));\n&#125;\n\n\n（5）findFirst：获取流中的第一个元素。\n//获取一个年龄最小的作家，并输出他的姓名\npublic static void test7() &#123;\n    List&lt;Author> authors = getAuthors();\n    Optional&lt;Author> any = authors.stream()\n            .sorted((o1, o2) -> o1.getAge()-o2.getAge())\n            .findFirst();\n    any.ifPresent(author -> System.out.println(author.getName()));\n&#125;\n\n\n（6）reduce归并对流中的数据按照你指定的计算方式计算出一个结果。（缩减操作）\nreduce的作用是把stream中的元素给组合起来。我们可以传入一个初始值，它会按照我们的计算方式依次拿流中的元素和初始化值进行计算，计算结果再和后面的元素计算。\n它内部的计算方式如下：\nT result = identity;\nfor (T element : this stream)\n\tresult = accumulator.apply(result, element)\nreturn result;\n\n其中identity就是我们可以通过方法参数传入的初始值，accumulator的apply具体进行什么计算也是我们通过方法参数来确定的。案例1：使用reduce求所有作者年龄的和\npublic static void test8() &#123;\n    List&lt;Author> authors = getAuthors();\n    Integer sum = authors.stream()\n            .distinct()\n            .map(author -> author.getAge())\n            //初始值为0，后面 result+=element，最后 return result\n            .reduce(0, (result, element) -> result + element);\n    System.out.println(sum);\n&#125;\n\n案例2：使用reduce求所有作者中年龄的最大值\npublic static void test8() &#123;\n    List&lt;Author> authors = getAuthors();\n    Integer max = authors.stream()\n            .map(author -> author.getAge())\n            .reduce(Integer.MIN_VALUE, (result, element) -> result > element ? result : element);\n    System.out.println(max);\n&#125;\n\nreduce有个重载形式，内部代码如下：\nboolean foundAny = false;\nT result = null;\nfor (T element : this stream) &#123;\n\tif(!foundAny) &#123;\n\t\tfoundAny = true;\n\t\tresult = element;\n\t&#125; else &#123;\n\t\tresult = accumulator.apply(result, element);\n\t&#125;\n&#125;\nreturn foundAny ? Optional.of(result) : Optional.empty();\n\n利用这个重载形式，求作者年龄的最大值，不用传递初始值了。\npublic static void test8() &#123;\n    List&lt;Author> authors = getAuthors();\n    Optional&lt;Integer> max = authors.stream()\n            .map(author -> author.getAge())\n            .reduce((result, element) -> result > element ? result : element);\n    System.out.println(max.get());\n&#125;\n\n\n\n\n\n\n\n\n\n\n惰性求值：如果没有终结操作，中间操作是不会得到执行的。\n流的一次性：一旦一个流对象经过一个终结操作后，就不能再被使用了，只能重新创建流对象再使用。\n不会影响原数据：我们在流中可以对数据做很多处理，但正常情况下是不会影响原来集合中的元素的。\nOptional我们在编写代码的时候出现最多的就是空指针异常，所以在很多情况下我们需要做各种非空的判断。尤其是对象中的属性还是一个对象的情况下，这种判断会更多。过多的判断语句会让我们的代码显得臃肿，所以在JDK8中引入了Optional，养成使用Optional的习惯后，你可以写出更优雅的代码来避免空指针异常。\n创建对象Optional就好像是包装类，可以把我们的具体数据封装到Optional对象内部，然后我们去使用Optional中封装好的方法 操作封装进去的数据，就可以非常优雅的避免空指针异常。\n我们一般使用Optional的静态方法ofNullable来把数据封装成一个Optional对象，无论传入的参数是否为null都不会出现问题。\npublic static void main(String[] args) &#123;\n    Optional&lt;Author> optionalAuthor = getOptionalAuthors();\n    optionalAuthor.ifPresent(author -> System.out.println(author.getName()));\n&#125;\n\nprivate static Optional&lt;Author> getOptionalAuthors() &#123;\n    Author author = new Author(1L, \"蒙多\", 17, \"一个祖安人\", null);\n    return Optional.ofNullable(author);\n&#125;\n\n\n可以使用**Optional的静态方法of()**来把不为空的对象封装成Optional对象\nAuthor author = new Author();\nOptional&lt;Author> optionalAuthor = Optional.of(author);\n\n观察ofNullable()方法的源码也可以发现，它是调用了of()方法的，只是对对象做了非空的判断\n\n优雅使用方式\nprivate static Optional&lt;Author> getOptionalAuthor2() &#123;\n    Author author = new Author(1L, \"蒙多\", 17, \"一个祖安人\", null);\n    return author == null ? Optional.empty() : Optional.of(author);\n&#125;\n\n安全获取值如果我们期望安全获取值，不推荐使用get()方法获取，而是用如下方法：\norElseGet获取数据并且设置数据为空时的默认值。如果数据不为空则获取到该数据，如果为空则返回传入的参数来创建对象。\npublic static void main(String[] args) &#123;\n    Optional&lt;Author> optionalAuthor = getOptionalAuthors();\n    Author author = optionalAuthor.orElseGet(() -> new Author(2L, \"蒙多2\", 17, \"一个祖安人\", null));\n    System.out.println(author.getName());\n&#125;\n\nprivate static Optional&lt;Author> getOptionalAuthors() &#123;\n    Author author = new Author(1L, \"蒙多\", 17, \"一个祖安人\", null);\n    return Optional.ofNullable(author);\n&#125;\n\norElseThrow\n获取数据，如果数据不为空则获取数据，如果为空则根据传入的参数来创建异常抛出。\npublic static void main(String[] args) &#123;\n    Optional&lt;Author> optionalAuthor = getOptionalAuthors();\n    Author author = optionalAuthor.orElseThrow(() -> new RuntimeException(\"数据为null\"));\n    System.out.println(author);\n&#125;\n\n过滤我们可以使用filter方法对数据进行过滤。如果原本是有数据的，但不符合判断条件，也会变成一个无数据的Optional对象。\nprivate static void getOptionalAuthor3() &#123;\n    Optional&lt;Author> authorOptional = getOptionalAuthors();\n    authorOptional\n            .filter(author -> author.getAge() &lt; 18)\t//过滤\n            .ifPresent(author -> System.out.println(author.getName()));\t//消费\n&#125;\n\n判断我们可以使用isPresent方法进行判断是否存在数据的判断。如果为空，返回值为false，如果不为空，返回值为true。但这种方式不能体现Optional的好处，更推荐使用ifPresent方法。\npublic static void testIsPresent() &#123;\n    Optional&lt;Author> authorOptional = getOptionalAuthors();\n    if (authorOptional.isPresent()) &#123;\n        System.out.println(authorOptional.get().getName());\n    &#125;\n&#125;\n\n数据转换Optional还提供了map可以让我们的数据进行转换，并且转换得到的数据也还是Optional包装好的，保证了我们的使用安全。\npublic static void testMap() &#123;\n    Optional&lt;Author> optionalAuthors = getOptionalAuthors();\n    Optional&lt;List&lt;Book>> optionalBooks = optionalAuthors.map(author -> author.getBooks());\n    optionalBooks.ifPresent(books -> System.out.println(books));\n&#125;\n\n函数式接口只有一个抽象方法的接口我们称为函数接口。\nJDK的函数式接口都加上了@FunctionalInterface注解进行标识。但是无论是否加上该注解，只要接口中只有一个抽象方法，就是函数式接口。\n常见函数式接口\nConsumer 消费接口根据其中抽象方法的参数列表和返回值类型可以知道，我们可以在方法中对传入的参数进行消费。\n\n@FunctionalInterface\npublic interface Consumer&lt;T> &#123;\nvoid accept(T t);\n&#125;\n\n\nFunction 计算转换接口根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中对传入的参数计算或转换，把结果返回。\n\n@FunctionalInterface\npublic interface Function&lt;T, R> &#123;\nR apply(T t);\n&#125;\n\n\nPredicate 判断接口根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中对传入的参数条件判断，返回判断结果。\n\n@FunctionalInterface\npublic interface Predicate&lt;T> &#123;\nboolean test(T t);\n&#125;\n\n\nSupplier 生产型接口根据其中抽象方法的参数列表和返回值类型知道，我们可以在方法中创建对象，把创建好的对象返回。\n\n@FunctionalInterface\npublic interface Supplier&lt;T> &#123;\nT get();\n&#125;\n\n常用的默认方法\nand我们在使用Predicate接口的时候可能需要进行判断条件的拼接，而and方法相当于是使用&amp;&amp;来拼接两个判断条件。\n\nor我们在使用Predicate接口的时候可能需要进行判断条件的拼接。而or方法相当于是使用 || 来拼接两个判断条件。\n\nnegatePredicate接口中的方法。negate方法相当于是在判断添加前面加了个 !，表示取反\n\n\n方法引用我们在使用lambda时，如果方法体中只有一个方法的调用的话，我们可以用方法引用进一步简化代码。\n推荐用法：我们在使用lambda时不需要考虑什么时候用方法引用，方法引用的格式是什么。我们只需要在写完lambda方法发现方法体只有一行代码，并且在方法的调用时使用快捷键尝试是否能转换成方法引用即可。\n基本格式：类名或对象名::方法名引用类的静态方法    类名::方法名\n使用前提：如果我们在重写方法的时候，方法体中只有一行代码，并且这行代码是调用了某个类的静态方法，并且我们把要重写的抽象方法中所有的参数都按照顺序传入了这个静态方法中，这个时候我们就可以引用类的静态方法。\nList&lt;Author> authors = getAuthors();\nauthors.stream()\n        .map(author -> author.getAge())\n        .map(new Function&lt;Integer, String>() &#123;\n            @Override\n            public String apply(Integer age) &#123;\n                return String.valueOf(age);\n            &#125;\n        &#125;);\n\n优化\nList&lt;Author> authors = getAuthors();\nauthors.stream()\n        .map(author -> author.getAge())\n        .map(String::valueOf);\n\n引用对象的实例方法格式：对象名::方法名\n使用前提：如果我们在重写方法的时候，方法体中只有一行代码，并且这行代码是调用了某个对象的成员方法，并且我们把要重写的抽象方法中所有的参数都按照顺序传入了这个成员方法中，这个时候我们就可以引用对象的实例方法。\nList&lt;Author> authors = getAuthors();\nStringBuilder sb = new StringBuilder();\nauthors.stream()\n        .map(author -> author.getName())\n        .forEach(new Consumer&lt;String>() &#123;\n            @Override\n            public void accept(String name) &#123;\n                sb.append(name);\n            &#125;\n        &#125;);\n\nList&lt;Author> authors = getAuthors();\nStringBuilder sb = new StringBuilder();\nauthors.stream()\n        .map(author -> author.getName())\n        .forEach(sb::append);\n\n引用类的实例方法\n格式：类名::方法名\n使用前提：如果我们在重写方法的时候，方法体中只有一行代码，并且这行代码是调用了第一个参数的成员方法，并且我们把要重写的抽象方法中剩余的所有的参数都按照顺序传入这个成员方法中，这个时候我们就可以引用类的实例方法。\n构造器引用如果方法体中的一行代码是构造器的话就可以使用构造器引用。\n格式：类名::new\n使用前提：如果我们在重写方法的时候，方法体中只有一行代码，并且这行代码是调用了某个类的构造方法，并且我们把要重写的抽象方法中的所有参数都按照顺序传入了这个构造方法中，这个时候我们就可以引用构造器。\n高级用法基本数据类型优化我们之前用到的很多Stream的方法由于都使用了泛型，所以涉及到的参数和返回值都是引用数据类型。\n即使我们操作的数字类型，但实际使用的是它们的包装类。\n自动装箱和拆箱是需要消耗时间的，在数据量大的情况下，不断重复装箱拆箱的时候，就不能无视这个时间的损耗了。\nStream提供了很多专门针对基本数据类型的方法，对这部分的时间消耗进行优化。\n优化前：匿名内部类中转换的是包装类型，也就意味着后面的操作都要进行装箱和拆箱。\n并行流当流中有大量元素时，我们可以使用并行流去提高操作的效率。\n并行流就是把任务分配给多个线程去执行。如果我们自己使用代码去实现会很复杂，并且要求你对并发编程有足够的理解和认识。\n如果使用Stream的话，我们只需要修改一个方法的调用就可以使用并行流帮我们实现，从而提高效率。\nparallel方法可以把串行流转换成并行流。\nStream&lt;Integer> stream = Stream.of(1,2,3,4,5,6,7,8,9);\nInteger sum = stream.parallel()\n        .peek(new Consumer&lt;Integer>() &#123;\n            @Override\n            public void accept(Integer num) &#123;\n                System.out.println(num + \"->\" + Thread.currentThread().getName());\n            &#125;\n        &#125;)\n        .filter(num -> num > 5)\n        .reduce((result, element) -> result + element)\n        .get();\nSystem.out.println(sum);\n\n也可以通过parallelStream直接获取并行流对象。\nList&lt;Author> authors = getAuthors();\nauthors.parallelStream()\n        .map(author -> author.getName())\n        .map(StringBuilder::new)\n        .map(sb -> sb.append(\"hxp\"))\n        .forEach(str -> System.out.println(str));\n\nJava 8并行流API确保在多线程环境中执行操作时是线程安全的。默认情况下，Java使用一个名为ForkJoinPool的共享线程池来为并行流处理提供线程。ForkJoinPool建立了多个工作线程，每个线程都可以并行执行流处理管道的不同部分，从而通过利用所有可用的CPU核心提高了并行流的处理速度。\nJava 8并行流API通过拆分流元素、将它们分配到不同的线程中，以及在计算过程中使用出发器（例如，join()）等方式，来保证并行流的线程安全。具体来说，每个线程只处理自己负责的那一部分元素，而不会改变其他线程正在处理的元素。\n","slug":"Lambda表达式","date":"2023-06-10T07:21:11.000Z","categories_index":"","tags_index":"Java,Lambda","author_index":"大宝贝的程序员"},{"id":"a4984df6ed448f92b70760ecd7f1272b","title":"接口优化技巧","content":"接口优化的通用方案\n1.批处理批量思想：批量操作数据库，这个很好理解，我们在循环插入场景的接口中，可以在批处理执行完成后一次性插入或更新数据库，避免多次 IO。\n//for循环单笔入库\nlist.stream().forEatch(msg->&#123;\n    insert();\n&#125;);\n//批量入库\nbatchInsert();\n\n2. 异步处理异步思想：针对耗时比较长且不是结果必须的逻辑，我们可以考虑放到异步执行，这样能降低接口耗时。\n例如一个理财的申购接口，入账和写入申购文件是同步执行的，因为是 T+1 交易，后面这两个逻辑其实不是结果必须的，我们并不需要关注它的实时结果，所以我们考虑把入账和写入申购文件改为异步处理。\n至于异步的实现方式，可以用线程池，也可以用消息队列，还可以用一些调度任务框架。\n3.空间换时间一个很好理解的空间换时间的例子是合理使用缓存，针对一些频繁使用且不频繁变更的数据，可以提前缓存起来，需要时直接查缓存，避免频繁地查询数据库或者重复计算。\n需要注意的事，这里用了合理二字，因为空间换时间也是一把双刃剑，需要综合考虑你的使用场景，毕竟缓存带来的数据一致性问题也挺令人头疼。\n这里的缓存可以是 R2M，也可以是本地缓存、memcached，或者 Map。\n4. 预处理也就是预取思想，就是提前要把查询的数据，提前计算好，放入缓存或者表中的某个字段，用的时候会大幅提高接口性能。跟上面那个例子很像，但是关注点不同。\n举个简单的例子：理财产品，会有根据净值计算年化收益率的数据展示需求，利用净值去套用年化收益率计算公式计算的逻辑我们可以采用预处理，这样每一次接口调用直接取对应字段就可以了。\n5. 池化思想我们都用过数据库连接池，线程池等，这就是池思想的体现，它们解决的问题就是避免重复创建对象或创建连接，可以重复利用，避免不必要的损耗，毕竟创建销毁也会占用时间。\n池化思想包含但并不局限于以上两种，总的来说池化思想的本质是预分配与循环使用，明白这个原理后，我们即使是在做一些业务场景的需求时，也可以利用起来。\n比如：对象池\n6. 串行改并行串行就是，当前执行逻辑必须等上一个执行逻辑结束之后才执行，并行就是两个执行逻辑互不干扰，所以并行相对来说就比较节省时间，当然是建立在没有结果参数依赖的前提下。\n比如，理财的持仓信息展示接口，我们既需要查询用户的账户信息，也需要查询商品信息和 banner 位信息等等来渲染持仓页，如果是串行，基本上接口耗时就是累加的。如果是并行，接口耗时将大大降低。\n7. 索引加索引能大大提高数据查询效率，这个在接口设计之出也会考虑到，这里不再多赘述，随着需求的迭代，我们重点整理一下索引不生效的一些场景，希望对小伙伴们有所帮助。\n具体不生效场景不再一一举例，后面有时间的话，单独整理一下。\n\n8. 避免大事务所谓大事务问题，就是运行时间较长的事务，由于事务一致不提交，会导致数据库连接被占用，影响到别的请求访问数据库，影响别的接口性能。\n@Transactional(value =\"taskTransactionManager\", propagation =Propagation.REQUIRED, isolation =Isolation.READ_COMMITTED, rollbackFor =&#123;RuntimeException.class,Exception.class&#125;)\n    publicBasicResultpurchaseRequest(PurchaseRecordrecord)&#123;\n        BasicResult result =newBasicResult();\n        //插入账户任务\n        taskMapper.insert(ManagerParamUtil.buildTask(record,TaskEnum.Task_type.pension_account.type(),TaskEnum.Account_bizType.purchase_request.type()));\n        //插入同步任务\n        taskMapper.insert(ManagerParamUtil.buildTask(record,TaskEnum.Task_type.pension_sync.type(),TaskEnum.Sync_bizType.purchase.type()));\n        //插入影像件上传任务\n        taskMapper.insert(ManagerParamUtil.buildTask(record,TaskEnum.Task_type.pension_sync.type(),TaskEnum.Sync_bizType.cert.type()));\n        result.setInfo(ResultInfoEnum.SUCCESS);\n        return result;\n    &#125;\n\n上面这块代码主要是申购申请完成后，执行一系列的后续操作，如果现在新增申购完成后，发送 push 通知用户的需求。很有可能我们会在后面直接追加，如下图所示：事务中嵌套 RPC 调用，即非 DB 操作，这些非 DB 操作如果耗时较大的话，可能会出现大事务问题。大数据引发的问题主要有：死锁、接口超时、主从延迟等。\n@Transactional(value =\"taskTransactionManager\", propagation =Propagation.REQUIRED, isolation =Isolation.READ_COMMITTED, rollbackFor =&#123;RuntimeException.class,Exception.class&#125;)\n    public BasicResult purchaseRequest(PurchaseRecord record)&#123;\n        BasicResult result =newBasicResult();\n        ...\n        pushRpc.doPush(record);\n        result.setInfo(ResultInfoEnum.SUCCESS);\n        return result;\n    &#125;\n\n所以为避免大事务问题，我们可以通过以下方案规避：\n1，RPC 调用不放到事务里面\n2，查询操作尽量放到事务之外\n3，事务中避免处理太多数据\n9. 优化程序结构程序结构问题一般出现在多次需求迭代后，代码叠加形成。会造成一些重复查询、多次创建对象等耗时问题。在多人维护一个项目时比较多见。解决起来也比较简单，我们需要针对接口整体做重构，评估每个代码块的作用和用途，调整执行顺序。\n10. 深分页问题深分页问题比较常见，分页我们一般最先想到的就是 limit ，为什么会慢，我们可以看下这个 SQL：\nselect*from purchase_record where productCode &#x3D;&#39;PA9044&#39;and status&#x3D;4 order by orderTime desc limit 100000,200\n\nlimit 100000,200 意味着会扫描 100200 行，然后返回 200 行，丢弃掉前 100000 行。所以执行速度很慢。一般可以采用标签记录法来优化，比如：\nselect * from purchase_record where productCode &#x3D;&#39;PA9044&#39;and status&#x3D;4 and id &gt;100000 limit 200\n\n这样优化的好处是命中了主键索引，无论多少页，性能都还不错，但是局限性是需要一个连续自增的字段\n11. 锁粒度避免过粗锁一般是为了在高并发场景下保护共享资源采用的一种手段，但是如果锁的粒度太粗，会很影响接口性能。\n关于锁粒度：就是你要锁的范围有多大，不管是 synchronized 还是 redis 分布式锁，只需要在临界资源处加锁即可，不涉及共享资源的，不必要加锁。\n","slug":"接口优化技巧","date":"2023-06-10T06:32:07.000Z","categories_index":"","tags_index":"优化","author_index":"大宝贝的程序员"},{"id":"89180d8ad200729090513c1ce4f6ad31","title":"进程与线程","content":"进程与线程进程程序由指令和数据组成，但这些指令要运行，数据要读写，就必须将指令加载至 CPU，数据加载至内存。在指令运行过程中还需要用到磁盘、网络等设备。进程就是用来加载指令、管理内存、管理 IO 的当一个程序被运行，从磁盘加载这个程序的代码至内存，这时就开启了一个进程。\n进程就可以视为程序的一个实例。大部分程序可以同时运行多个实例进程（例如记事本、画图、浏览器等），也有的程序只能启动一个实例进程（例如网易云音乐、360 安全卫士等）\n线程一个进程之内可以分为一到多个线程。\n一个线程就是一个指令流，将指令流中的一条条指令以一定的顺序交给 CPU 执行\nJava 中，线程作为最小调度单位，进程作为资源分配的最小单位。 在 windows 中进程是不活动的，只是作\n为线程的容器\n二者对比\n进程基本上相互独立的，而线程存在于进程内，是进程的一个子集\n进程拥有共享的资源，如内存空间等，供其内部的线程共享\n进程间通信较为复杂\n\n同一台计算机的进程通信称为 IPC（Inter-process communication）\n\n不同计算机之间的进程通信，需要通过网络，并遵守共同的协议，例如 HTTP\n\n\n线程通信相对简单，因为它们共享进程内的内存，一个例子是多个线程可以访问同一个共享变量\n线程更轻量，线程上下文切换成本一般上要比进程上下文切换低\n并行与并发单核 cpu 下，线程实际还是串行执行的。操作系统中有一个组件叫做任务调度器，将 cpu 的时间片分给不同的程序使用，只是由于 cpu 在线程间（时间片很短）的切换非常快。总结为一句话就是： 微观串行，宏观并行 ，\n多核 cpu下，每个 核（core） 都可以调度运行线程，这时候线程可以是并行的。\n并发（concurrent）是同一时间应对（dealing with）多件事情的能力\n并行（parallel）是同一时间动手做（doing）多件事情的能力\n小结 ：\n\n单核 cpu 下，多线程不能实际提高程序运行效率，只是为了能够在不同的任务之间切换，不同线程轮流使用cpu ，不至于一个线程总占用 cpu，别的线程没法干活\n\n多核 cpu 可以并行跑多个线程，但能否提高程序运行效率还是要分情况的有些任务，经过精心设计，将任务拆分，并行执行，当然可以提高程序的运行效率。但不是所有计算任务都能拆分也不是所有任务都需要拆分，任务的目的如果不同，谈拆分和效率没啥意义\n\nIO 操作不占用 cpu，只是我们一般拷贝文件使用的是【阻塞 IO】，这时相当于线程虽然不用 cpu，但需要一直等待 IO 结束，没能充分利用线程。\n\n\nJava线程创建和运行线程方法一，直接使用Thread\n// 创建线程对象\nThread t = new Thread() &#123;\n public void run() &#123;\n // 要执行的任务\n &#125;\n&#125;;\n// 启动线程\nt.start()\n\n方法二，使用Runnable配合Thread\nRunnable runnable = new Runnable() &#123;\n public void run()&#123;\n // 要执行的任务\n &#125;\n&#125;;\n// 创建线程对象\nThread t = new Thread( runnable );\n// 启动线程\nt.start();\n\n用 Runnable 更容易与线程池等高级 API 配合，用 Runnable 让任务类脱离了 Thread 继承体系，更灵活\n方法三，FutureTask配合Thread\nFutureTask 能够接收 Callable 类型的参数，用来处理有返回结果的情况\n// 创建任务对象\nFutureTask&lt;Integer> task3 = new FutureTask&lt;>(() -> &#123;\n log.debug(\"hello\");\n return 100;\n&#125;);\n// 参数1 是任务对象; 参数2 是线程名字，推荐\nnew Thread(task3, \"t3\").start();\n// 主线程阻塞，同步等待 task 执行完毕的结果\nInteger result = task3.get();\nlog.debug(\"结果是:&#123;&#125;\", result);\n\n观察多个线程同时运行查看进程线程的方法windows\n\n任务管理器可以查看进程和线程数，也可以用来杀死进程\n\ntasklist 查看进程\n\ntaskkill 杀死进程\n\n\nlinux\n\nps -fe 查看所有进程\n\nps -fT -p  查看某个进程（PID）的所有线程\n\nkill 杀死进程\n\ntop 按大写 H 切换是否显示线程\n\ntop -H -p  查看某个进程（PID）的所有线程\n\n\nJava\n\njps 命令查看所有 Java 进程\n\njstack    查看某个 Java 进程（PID）的所有线程状态\n\njconsole 来查看某个 Java 进程中线程的运行情况（图形界面）\n\njconsole 远程监控配置\n需要以如下方式运行你的 java 类\n    java -Djava.rmi.server.hostname&#x3D;&#96;ip地址&#96; -Dcom.sun.management.jmxremote -\n    Dcom.sun.management.jmxremote.port&#x3D;&#96;连接端口&#96; -Dcom.sun.management.jmxremote.ssl&#x3D;是否安全连接 -\n    Dcom.sun.management.jmxremote.authenticate&#x3D;是否认证 java类\n\n#### 线程运行的原理\n\n**栈与栈帧**\n\nJava Virtual Machine Stacks （Java 虚拟机栈）\n\n JVM 中由堆、栈、方法区所组成，其中栈内存就是给线程用的，每个线程启动后，虚拟机就会为其分配一块栈内存。\n\n每个栈由多个栈帧（Frame）组成，对应着每次方法调用时所占用的内存\n\n每个线程只能有一个活动栈帧，对应着当前正在执行的那个方法\n\n#### 线程上下文切换（Thread Context Switch）\n\n因为以下一些原因导致 cpu 不再执行当前的线程，转而执行另一个线程的代码\n\n- 线程的 cpu 时间片用完\n\n- 垃圾回收\n\n- 有更高优先级的线程需要运行\n\n- 线程自己调用了 sleep、yield、wait、join、park、synchronized、lock 等方法\n\n当 Context Switch 发生时，需要由操作系统保存当前线程的状态，并恢复另一个线程的状态，Java 中对应的概念就是程序计数器（Program Counter Register），它的作用是记住下一条 jvm 指令的执行地址，是线程私有的\n\n- 状态包括程序计数器、虚拟机栈中每个栈帧的信息，如局部变量、操作数栈、返回地址等\n\n- Context Switch 频繁发生会影响性能\n\n#### 常见方法\n\n**start** **与** **run**\n\n- 直接调用 run 是在主线程中执行了 run，没有启动新的线程\n\n- 使用 start 是启动新的线程，通过新的线程间接执行 run 中的代码\n\n**sleep** **与** **yield**\n\n**sleep**\n\n1. 调用 sleep 会让当前线程从 *Running* 进入 *Timed Waiting* 状态（阻塞）\n\n2. 其它线程可以使用 interrupt 方法打断正在睡眠的线程，这时 sleep 方法会抛出 InterruptedException\n\n3. 睡眠结束后的线程未必会立刻得到执行\n\n4. 建议用 TimeUnit 的 sleep 代替 Thread 的 sleep 来获得更好的可读性\n\n**yield**\n\n1. 调用 yield 会让当前线程从 *Running* 进入 *Runnable* 就绪状态，然后调度执行其它线程\n\n2. 具体的实现依赖于操作系统的任务调度器\n\n**线程优先级**\n\n- 线程优先级会提示（hint）调度器优先调度该线程，但它仅仅是一个提示，调度器可以忽略它\n\n- 如果 cpu 比较忙，那么优先级高的线程会获得更多的时间片，但 cpu 闲时，优先级几乎没作用\n\n**wait vs sleep**\n\n共同点\n\n* wait() ，wait(long) 和 sleep(long) 的效果都是让当前线程暂时放弃 CPU 的使用权，进入阻塞状态\n\n不同点\n\n* 方法归属不同\n  * sleep(long) 是 Thread 的静态方法\n  * 而 wait()，wait(long) 都是 Object 的成员方法，每个对象都有\n\n* 醒来时机不同\n  * 执行 sleep(long) 和 wait(long) 的线程都会在等待相应毫秒后醒来\n  * wait(long) 和 wait() 还可以被 notify 唤醒，wait() 如果不唤醒就一直等下去\n  * 它们都可以被打断唤醒\n\n* 锁特性不同（重点）\n  * wait 方法的调用必须先获取 wait 对象的锁，而 sleep 则无此限制\n  * wait 方法执行后会释放对象锁，允许其它线程获得该对象锁（我放弃 cpu，但你们还可以用）\n  * 而 sleep 如果在 synchronized 代码块中执行，并不会释放对象锁（我放弃 cpu，你们也用不了）\n\n **join** \n\n等待线程运行结束，可以指定等待的时间，如果指定的等待时间超过了线程运行的时间就会提前结束。\n\n**interrupt** **方法详解**\n\n sleep，wait，join等方法都会让线程进入阻塞状态，interrupt打断sleep，wait，join的线程\n\n打断 sleep 的线程, 会清空打断状态，以 sleep 为例\n\n&#96;&#96;&#96;&#96;java\nprivate static void test1() throws InterruptedException &#123;\n Thread t1 &#x3D; new Thread(()-&gt;&#123;\n sleep(1);\n &#125;, &quot;t1&quot;);\n t1.start();\n sleep(0.5);\n t1.interrupt();\n log.debug(&quot; 打断状态: &#123;&#125;&quot;, t1.isInterrupted());\n&#125;\n\n\n\n输出：\njava.lang.InterruptedException: sleep interrupted\n at java.lang.Thread.sleep(Native Method)\n at java.lang.Thread.sleep(Thread.java:340)\n at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)\n at cn.itcast.n2.util.Sleeper.sleep(Sleeper.java:8)\n at cn.itcast.n4.TestInterrupt.lambda$test1$3(TestInterrupt.java:59)\n at java.lang.Thread.run(Thread.java:745)\n21:18:10.374 [main] c.TestInterrupt - 打断状态: false\n\n打断正常运行的线程, 不会清空打断状态\nprivate static void test2() throws InterruptedException &#123;\n Thread t2 = new Thread(()->&#123;\n while(true) &#123;\n Thread current = Thread.currentThread();\n boolean interrupted = current.isInterrupted();\n if(interrupted) &#123;\n log.debug(\" 打断状态: &#123;&#125;\", interrupted);\n break;\n &#125;\n &#125;\n &#125;, \"t2\");\n t2.start();\n sleep(0.5);\n t2.interrupt();\n&#125;\n\n输出\n20:57:37.964 [t2] c.TestInterrupt - 打断状态: true\n\n打断 park 线程, 不会清空打断状态\nprivate static void test3() throws InterruptedException &#123;\n Thread t1 = new Thread(() -> &#123;\n log.debug(\"park...\");\n LockSupport.park();\n log.debug(\"unpark...\");\n log.debug(\"打断状态：&#123;&#125;\", Thread.currentThread().isInterrupted());\n &#125;, \"t1\");\n t1.start();\n sleep(0.5);\n t1.interrupt();\n&#125;\n\n输出\n21:11:52.795 [t1] c.TestInterrupt - park... \n21:11:53.295 [t1] c.TestInterrupt - unpark... \n21:11:53.295 [t1] c.TestInterrupt - 打断状态：true\n\n不推荐的方法\nstop()\t停止线程运行\nsuspend() \t挂起（暂停）线程运行\nresume() \t恢复线程运行\nPark &amp; Unpark它们是 LockSupport 类中的方法\n// 暂停当前线程\nLockSupport.park(); \n// 恢复某个线程的运行\nLockSupport.unpark(暂停线程对象)\n\n先 park 再 unpark\nThread t1 = new Thread(() -> &#123;\n log.debug(\"start...\");\n sleep(1);\n log.debug(\"park...\");\n LockSupport.park();\n log.debug(\"resume...\");\n&#125;,\"t1\");\nt1.start();\nsleep(2);\nlog.debug(\"unpark...\");\nLockSupport.unpark(t1);\n\n输出：\n18:42:52.585 c.TestParkUnpark [t1] - start... \n18:42:53.589 c.TestParkUnpark [t1] - park... \n18:42:54.583 c.TestParkUnpark [main] - unpark... \n18:42:54.583 c.TestParkUnpark [t1] - resume...\n\n先 unpark 再 park\nThread t1 = new Thread(() -> &#123;\n log.debug(\"start...\");\n sleep(2);\n log.debug(\"park...\");\n LockSupport.park();\n log.debug(\"resume...\");\n&#125;, \"t1\");\nt1.start();\nsleep(1);\nlog.debug(\"unpark...\");\nLockSupport.unpark(t1)\n\n输出\n18:43:50.765 c.TestParkUnpark [t1] - start... \n18:43:51.764 c.TestParkUnpark [main] - unpark... \n18:43:52.769 c.TestParkUnpark [t1] - park... \n18:43:52.769 c.TestParkUnpark [t1] - resume...\n\n主线程与守护线程默认情况下，Java 进程需要等待所有线程都运行结束，才会结束。有一种特殊的线程叫做守护线程，只要其它非守护线程运行结束了，即使守护线程的代码没有执行完，也会强制结束。\n\n\n\n\n\n\n\n\n\n垃圾回收器线程就是一种守护线程\nTomcat 中的 Acceptor 和 Poller 线程都是守护线程，所以 Tomcat 接收到 shutdown 命令后，不会等待它们处理完当前请求\n操作系统层面的五种状态从操作系统层面来描述\n\n【初始状态】仅是在语言层面创建了线程对象，还未与操作系统线程关联\n【可运行状态&#x2F;就绪状态】指该线程已经被创建（与操作系统线程关联），可以由 CPU 调度执行\n【运行状态】指获取了 CPU 时间片运行中的状态\n\n当 CPU 时间片用完，会从【运行状态】转换至【可运行状态】，会导致线程的上下文切换\n\n【阻塞状态】\n\n如果调用了阻塞 API，如 BIO 读写文件，这时该线程实际不会用到 CPU，会导致线程上下文切换，进入【阻塞状态】\n\n等 BIO 操作完毕，会由操作系统唤醒阻塞的线程，转换至【可运行状态】\n\n与【可运行状态】的区别是，对【阻塞状态】的线程来说只要它们一直不唤醒，调度器就一直不会考虑调度它们\n\n\n【终止状态】表示线程已经执行完毕，生命周期已经结束，不会再转换为其它状态\nJava API 层面的六种状态六种状态及转换\n\n分别是\n\n新建(NEW)\n当一个线程对象被创建，但还未调用 start 方法时处于新建状态\n此时未与操作系统底层线程关联\n\n\n可运行(RUNNABLE)\n调用了 start 方法，就会由新建进入可运行\n此时与底层线程关联，由操作系统调度执行\n\n\n终结(TERMINATED)\n线程内代码已经执行完毕，由可运行进入终结\n此时会取消与底层线程关联\n\n\n阻塞(BLOCKED)\n当获取锁失败后，由可运行进入 Monitor 的阻塞队列阻塞，此时不占用 cpu 时间\n当持锁线程释放锁时，会按照一定规则唤醒阻塞队列中的阻塞线程，唤醒后的线程进入可运行状态\n\n\n等待(WAITING)\n当获取锁成功后，但由于条件不满足，调用了 wait() 方法，此时从可运行状态释放锁进入 Monitor 等待集合等待，同样不占用 cpu 时间\n当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的等待线程，恢复为可运行状态\n\n\n有时限等待(TIME_WAITING)\n当获取锁成功后，但由于条件不满足，调用了 wait(long) 方法，此时从可运行状态释放锁进入 Monitor 等待集合进行有时限等待，同样不占用 cpu 时间\n当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的有时限等待线程，恢复为可运行状态，并重新去竞争锁\n如果等待超时，也会从有时限等待状态恢复为可运行状态，并重新去竞争锁\n还有一种情况是调用 sleep(long) 方法也会从可运行状态进入有时限等待状态，但与 Monitor 无关，不需要主动唤醒，超时时间到自然恢复为可运行状态\n\n\n\n\n\n\n\n\n\n\n\n\n其它情况\n\n可以用 interrupt() 方法打断等待、有时限等待的线程，让它们恢复为可运行状态\npark，unpark 等方法也可以让线程等待和唤醒\n\n共享模型两个线程对初始值为 0 的静态变量一个做自增，一个做自减，各做 5000 次，的结果可能是正数、负数、零。\n因为 Java 中对静态变量的自增，自减并不是原子操作，要彻底理解，必须从字节码来进行分析\n如对于 i++ 而言（i 为静态变量），实际会产生如下的 JVM 字节码指令：\ngetstatic i // 获取静态变量i的值\niconst_1 // 准备常量1\niadd // 自增\nputstatic i // 将修改后的值存入静态变量i\n\n i– 也是类似：\ngetstatic i // 获取静态变量i的值\niconst_1 // 准备常量1\nisub // 自减\nputstatic i // 将修改后的值存入静态变量i\n\n\n单线程以上代码是顺序执行（不会交错）没有问题。\n\n多线程下会产生指令交错。\n\n\n例如出现正数的情况：\n\n临界区Critical Section\n\n一个程序运行多个线程本身是没有问题的\n\n问题出在多个线程访问共享资源\n\n多个线程读共享资源其实也没有问题\n在多个线程对共享资源读写操作时发生指令交错，就会出现问题\n\n\n一段代码块内如果存在对共享资源的多线程读写操作，称这段代码块为临界区\n\n\n竞态条件Race Condition\n多个线程在临界区内执行，由于代码的执行序列不同而导致结果无法预测，称之为发生了竞态条件\n互斥的解决方案为了避免临界区的竞态条件发生，有多种手段可以达到目的。\n\n阻塞式的解决方案：synchronized，Lock\n\n非阻塞式的解决方案：原子变量\n\n\nsynchronized\nsynchronized(对象) // 线程1， 线程2(blocked)\n&#123;\n 临界区\n&#125;\n\n加对象锁的情况：\n\nsynchronized实际是用对象锁保证了临界区内代码的原子性，临界区内的代码对外是不可分割的，不会被线程切换所打断。\n方法上的synchronized\n非静态方法相当于是锁住this(当前对象)\nclass Test&#123;\n public synchronized void test() &#123;\n \n \t&#125;\n&#125;\n等价于\nclass Test&#123;\n public void test() &#123;\n synchronized(this) &#123;\n \n \t\t&#125;\n\t&#125;\n&#125;\n\n静态方法相当于锁住整个类（.class）\nclass Test&#123;\n public synchronized static void test() &#123;\n \t&#125;\n&#125;\n等价于\nclass Test&#123;\n public static void test() &#123;\n synchronized(Test.class) &#123;\n \n \t\t&#125;\n \t&#125;\n&#125;\n\n\n\n\n\n\n\n\n\n\njava 中互斥和同步都可以采用 synchronized 关键字来完成，但它们还是有区别的：\n互斥是保证临界区的竞态条件发生，同一时刻只能有一个线程执行临界区代码\n同步是由于线程执行的先后、顺序不同、需要一个线程等待其它线程运行到某个点\nwait &#x2F; notify它们都是线程之间进行协作的手段，都属于 Object 对象的方法。必须获得此对象的锁，才能调用这几个方法\nfinal static Object obj = new Object();\npublic static void main(String[] args) &#123;\n new Thread(() -> &#123;\n synchronized (obj) &#123;\n log.debug(\"执行....\");\n try &#123;\n obj.wait(); // 让线程在obj上一直等待下去\n &#125; catch (InterruptedException e) &#123;\n e.printStackTrace();\n &#125;\n log.debug(\"其它代码....\");\n &#125;\n &#125;).start();\n    \n new Thread(() -> &#123;\n synchronized (obj) &#123;\n log.debug(\"执行....\");\n try &#123;\n obj.wait(); // 让线程在obj上一直等待下去\n &#125; catch (InterruptedException e) &#123;\n e.printStackTrace();\n &#125;\n log.debug(\"其它代码....\");\n &#125;\n &#125;).start();\n    \n // 主线程两秒后执行\n sleep(2);\n log.debug(\"唤醒 obj 上其它线程\");\n synchronized (obj) &#123;\n obj.notify(); //①唤醒obj上一个线程 \n // obj.notifyAll(); //②唤醒obj上所有等待线程\n &#125;\n&#125;\n\n①\n20:00:53.096 [Thread-0] c.TestWaitNotify - 执行.... \n20:00:53.099 [Thread-1] c.TestWaitNotify - 执行.... \n20:00:55.096 [main] c.TestWaitNotify - 唤醒 obj 上其它线程\n20:00:55.096 [Thread-0] c.TestWaitNotify - 其它代码....\n\n②\n19:58:15.457 [Thread-0] c.TestWaitNotify - 执行.... \n19:58:15.460 [Thread-1] c.TestWaitNotify - 执行.... \n19:58:17.456 [main] c.TestWaitNotify - 唤醒 obj 上其它线程\n19:58:17.456 [Thread-1] c.TestWaitNotify - 其它代码.... \n19:58:17.456 [Thread-0] c.TestWaitNotify - 其它代码....\n\nwait() 方法会释放对象的锁，进入 WaitSet 等待区，从而让其他线程就机会获取对象的锁。无限制等待，直到notify 为止。wait(long n) 有时限的等待, 到 n 毫秒后结束等待，或是被 notify\n变量的线程安全分析成员变量和静态变量是否线程安全？\n\n如果它们没有共享，则线程安全\n\n如果它们被共享了，根据它们的状态是否能够改变，又分两种情况\n\n如果只有读操作，则线程安全\n如果有读写操作，则这段代码是临界区，需要考虑线程安全\n\n\n\n局部变量是否线程安全？\n\n局部变量是线程安全的\n\n但局部变量引用的对象则未必\n\n如果该对象没有逃离方法的作用访问，它是线程安全的\n如果该对象逃离方法的作用范围，需要考虑线程安全\n\n\n\n局部变量线程安全分析\npublic static void test1() &#123;\n int i = 10;\n i++;\n&#125;\n\n每个线程调用 test1() 方法时局部变量 i，会在每个线程的栈帧内存中被创建多份，因此不存在共享\n如同：\n\n对象逃离方法的作用范围\nclass ThreadUnsafe &#123;\n ArrayList&lt;String> list = new ArrayList&lt;>();\n public void method1(int loopNumber) &#123;\n for (int i = 0; i &lt; loopNumber; i++) &#123;\n // &#123; 临界区, 会产生竞态条件\n method2();\n method3();\n // &#125; 临界区\n \t&#125;\n &#125;\n private void method2() &#123;\n list.add(\"1\");\n &#125;\n private void method3() &#123;\n list.remove(0);\n &#125;\n&#125;\n\nstatic final int THREAD_NUMBER = 2;\nstatic final int LOOP_NUMBER = 200;\npublic static void main(String[] args) &#123;\n ThreadUnsafe test = new ThreadUnsafe();\n for (int i = 0; i &lt; THREAD_NUMBER; i++) &#123;\n new Thread(() -> &#123;\n test.method1(LOOP_NUMBER);\n &#125;, \"Thread\" + i).start();\n &#125;\n&#125;\n\n如果线程2 还未 add，线程1 remove 就会报错：\n\n将 list 修改为局部变量\nclass ThreadSafe &#123;\n public final void method1(int loopNumber) &#123;\n ArrayList&lt;String> list = new ArrayList&lt;>();\n for (int i = 0; i &lt; loopNumber; i++) &#123;\n method2(list);\n method3(list);\n &#125;\n &#125;\n private void method2(ArrayList&lt;String> list) &#123;\n list.add(\"1\");\n &#125;\n private void method3(ArrayList&lt;String> list) &#123;\n list.remove(0);\n &#125;\n&#125;\n\nlist 是局部变量，每个线程调用时会创建其不同实例，没有共享，而 method2 的参数是从 method1 中传递过来的，与 method1 中引用同一个对象，method3 的参数分析与 method2 相同\n\n常见线程安全类String\nInteger\nStringBuffer\nRandom\nVector\nHashtable\njava.util.concurrent 包下的类\n这里说它们是线程安全的是多个线程调用它们同一个实例的某个方法时，是线程安全的。\nHashtable table = new Hashtable();\nnew Thread(()->&#123;\n table.put(\"key1\", \"value1\");\n&#125;).start();\nnew Thread(()->&#123;\n table.put(\"key2\", \"value2\");\n&#125;).start();\n\n它们的每个方法是原子的，但它们多个方法的组合不是原子的。\n线程安全类方法的组合码不是线程安全的\nHashtable table = new Hashtable();\n// 线程1，线程2\nif( table.get(\"key\") == null) &#123;\n table.put(\"key\", value);\n&#125;\n\n\n不可变类线程安全性\nString、Integer 等都是不可变类，因为其内部的状态不可以改变，因此它们的方法都是线程安全的\nMonitor概念Java对象头普通对象\n|--------------------------------------------------------------|\n| Object Header (64 bits)                                      |\n|------------------------------------|-------------------------|\n| Mark Word (32 bits)                | Klass Word (32 bits)    |\n|------------------------------------|-------------------------|\n\n数组对象\n|---------------------------------------------------------------------------------|\n| Object Header (96 bits)                                                         |\n|--------------------------------|-----------------------|------------------------|\n| Mark Word(32bits)              | Klass Word(32bits)    | array length(32bits)   | \n|--------------------------------|-----------------------|------------------------|\n\n其中 Mark Word 结构为\n|-------------------------------------------------------|--------------------|\n| Mark Word (32 bits) \t\t\t\t                    |       State        |\n|-------------------------------------------------------|--------------------|\n| hashcode:25              | age:4 | biased_lock:0 | 01 |       Normal       |\n|-------------------------------------------------------|--------------------|\n| thread:23      | epoch:2 | age:4 | biased_lock:1 | 01 |       Biased       |\n|-------------------------------------------------------|--------------------|\n| ptr_to_lock_record:30 \t                       | 00 | Lightweight Locked |\n|-------------------------------------------------------|--------------------|\n| ptr_to_heavyweight_monitor:30                    | 10 | Heavyweight Locked |\n|-------------------------------------------------------|--------------------|\n|                                                  | 11 |     Marked for GC  |\n|-------------------------------------------------------|--------------------|\n\n64 位虚拟机 Mark Word\n","slug":"进程与线程","date":"2023-06-09T12:37:40.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"cbf42b2f3cbb0eee5de5bbaaf2ed03ea","title":"微服务-基础","content":"Nacos配置管理Nacos除了可以做注册中心，同样可以做配置管理来使用。\n统一配置管理当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会让人抓狂，而且很容易出错。我们需要一种统一配置管理方案，可以集中管理所有实例的配置。\n\nNacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。\n在nacos中添加配置文件如何在nacos中管理配置呢？\n\n然后在弹出的表单中，填写配置信息：\n\n\n\n\n\n\n\n\n\n\n注意：项目的核心配置，需要热更新的配置才有放到nacos管理的必要。基本不会变更的一些配置还是保存在微服务本地比较好。\n从微服务拉取配置微服务要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目启动。\n但如果尚未读取application.yml，又如何得知nacos地址呢？\n因此spring引入了一种新的配置文件：bootstrap.yaml文件，会在application.yml之前被读取，流程如下：\n\n1）引入nacos-config依赖\n首先，在user-service服务中，引入nacos-config的客户端依赖：\n&lt;!--nacos配置管理依赖-->\n&lt;dependency>\n    &lt;groupId>com.alibaba.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-alibaba-nacos-config&lt;/artifactId>\n&lt;/dependency>\n\n2）添加bootstrap.yaml\n然后，在user-service中添加一个bootstrap.yaml文件，内容如下：\nspring:\n  application:\n    name: userservice # 服务名称\n  profiles:\n    active: dev #开发环境，这里是dev \n  cloud:\n    nacos:\n      server-addr: localhost:8848 # Nacos地址\n      config:\n        file-extension: yaml # 文件后缀名\n\n这里会根据spring.cloud.nacos.server-addr获取nacos地址，再根据\n$&#123;spring.application.name&#125;-$&#123;spring.profiles.active&#125;.$&#123;spring.cloud.nacos.config.file-extension&#125;作为文件id，来读取配置。\n本例中，就是去读取userservice-dev.yaml：\n\n3）读取nacos配置\n在user-service中的UserController中添加业务逻辑，读取pattern.dateformat配置：\n\n代码：\nimport cn.itcast.user.pojo.User;\nimport cn.itcast.user.service.UserService;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.web.bind.annotation.*;\n\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\n\n@Slf4j\n@RestController\n@RequestMapping(\"/user\")\npublic class UserController &#123;\n\n    @Autowired\n    private UserService userService;\n\n    @Value(\"$&#123;pattern.dateformat&#125;\")\n    private String dateformat;\n    \n    @GetMapping(\"now\")\n    public String now()&#123;\n        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat));\n    &#125;\n    // ...略\n&#125;\n\n\n\n在页面访问，可以看到效果：\n\n配置热更新我们最终的目的，是修改nacos中的配置后，微服务中无需重启即可让配置生效，也就是配置热更新。\n要实现配置热更新，可以使用两种方式：\n方式一在@Value注入的变量所在类上添加注解@RefreshScope：\n\n方式二使用@ConfigurationProperties注解代替@Value注解。\n在user-service服务中，添加一个类，读取patterrn.dateformat属性：\npackage cn.itcast.user.config;\n\nimport lombok.Data;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.stereotype.Component;\n\n@Component\n@Data\n@ConfigurationProperties(prefix = \"pattern\")\npublic class PatternProperties &#123;\n    private String dateformat;\n&#125;\n\n在UserController中使用这个类代替@Value：\n\n完整代码：\nimport cn.itcast.user.config.PatternProperties;\nimport cn.itcast.user.pojo.User;\nimport cn.itcast.user.service.UserService;\nimport lombok.extern.slf4j.Slf4j;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\n\n@Slf4j\n@RestController\n@RequestMapping(\"/user\")\npublic class UserController &#123;\n\n    @Autowired\n    private UserService userService;\n\n    @Autowired\n    private PatternProperties patternProperties;\n\n    @GetMapping(\"now\")\n    public String now()&#123;\n        return LocalDateTime.now().format(DateTimeFormatter.ofPattern(patternProperties.getDateformat()));\n    &#125;\n\n    // 略\n&#125;\n\n配置共享其实微服务启动时，会去nacos读取多个配置文件，例如：\n\n[spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml\n\n[spring.application.name].yaml，例如：userservice.yaml\n\n\n而[spring.application.name].yaml不包含环境，因此可以被多个环境共享。\n下面我们通过案例来测试配置共享\n添加一个环境共享配置我们在nacos中添加一个userservice.yaml文件：\n\n在user-service中读取共享配置在user-service服务中，修改PatternProperties类，读取新添加的属性：\n\n在user-service服务中，修改UserController，添加一个方法：\n\n运行两个UserApplication，使用不同的profile修改UserApplication2这个启动项，改变其profile值：\n\n\n这样，UserApplication(8081)使用的profile是dev，UserApplication2(8082)使用的profile是test。\n启动UserApplication和UserApplication2\n访问http://localhost:8081/user/prop，结果：\n\n访问http://localhost:8082/user/prop，结果：\n\n可以看出来，不管是dev，还是test环境，都读取到了envSharedValue这个属性的值。\n配置共享的优先级当nacos、服务本地同时出现相同属性时，优先级有高低之分：\n\n搭建Nacos集群&#x2F;&#x2F;略\nFeign远程调用先来看以前利用RestTemplate发起远程调用的代码：\n\n存在下面的问题：\n•代码可读性差，编程体验不统一\n•参数复杂URL难以维护\nFeign替代RestTemplateFegin的使用步骤如下：\n引入依赖我们在order-service服务的pom文件中引入feign的依赖：\n&lt;dependency>\n    &lt;groupId>org.springframework.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-openfeign&lt;/artifactId>\n&lt;/dependency>\n\n添加注解在order-service的启动类添加注解开启Feign的功能：\n\n编写Feign的客户端在order-service中新建一个接口，内容如下：\nimport cn.itcast.order.pojo.User;\nimport org.springframework.cloud.openfeign.FeignClient;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\n\n@FeignClient(\"userservice\")\npublic interface UserClient &#123;\n    @GetMapping(\"/user/&#123;id&#125;\")\n    User findById(@PathVariable(\"id\") Long id);\n&#125;\n\n这个客户端主要是基于SpringMVC的注解来声明远程调用的信息，比如：\n\n服务名称：userservice\n请求方式：GET\n请求路径：&#x2F;user&#x2F;{id}\n请求参数：Long id\n返回值类型：User\n\n这样，Feign就可以帮助我们发送http请求，无需自己使用RestTemplate来发送了。\n测试\n修改order-service中的OrderService类中的queryOrderById方法，使用Feign客户端代替RestTemplate：\n\n小结\n使用Feign的步骤：\n① 引入依赖\n② 添加@EnableFeignClients注解\n③ 编写FeignClient接口\n④ 使用FeignClient中定义的方法代替RestTemplate\n自定义配置Feign可以支持很多的自定义配置，如下表所示：\n\n\n\n类型\n作用\n说明\n\n\n\nfeign.Logger.Level\n修改日志级别\n包含四种不同的级别：NONE、BASIC、HEADERS、FULL\n\n\nfeign.codec.Decoder\n响应结果的解析器\nhttp远程调用的结果做解析，例如解析json字符串为java对象\n\n\nfeign.codec.Encoder\n请求参数编码\n将请求参数编码，便于通过http请求发送\n\n\nfeign.Contract\n支持的注解格式\n默认是SpringMVC的注解\n\n\nfeign. Retryer\n失败重试机制\n请求失败的重试机制，默认是没有，不过会使用Ribbon的重试\n\n\n一般情况下，默认值就能满足我们使用，如果要自定义时，只需要创建自定义的@Bean覆盖默认Bean即可。\n配置文件方式基于配置文件修改feign的日志级别可以针对单个服务：\nfeign:  \n  client:\n    config: \n      userservice: # 针对某个微服务的配置\n        loggerLevel: FULL #  日志级别 \n\n也可以针对所有服务：\nfeign:  \n  client:\n    config: \n      default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置\n        loggerLevel: FULL #  日志级别 \n\n而日志的级别分为四种：\n\nNONE：不记录任何日志信息，这是默认值。\nBASIC：仅记录请求的方法，URL以及响应状态码和执行时间\nHEADERS：在BASIC的基础上，额外记录了请求和响应的头信息\nFULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。\n\nJava代码方式也可以基于Java代码来修改日志级别，先声明一个类，然后声明一个Logger.Level的对象：\npublic class DefaultFeignConfiguration  &#123;\n    @Bean\n    public Logger.Level feignLogLevel()&#123;\n        return Logger.Level.BASIC; // 日志级别为BASIC\n    &#125;\n&#125;\n\n如果要全局生效，将其放到启动类的@EnableFeignClients这个注解中：\n@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration .class) \n\n如果是局部生效，则把它放到对应的@FeignClient这个注解中：\n@FeignClient(value = \"userservice\", configuration = DefaultFeignConfiguration .class) \n\nFeign使用优化Feign底层发起http请求，依赖于其它的框架。其底层客户端实现包括：\n•URLConnection：默认实现，不支持连接池\n•Apache HttpClient ：支持连接池\n•OKHttp：支持连接池\n因此提高Feign的性能主要手段就是使用连接池代替默认的URLConnection。\n这里我们用Apache的HttpClient来演示。\n1）引入依赖\n在order-service的pom文件中引入Apache的HttpClient依赖：\n&lt;!--httpClient的依赖 -->\n&lt;dependency>\n    &lt;groupId>io.github.openfeign&lt;/groupId>\n    &lt;artifactId>feign-httpclient&lt;/artifactId>\n&lt;/dependency>\n\n2）配置连接池\n在order-service的application.yml中添加配置：\nfeign:\n  client:\n    config:\n      default: # default全局的配置\n        loggerLevel: BASIC # 日志级别，BASIC就是基本的请求和响应信息\n  httpclient:\n    enabled: true # 开启feign对HttpClient的支持\n    max-connections: 200 # 最大的连接数\n    max-connections-per-route: 50 # 每个路径的最大连接数\n\n接下来，在FeignClientFactoryBean中的loadBalance方法中打断点：\n\nDebug方式启动order-service服务，可以看到这里的client，底层就是Apache HttpClient：\n\n总结，Feign的优化：\n1.日志级别尽量用basic\n2.使用HttpClient或OKHttp代替URLConnection\n①  引入feign-httpClient依赖\n②  配置文件开启httpClient功能，设置连接池参数\n最佳实践观察可以发现，Feign的客户端与服务提供者的controller代码非常相似：\nfeign客户端：\n\nUserController：\n\n有没有一种办法简化这种重复的代码编写呢？\n继承方式一样的代码可以通过继承来共享：\n1）定义一个API接口，利用定义方法，并基于SpringMVC注解做声明。\n2）Feign客户端和Controller都集成改接口\n\n优点：\n\n简单\n实现了代码共享\n\n缺点：\n\n服务提供方、服务消费方紧耦合\n\n参数列表中的注解映射并不会继承，因此Controller中必须再次声明方法、参数列表、注解\n\n\n实现基于抽取的最佳实践将Feign的Client抽取为独立模块，并且把接口有关的POJO、默认的Feign配置都放到这个模块中，提供给所有消费者使用。\n例如，将UserClient、User、Feign的默认配置都抽取到一个feign-api包中，所有微服务引用该依赖包，即可直接使用。\n\n项目结构：\n\n在feign-api中然后引入feign的starter依赖\n&lt;dependency>\n    &lt;groupId>org.springframework.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-openfeign&lt;/artifactId>\n&lt;/dependency>\n\n然后，order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中\n\n在order-service中使用feign-api首先，删除order-service中的UserClient、User、DefaultFeignConfiguration等类或接口。\n在order-service的pom文件中中引入feign-api的依赖：\n&lt;dependency>\n    &lt;groupId>cn.itcast.demo&lt;/groupId>\n    &lt;artifactId>feign-api&lt;/artifactId>\n    &lt;version>1.0&lt;/version>\n&lt;/dependency>\n\n修改order-service中的所有与上述三个组件有关的导包部分，改成导入feign-api中的包\n重启后，发现服务报错了：\n\n这是因为UserClient现在在cn.itcast.feign.clients包下，\n而order-service的@EnableFeignClients注解是在cn.itcast.order包下，不在同一个包，无法扫描到UserClient。\n解决扫描包问题方式一：\n指定Feign应该扫描的包：\n@EnableFeignClients(basePackages = \"cn.itcast.feign.clients\")\n\n方式二：\n指定需要加载的Client接口：\n@EnableFeignClients(clients = &#123;UserClient.class&#125;)\n\n\n\nGateway服务网关Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。\n网关的核心功能特性：\n\n请求路由\n权限控制\n限流\n\n架构图：\n\n权限控制：网关作为微服务入口，需要校验用户是是否有请求资格，如果没有则进行拦截。\n路由和负载均衡：一切请求都必须先经过gateway，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。\n限流：当请求流量过高时，在网关中按照下流的微服务能够接受的速度来放行请求，避免服务压力过大。\n在SpringCloud中网关的实现包括两种：\n\ngateway\nzuul\n\nZuul是基于Servlet的实现，属于阻塞式编程。而SpringCloudGateway则是基于Spring5中提供的WebFlux ，属于响应式编程的实现，具备更好的性能。\ngateway快速入门网关的基本路由功能。\n基本步骤如下：\n\n创建SpringBoot工程gateway，引入网关依赖\n编写启动类\n编写基础配置和路由规则\n启动网关服务进行测试\n\n创建gateway服务，引入依赖引入依赖：\n&lt;!--网关-->\n&lt;dependency>\n    &lt;groupId>org.springframework.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-gateway&lt;/artifactId>\n&lt;/dependency>\n&lt;!--nacos服务发现依赖-->\n&lt;dependency>\n    &lt;groupId>com.alibaba.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId>\n&lt;/dependency>\n\n编写启动类package cn.itcast.gateway;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class GatewayApplication &#123;\n\n\tpublic static void main(String[] args) &#123;\n\t\tSpringApplication.run(GatewayApplication.class, args);\n\t&#125;\n&#125;\n\n编写基础配置和路由规则创建application.yml文件，内容如下：\nserver:\n  port: 10010 # 网关端口\nspring:\n  application:\n    name: gateway # 服务名称\n  cloud:\n    nacos:\n      server-addr: localhost:8848 # nacos地址\n    gateway:\n      routes: # 网关路由配置\n        - id: user-service # 路由id（路由名称），自定义只要唯一即可\n          # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址\n          uri: lb://userservice # 路由的目标地址 lb就是负载均衡，后面跟服务名称\n          predicates: # 路由断言，也就是判断请求是否符合路由规则的条件\n            - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求\n\n我们将符合Path 规则的一切请求，都代理到 uri参数指定的地址。\n本例中，我们将 /user/**开头的请求，代理到lb://userservice，lb是负载均衡，根据服务名拉取服务列表，实现负载均衡。\n网关路由的流程图整个访问的流程如下：\n\n总结：\n网关搭建步骤：\n\n创建项目，引入nacos服务发现和gateway依赖\n\n配置application.yml，包括服务基本信息、nacos地址、路由\n\n\n路由配置包括：\n\n路由id：路由的唯一标示\n\n路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡\n\n路由断言（predicates）：判断路由的规则，\n\n路由过滤器（filters）：对请求或响应做处理\n\n\n接下来，就重点来学习路由断言和路由过滤器的详细知识\n断言工厂我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，转变为路由判断的条件\n例如Path&#x3D;&#x2F;user&#x2F;**是按照路径匹配，这个规则是由\norg.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory类来\n处理的，像这样的断言工厂在SpringCloudGateway还有十几个:\n\n\n\n名称\n说明\n示例\n\n\n\nAfter\n是某个时间点后的请求\n-  After&#x3D;2037-01-20T17:42:47.789-07:00[America&#x2F;Denver]\n\n\nBefore\n是某个时间点之前的请求\n-  Before&#x3D;2031-04-13T15:14:47.433+08:00[Asia&#x2F;Shanghai]\n\n\nBetween\n是某两个时间点之前的请求\n-  Between&#x3D;2037-01-20T17:42:47.789-07:00[America&#x2F;Denver],  2037-01-21T17:42:47.789-07:00[America&#x2F;Denver]\n\n\nCookie\n请求必须包含某些cookie\n- Cookie&#x3D;chocolate, ch.p\n\n\nHeader\n请求必须包含某些header\n- Header&#x3D;X-Request-Id, \\d+\n\n\nHost\n请求必须是访问某个host（域名）\n-  Host&#x3D;.somehost.org,.anotherhost.org\n\n\nMethod\n请求方式必须是指定方式\n- Method&#x3D;GET,POST\n\n\nPath\n请求路径必须符合指定规则\n- Path&#x3D;&#x2F;red&#x2F;{segment},&#x2F;blue&#x2F;**\n\n\nQuery\n请求参数必须包含指定参数\n- Query&#x3D;name, Jack或者-  Query&#x3D;name\n\n\nRemoteAddr\n请求者的ip必须是指定范围\n- RemoteAddr&#x3D;192.168.1.1&#x2F;24\n\n\nWeight\n权重处理\n\n\n\n过滤器工厂GatewayFilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理：\n\n路由过滤器的种类Spring提供了31种不同的路由过滤器工厂。例如：\n\n\n\n名称\n说明\n\n\n\nAddRequestHeader\n给当前请求添加一个请求头\n\n\nRemoveRequestHeader\n移除请求中的一个请求头\n\n\nAddResponseHeader\n给响应结果中添加一个响应头\n\n\nRemoveResponseHeader\n从响应结果中移除有一个响应头\n\n\nRequestRateLimiter\n限制请求的流量\n\n\n请求头过滤器下面我们以AddRequestHeader 为例来讲解。\n\n\n\n\n\n\n\n\n\n需求：给所有进入userservice的请求添加一个请求头：Truth&#x3D;itcast is freaking awesome!\n只需要修改gateway服务的application.yml文件，添加路由过滤即可：\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: user-service \n        uri: lb://userservice \n        predicates: \n        - Path=/user/** \n        filters: # 过滤器\n        - AddRequestHeader=Truth, Itcast is freaking awesome! # 添加请求头\n\n当前过滤器写在userservice路由下，因此仅仅对访问userservice的请求有效。\n默认过滤器如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下：\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: user-service \n        uri: lb://userservice \n        predicates: \n        - Path=/user/**\n      default-filters: # 默认过滤项\n      - AddRequestHeader=Truth, Itcast is freaking awesome! \n\n小结\n过滤器的作用是什么？\n① 对路由的请求或响应做加工处理，比如添加请求头\n② 配置在路由下的过滤器只对当前路由的请求生效\ndefaultFilters的作用是什么？\n① 对所有路由都生效的过滤器\n全局过滤器全局过滤器作用全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的；而GlobalFilter的逻辑需要自己写代码实现。\n定义方式是实现GlobalFilter接口\npublic interface GlobalFilter &#123;\n    /**\n     *  处理当前请求，有必要的话通过&#123;@link GatewayFilterChain&#125;将请求交给下一个过滤器处理\n     *\n     * @param exchange 请求上下文，里面可以获取Request、Response等信息\n     * @param chain 用来把请求委托给下一个过滤器 \n     * @return &#123;@code Mono&lt;Void>&#125; 返回标示当前过滤器业务结束\n     */\n    Mono&lt;Void> filter(ServerWebExchange exchange, GatewayFilterChain chain);\n&#125;\n\n在filter中编写自定义逻辑，可以实现下列功能：\n\n登录状态判断\n权限校验\n请求限流等\n\n自定义全局过滤器需求：定义全局过滤器，拦截请求，判断请求的参数是否满足下面条件：\n\n参数中是否有authorization，\n\nauthorization参数值是否为admin\n\n\n如果同时满足则放行，否则拦截\n实现：\n在gateway中定义一个过滤器：\npackage cn.itcast.gateway.filters;\n\nimport org.springframework.cloud.gateway.filter.GatewayFilterChain;\nimport org.springframework.cloud.gateway.filter.GlobalFilter;\nimport org.springframework.core.annotation.Order;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.server.ServerWebExchange;\nimport reactor.core.publisher.Mono;\n\n@Order(-1)\n@Component\npublic class AuthorizeFilter implements GlobalFilter &#123;\n    @Override\n    public Mono&lt;Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123;\n        // 1.获取请求参数\n        MultiValueMap&lt;String, String> params = exchange.getRequest().getQueryParams();\n        // 2.获取authorization参数\n        String auth = params.getFirst(\"authorization\");\n        // 3.校验\n        if (\"admin\".equals(auth)) &#123;\n            // 放行\n            return chain.filter(exchange);\n        &#125;\n        // 4.拦截\n        // 4.1.禁止访问，设置状态码\n        exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN);\n        // 4.2.结束处理\n        return exchange.getResponse().setComplete();\n    &#125;\n&#125;\n\n过滤器执行顺序请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter\n请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器：\n\n排序的规则是什么呢？\n\n每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前。\nGlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定\n路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增。\n当过滤器的order值一样时，会按照 defaultFilter &gt; 路由过滤器 &gt; GlobalFilter的顺序执行。\n\n详细内容，可以查看源码：\norg.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()方法是先加载defaultFilters，然后再加载某个route的filters，然后合并。\norg.springframework.cloud.gateway.handler.FilteringWebHandler#handle()方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链\n跨域问题跨域：域名不一致就是跨域，主要包括：\n\n域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com\n\n域名相同，端口不同：localhost:8080和localhost8081\n\n\n跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题\n解决方案：CORS，这个以前应该学习过，这里不再赘述了。不知道的小伙伴可以查看https://www.ruanyifeng.com/blog/2016/04/cors.html\n模拟跨域问题\n可以在浏览器控制台看到下面的错误：\n\n从localhost:8090访问localhost:10010，端口不同，显然是跨域的请求。\n解决跨域问题在gateway服务的application.yml文件中，添加下面的配置：\nspring:\n  cloud:\n    gateway:\n      # 。。。\n      globalcors: # 全局的跨域处理\n        add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题\n        corsConfigurations:\n          '[/**]':\n            allowedOrigins: # 允许哪些网站的跨域请求 \n              - \"http://localhost:8090\"\n            allowedMethods: # 允许的跨域ajax的请求方式\n              - \"GET\"\n              - \"POST\"\n              - \"DELETE\"\n              - \"PUT\"\n              - \"OPTIONS\"\n            allowedHeaders: \"*\" # 允许在请求中携带的头信息\n            allowCredentials: true # 是否允许携带cookie\n            maxAge: 360000 # 这次跨域检测的有效期\n\n\n\n\n\n\n\n\n\n","slug":"微服务-基础","date":"2023-06-09T08:53:51.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"eb3ef2175aab37a16c1ed443702f7a7d","title":"RabbitMQ","content":"RabbitMQ一篇就够了同步通讯：就像打电话，需要实时响应。\n异步通讯：就像发邮件，不需要马上回复。\n同步通讯存在下面的问题：\n\n总结：\n同步调用的优点：\n\n时效性较强，可以立即得到结果\n\n同步调用的问题：\n\n耦合度高\n性能和吞吐能力下降\n有额外的资源消耗\n有级联失败问题\n\n异步通讯异步调用则可以避免上述问题：\n我们以购买商品为例，用户支付后需要调用订单服务完成订单状态修改，调用物流服务，从仓库分配响应的库存并准备发货。\n在事件模式中，支付服务是事件发布者（publisher），在支付完成后只需要发布一个支付成功的事件（event），事件中带上订单id。\n订单服务和物流服务是事件订阅者（Consumer），订阅支付成功的事件，监听到事件后完成自己业务即可。\n为了解除事件发布者与订阅者之间的耦合，两者并不是直接通信，而是有一个中间人（Broker）。发布者发布事件到Broker，不关心谁来订阅事件。订阅者从Broker订阅事件，不关心谁发来的消息。\n\nBroker 是一个像数据总线一样的东西，所有的服务要接收数据和发送数据都发到这个总线上，这个总线就像协议一样，让服务间的通讯变得标准和可控。\n好处：\n\n吞吐量提升：无需等待订阅者处理完成，响应更快速\n\n故障隔离：服务没有直接调用，不存在级联失败问题\n\n调用间没有阻塞，不会造成无效的资源占用\n\n耦合度极低，每个服务都可以灵活插拔，可替换\n\n流量削峰：不管发布事件的流量波动多大，都由Broker接收，订阅者可以按照自己的速度去处理事件\n\n\n缺点：\n\n架构复杂了，业务没有明显的流程线，不好管理\n需要依赖于Broker的可靠、安全、性能\n\n技术对比：MQ，中文是消息队列（MessageQueue），字面来看就是存放消息的队列。也就是事件驱动架构中的Broker。\n比较常见的MQ实现：\n\nActiveMQ\nRabbitMQ\nRocketMQ\nKafka\n\n几种常见MQ的对比：\n\n\n\n\nRabbitMQ\nActiveMQ\nRocketMQ\nKafka\n\n\n\n公司&#x2F;社区\nRabbit\nApache\n阿里\nApache\n\n\n开发语言\nErlang\nJava\nJava\nScala&amp;Java\n\n\n协议支持\nAMQP，XMPP，SMTP，STOMP\nOpenWire,STOMP，REST,XMPP,AMQP\n自定义协议\n自定义协议\n\n\n可用性\n高\n一般\n高\n高\n\n\n单机吞吐量\n一般\n差\n高\n非常高\n\n\n消息延迟\n微秒级\n毫秒级\n毫秒级\n毫秒以内\n\n\n消息可靠性\n高\n一般\n高\n一般\n\n\n追求可用性：Kafka、 RocketMQ 、RabbitMQ\n追求可靠性：RabbitMQ、RocketMQ\n追求吞吐能力：RocketMQ、Kafka\n追求消息低延迟：RabbitMQ、Kafka\n快速入门MQ的基本结构：\n\nRabbitMQ中的一些角色：\n\npublisher：生产者\nconsumer：消费者\nexchange：交换机，负责消息路由\nqueue：队列，存储消息\nvirtualHost：虚拟主机，隔离不同租户的exchange、queue、消息的隔离\n\nRabbitMQ消息模型RabbitMQ官方提供了5个不同的Demo示例，对应了不同的消息模型：\n\n入门案例练习项目结构如下：\n\n包括三部分：\n\nmq-demo：父工程，管理项目依赖\npublisher：消息的发送者\nconsumer：消息的消费者\n\n简单队列模式的模型图：\n \n官方的HelloWorld是基于最基础的消息队列模型来实现的，只包括三个角色：\n\npublisher：消息发布者，将消息发送到队列queue\nqueue：消息队列，负责接受并缓存消息\nconsumer：订阅队列，处理队列中的消息\n\npublisher实现步骤：\n\n建立连接\n创建Channel\n声明队列\n发送消息\n关闭连接和channel\n\n代码实现：\nimport com.rabbitmq.client.Channel;\nimport com.rabbitmq.client.Connection;\nimport com.rabbitmq.client.ConnectionFactory;\nimport org.junit.Test;\n\nimport java.io.IOException;\nimport java.util.concurrent.TimeoutException;\n\npublic class PublisherTest &#123;\n    @Test\n    public void testSendMessage() throws IOException, TimeoutException &#123;\n        // 1.建立连接\n        ConnectionFactory factory = new ConnectionFactory();\n        // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码\n        factory.setHost(\"192.168.150.101\");\n        factory.setPort(5672);\n        factory.setVirtualHost(\"/\");\n        factory.setUsername(\"itcast\");\n        factory.setPassword(\"123321\");\n        // 1.2.建立连接\n        Connection connection = factory.newConnection();\n\n        // 2.创建通道Channel\n        Channel channel = connection.createChannel();\n\t\t/**\n\t\t * 参数说明：\n\t\t * 1、第一个为队列/交换器名称\n \t\t * 2、是否持久化，默认true\n\t\t * 3、消费者断开时是否删除\n \t\t * 4、消息其他参数    实际开发需要持久化，只需要输入交换器的名称，其他用默认的\n \t\t * 5、\n \t\t*/\n        // 3.创建队列\n        String queueName = \"simple.queue\";\n        channel.queueDeclare(queueName, false, false, false, null);\n\n        // 4.发送消息\n        String message = \"hello, rabbitmq!\";\n        \t\t\t\t\t\t\t//参数 3 设置为存储行为，2表示存储纯文本到磁盘；\t\n        channel.basicPublish(\"\", queueName, null, message.getBytes());\n        System.out.println(\"发送消息成功：【\" + message + \"】\");\n\n        // 5.关闭通道和连接\n        channel.close();\n        connection.close();\n\n    &#125;\n&#125;\n\nconsumer实现步骤：\n\n建立连接\n创建Channel\n声明队列\n订阅消息\n\n代码实现：\nimport com.rabbitmq.client.*;\nimport java.io.IOException;\nimport java.util.concurrent.TimeoutException;\n\npublic class ConsumerTest &#123;\n\n    public static void main(String[] args) throws IOException, TimeoutException &#123;\n        // 1.建立连接\n        ConnectionFactory factory = new ConnectionFactory();\n        // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码\n        factory.setHost(\"192.168.150.101\");\n        factory.setPort(5672);\n        factory.setVirtualHost(\"/\");\n        factory.setUsername(\"itcast\");\n        factory.setPassword(\"123321\");\n        // 1.2.建立连接\n        Connection connection = factory.newConnection();\n\n        // 2.创建通道Channel\n        Channel channel = connection.createChannel();\n\n        // 3.创建队列\n        String queueName = \"simple.queue\";\n        channel.queueDeclare(queueName, false, false, false, null);\n\n        // 4.订阅消息\n        channel.basicConsume(queueName, true, new DefaultConsumer(channel)&#123;\n            @Override\n            public void handleDelivery(String consumerTag, Envelope envelope,\n                                       AMQP.BasicProperties properties, byte[] body) throws IOException &#123;\n                // 5.处理消息\n                String message = new String(body);\n                System.out.println(\"接收到消息：【\" + message + \"】\");\n            &#125;\n        &#125;);\n        System.out.println(\"等待接收消息。。。。\");\n    &#125;\n&#125;\n\n小结\n基本消息队列的消息发送流程：\n\n建立connection\n\n创建channel\n\n利用channel声明队列\n\n利用channel向队列发送消息\n\n\n基本消息队列的消息接收流程：\n\n建立connection\n\n创建channel\n\n利用channel声明队列\n\n定义consumer的消费行为handleDelivery()\n\n利用channel将消费者与队列绑定\n\n\nSpringAMQPSpringAMQP是基于RabbitMQ封装的一套模板，并且还利用SpringBoot对其实现了自动装配，使用起来非常方便。\nSpringAmqp的官方地址：https://spring.io/projects/spring-amqp\n\n\nSpringAMQP提供了三个功能：\n\n自动声明队列、交换机及其绑定关系\n基于注解的监听器模式，异步接收消息\n封装了RabbitTemplate工具，用于发送消息\n\nBasic Queue 简单队列模型在父工程mq-demo中引入依赖\n&lt;!--AMQP依赖，包含RabbitMQ-->\n&lt;dependency>\n    &lt;groupId>org.springframework.boot&lt;/groupId>\n    &lt;artifactId>spring-boot-starter-amqp&lt;/artifactId>\n&lt;/dependency>\n\n消息发送首先配置MQ地址，在publisher服务的application.yml中添加配置：\nspring:\n  rabbitmq:\n    host: 192.168.150.101 # 主机名\n    port: 5672 # 端口\n    virtual-host: / # 虚拟主机\n    username: itcast # 用户名\n    password: 123321 # 密码\n\n然后在publisher服务中编写测试类SpringAmqpTest，并利用RabbitTemplate实现消息发送：\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringAmqpTest &#123;\n\n    @Autowired\n    private RabbitTemplate rabbitTemplate;\n\n    @Test\n    public void testSimpleQueue() &#123;\n        // 队列名称\n        String queueName = \"simple.queue\";\n        // 消息\n        String message = \"hello, spring amqp!\";\n        // 发送消息\n        rabbitTemplate.convertAndSend(queueName, message);\n    &#125;\n&#125;\n\n消息接收首先配置MQ地址，在consumer服务的application.yml中添加配置：\nspring:\n  rabbitmq:\n    host: 192.168.150.101 # 主机名\n    port: 5672 # 端口\n    virtual-host: / # 虚拟主机\n    username: itcast # 用户名\n    password: 123321 # 密码\n\n然后在consumer服务中新建一个类SpringRabbitListener，代码如下：\nimport org.springframework.amqp.rabbit.annotation.RabbitListener;\nimport org.springframework.stereotype.Component;\n\n@Component\npublic class SpringRabbitListener &#123;\n\n    @RabbitListener(queues = \"simple.queue\")\n    public void listenSimpleQueueMessage(String msg) throws InterruptedException &#123;\n        System.out.println(\"spring 消费者接收到消息：【\" + msg + \"】\");\n    &#125;\n&#125;\n\n启动consumer服务，然后在publisher服务中运行测试代码，发送MQ消息\nWorkQueueWork queues，也被称为（Task queues），任务模型。简单来说就是让多个消费者绑定到一个队列，共同消费队列中的消息。\n\n当消息处理比较耗时的时候，可能生产消息的速度会远远大于消息的消费速度。长此以往，消息就会堆积越来越多，无法及时处理。此时就可以使用work 模型，多个消费者共同处理消息处理，速度就能大大提高了。\n消息发送循环发送，模拟大量消息堆积现象。\n在publisher服务中的SpringAmqpTest类中添加一个测试方法：\n/**\n     * workQueue\n     * 向队列中不停发送消息，模拟消息堆积。\n     */\n@Test\npublic void testWorkQueue() throws InterruptedException &#123;\n    // 队列名称\n    String queueName = \"simple.queue\";\n    // 消息\n    String message = \"hello, message_\";\n    for (int i = 0; i &lt; 50; i++) &#123;\n        // 发送消息\n        rabbitTemplate.convertAndSend(queueName, message + i);\n        Thread.sleep(20);\n    &#125;\n&#125;\n\n消息接收要模拟多个消费者绑定同一个队列，我们在consumer服务的SpringRabbitListener中添加2个新的方法：\n@RabbitListener(queues = \"simple.queue\")\npublic void listenWorkQueue1(String msg) throws InterruptedException &#123;\n    System.out.println(\"消费者1接收到消息：【\" + msg + \"】\" + LocalTime.now());\n    Thread.sleep(20);\n&#125;\n\n@RabbitListener(queues = \"simple.queue\")\npublic void listenWorkQueue2(String msg) throws InterruptedException &#123;\n    System.err.println(\"消费者2........接收到消息：【\" + msg + \"】\" + LocalTime.now());\n    Thread.sleep(200);\n&#125;\n\n注意到这个消费者sleep模拟任务耗时。\n启动ConsumerApplication后，在执行publisher服务中刚刚编写的发送测试方法testWorkQueue。可以看到消费者1很快完成了自己的25条消息。消费者2却在缓慢的处理自己的25条消息。也就是说消息是平均分配给每个消费者，并没有考虑到消费者的处理能力。\n能者多劳在spring中有一个简单的配置，可以解决这个问题。我们修改consumer服务的application.yml文件，添加配置：\nspring:\n  rabbitmq:\n    listener:\n      simple:\n        prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息\n\nWork模型的使用：\n\n多个消费者绑定到一个队列，同一条消息只会被一个消费者处理\n通过设置prefetch来控制消费者预取的消息数量\n\n我们也知道每次发送消息都要指定交换机，为啥这里没有声明交换机，也没有使用routing_key，消息也走通了？\n这是因为RabbitMQ还有一个默认交换机（default exchange）：\n实际上是一个由消息代理预先声明好的没有名字（名字为空字符串）的直连交换机（direct exchange）。它有一个特殊的属性使得它对于简单应用特别有用处：那就是每个新建队列（queue）都会自动绑定到默认交换机上，绑定的路由键（routing key）名称与队列名称相同。\n举个栗子：当你声明了一个名为”hello”的队列，AMQP代理会自动将其绑定到默认交换机上，绑定（binding）的路由键名称也是为”hello”。因此，当携带着名为”hello”的路由键的消息被发送到默认交换机的时候，此消息会被默认交换机路由至名为”hello”的队列中。\n换句话说，默认交换机看起来貌似能够直接将消息投递给队列，尽管技术上并没有做相关的操作。所以我们发送消息时routing_key直接为队列名称了。这里就是使用了默认的直达交换机。\n发布&#x2F;订阅发布订阅的模型如图：\n\n可以看到，在订阅模型中，多了一个exchange角色，而且过程略有变化：\n\nPublisher：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机）\nExchange：交换机，图中的X。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有以下3种类型：\nFanout：广播，将消息交给所有绑定到交换机的队列\nDirect：定向，把消息交给符合指定routing key 的队列\nTopic：通配符，把消息交给符合routing pattern（路由模式） 的队列\n\n\nConsumer：消费者和以前一样，订阅队列，没有变化\nQueue：消息队列和以前一样，接收消息、缓存消息。\n\nExchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！\nFanoutFanout，英文翻译是扇出，我觉得在MQ中叫广播更合适。\n\n在广播模式下，消息发送流程是这样的：\n\n1）  可以有多个队列\n2）  每个队列都要绑定到Exchange（交换机）\n3）  生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定\n4）  交换机把消息发送给绑定过的所有队列\n5）  订阅队列的消费者都能拿到消息\n\n设计思路：\n\n创建一个交换机 itcast.fanout，类型是Fanout\n创建两个队列fanout.queue1和fanout.queue2，绑定到交换机itcast.fanout\n\n\n声明队列和交换机Spring提供了一个接口Exchange，来表示所有不同类型的交换机：\n\n在consumer中创建一个类，声明队列和交换机：\nimport org.springframework.amqp.core.Binding;\nimport org.springframework.amqp.core.BindingBuilder;\nimport org.springframework.amqp.core.FanoutExchange;\nimport org.springframework.amqp.core.Queue;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class FanoutConfig &#123;\n    /**\n     * 声明交换机\n     * @return Fanout类型交换机\n     */\n    @Bean\n    public FanoutExchange fanoutExchange()&#123;\n        return new FanoutExchange(\"itcast.fanout\");\n    &#125;\n\n    /**\n     * 第1个队列\n     */\n    @Bean\n    public Queue fanoutQueue1()&#123;\n        return new Queue(\"fanout.queue1\");\n    &#125;\n\n    /**\n     * 绑定队列和交换机\n     */\n    @Bean\n    public Binding bindingQueue1(Queue fanoutQueue1, FanoutExchange fanoutExchange)&#123;\n        return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange);\n    &#125;\n\n    /**\n     * 第2个队列\n     */\n    @Bean\n    public Queue fanoutQueue2()&#123;\n        return new Queue(\"fanout.queue2\");\n    &#125;\n\n    /**\n     * 绑定队列和交换机\n     */\n    @Bean\n    public Binding bindingQueue2(Queue fanoutQueue2, FanoutExchange fanoutExchange)&#123;\n        return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange);\n    &#125;\n&#125;\n\n消息发送在publisher服务的SpringAmqpTest类中添加测试方法：\n@Test\npublic void testFanoutExchange() &#123;\n    // 队列名称\n    String exchangeName = \"itcast.fanout\";\n    // 消息\n    String message = \"hello, everyone!\";\n    rabbitTemplate.convertAndSend(exchangeName, \"\", message);\n&#125;\n\n消息接收在consumer服务的SpringRabbitListener中添加两个方法，作为消费者：\n@RabbitListener(queues = \"fanout.queue1\")\npublic void listenFanoutQueue1(String msg) &#123;\n    System.out.println(\"消费者1接收到Fanout消息：【\" + msg + \"】\");\n&#125;\n\n@RabbitListener(queues = \"fanout.queue2\")\npublic void listenFanoutQueue2(String msg) &#123;\n    System.out.println(\"消费者2接收到Fanout消息：【\" + msg + \"】\");\n&#125;\n\n交换机的作用是什么？\n\n接收publisher发送的消息\n将消息按照规则路由到与之绑定的队列\n不能缓存消息，路由失败，消息丢失\nFanoutExchange的会将消息路由到每个绑定的队列\n\n声明队列、交换机、绑定关系的Bean是什么？\n\nQueue\nFanoutExchange\nBinding\n\nDirect在Fanout模式中，一条消息，会被所有订阅的队列都消费。但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。\n\n 在Direct模型下：\n\n队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key）\n消息的发送方在 向 Exchange发送消息时，也必须指定消息的 RoutingKey。\nExchange不再把消息交给每一个绑定的队列，而是根据消息的Routing Key进行判断，只有队列的Routingkey与消息的 Routing key完全一致，才会接收到消息\n\n案例需求如下：\n\n利用@RabbitListener声明Exchange、Queue、RoutingKey\n\n在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2\n\n在publisher中编写测试方法，向itcast. direct发送消息\n\n\n\n基于注解声明队列和交换机基于@Bean的方式声明队列和交换机比较麻烦，Spring还提供了基于注解方式来声明。\n在consumer的SpringRabbitListener中添加两个消费者，同时基于注解来声明队列和交换机：\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = \"direct.queue1\"),\n    exchange = @Exchange(name = \"itcast.direct\", type = ExchangeTypes.DIRECT),\n    key = &#123;\"red\", \"blue\"&#125;\n))\npublic void listenDirectQueue1(String msg)&#123;\n    System.out.println(\"消费者接收到direct.queue1的消息：【\" + msg + \"】\");\n&#125;\n\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = \"direct.queue2\"),\n    exchange = @Exchange(name = \"itcast.direct\", type = ExchangeTypes.DIRECT),\n    key = &#123;\"red\", \"yellow\"&#125;\n))\npublic void listenDirectQueue2(String msg)&#123;\n    System.out.println(\"消费者接收到direct.queue2的消息：【\" + msg + \"】\");\n&#125;\n\n消息发送在publisher服务的SpringAmqpTest类中添加测试方法：\n@Test\npublic void testSendDirectExchange() &#123;\n    // 交换机名称\n    String exchangeName = \"itcast.direct\";\n    // 消息\n    String message = \"红色警报！日本乱排核废水，导致海洋生物变异，惊现哥斯拉！\";\n    // 发送消息\n    rabbitTemplate.convertAndSend(exchangeName, \"red\", message);\n&#125;\n\n小结\n描述下Direct交换机与Fanout交换机的差异？\n\nFanout交换机将消息路由给每一个与之绑定的队列\nDirect交换机根据RoutingKey判断路由给哪个队列\n如果多个队列具有相同的RoutingKey，则与Fanout功能类似\n\n基于@RabbitListener注解声明队列和交换机有哪些常见注解？\n\n@Queue\n@Exchange\n\nTopicTopic类型的Exchange与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！\nRoutingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert\n 通配符规则：\n#：匹配一个或多个词\n*：匹配不多不少恰好1个词\n举例：\nitem.#：能够匹配item.spu.insert 或者 item.spu\nitem.*：只能匹配item.spu\n​     \n图示：\n \n解释：\n\nQueue1：绑定的是china.# ，因此凡是以 china.开头的routing key 都会被匹配到。包括china.news和china.weather\nQueue2：绑定的是#.news ，因此凡是以 .news结尾的 routing key 都会被匹配。包括china.news和japan.news\n\n案例需求：\n实现思路如下：\n\n并利用@RabbitListener声明Exchange、Queue、RoutingKey\n\n在consumer服务中，编写两个消费者方法，分别监听topic.queue1和topic.queue2\n\n在publisher中编写测试方法，向itcast. topic发送消息\n\n\n\n消息发送在publisher服务的SpringAmqpTest类中添加测试方法：\n/**\n     * topicExchange\n     */\n@Test\npublic void testSendTopicExchange() &#123;\n    // 交换机名称\n    String exchangeName = \"itcast.topic\";\n    // 消息\n    String message = \"喜报！孙悟空大战哥斯拉，胜!\";\n    // 发送消息\n    rabbitTemplate.convertAndSend(exchangeName, \"china.news\", message);\n&#125;\n\n消息接收在consumer服务的SpringRabbitListener中添加方法：\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = \"topic.queue1\"),\n    exchange = @Exchange(name = \"itcast.topic\", type = ExchangeTypes.TOPIC),\n    key = \"china.#\"\n))\npublic void listenTopicQueue1(String msg)&#123;\n    System.out.println(\"消费者接收到topic.queue1的消息：【\" + msg + \"】\");\n&#125;\n\n@RabbitListener(bindings = @QueueBinding(\n    value = @Queue(name = \"topic.queue2\"),\n    exchange = @Exchange(name = \"itcast.topic\", type = ExchangeTypes.TOPIC),\n    key = \"#.news\"\n))\npublic void listenTopicQueue2(String msg)&#123;\n    System.out.println(\"消费者接收到topic.queue2的消息：【\" + msg + \"】\");\n&#125;\n\n小结\n描述下Direct交换机与Topic交换机的差异？\n\nTopic交换机接收的消息RoutingKey必须是多个单词，以 **.** 分割\nTopic交换机与队列绑定时的bindingKey可以指定通配符\n#：代表0个或多个词\n*：代表1个词\n\n消息转换器之前说过，Spring会把你发送的消息序列化为字节发送给MQ，接收消息的时候，还会把字节反序列化为Java对象。\n\n只不过，默认情况下Spring采用的序列化方式是JDK序列化。JDK序列化存在下列问题：\n\n数据体积过大\n有安全漏洞\n可读性差\n\n配置JSON转换器显然，JDK序列化方式并不合适。我们希望消息体的体积更小、可读性更高，因此可以使用JSON方式来做序列化和反序列化。\n在publisher和consumer两个服务中都引入依赖：\n&lt;dependency>\n    &lt;groupId>com.fasterxml.jackson.dataformat&lt;/groupId>\n    &lt;artifactId>jackson-dataformat-xml&lt;/artifactId>\n    &lt;version>2.9.10&lt;/version>\n&lt;/dependency>\n\n配置消息转换器。\n在启动类中添加一个Bean即可：\n@Bean\npublic MessageConverter jsonMessageConverter()&#123;\n    return new Jackson2JsonMessageConverter();\n&#125;\n\n@RabbitListener注解提供的灵活性。您可以根据实际需要选择合适的消息转换器和入参类型。\n@RabbitListener注解支持多种消息转换器（MessageConverter），可以将接收到的消息转换为不同类型的对象。监听方法的入参为String类型，表示使用了SimpleMessageConverter将接收到的消息转换为String类型。在监听方法入参包括Message和Channel两个类型，表示接收到的消息未经转换，仍然是Message类型，同时还可以获取到Channel对象以进行手动ACK操作。\nRabbitMQ-原理基本概念提到RabbitMQ，就不得不提AMQP协议。AMQP协议是具有现代特征的二进制协议。是一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。\n先了解一下AMQP协议中间的几个重要概念：\n\nServer：接收客户端的连接，实现AMQP实体服务。\nConnection：连接，应用程序与Server的网络连接，TCP连接。\nChannel：信道，消息读写等操作在信道中进行。客户端可以建立多个信道，每个信道代表一个会话任务。\nMessage：消息，应用程序和服务器之间传送的数据，消息可以非常简单，也可以很复杂。由Properties和Body组成。Properties为外包装，可以对消息进行修饰，比如消息的优先级、延迟等高级特性；Body就是消息体内容。\nVirtual Host：虚拟主机，用于逻辑隔离。一个虚拟主机里面可以有若干个Exchange和Queue，同一个虚拟主机里面不能有相同名称的Exchange或Queue。\nExchange：交换器，接收消息，按照路由规则将消息路由到一个或者多个队列。如果路由不到，或者返回给生产者，或者直接丢弃。RabbitMQ常用的交换器常用类型有direct、topic、fanout、headers四种，后面详细介绍。\nBinding：绑定，交换器和消息队列之间的虚拟连接，绑定中可以包含一个或者多个RoutingKey。\nRoutingKey：路由键，生产者将消息发送给交换器的时候，会发送一个RoutingKey，用来指定路由规则，这样交换器就知道把消息发送到哪个队列。路由键通常为一个“ . ”分割的字符串，例如“com.rabbitmq”。\nQueue：消息队列，用来保存消息，供消费者消费。\n\n工作原理AMQP 协议模型由三部分组成：生产者、消费者和服务端，执行流程如下：\n\n生产者是连接到 Server，建立一个连接，开启一个信道。\n生产者声明交换器和队列，设置相关属性，并通过路由键将交换器和队列进行绑定。\n消费者也需要进行建立连接，开启信道等操作，便于接收消息。\n生产者发送消息，发送到服务端中的虚拟主机。\n虚拟主机中的交换器根据路由键选择路由规则，发送到不同的消息队列中。\n订阅了消息队列的消费者就可以获取到消息，进行消费。\n\n\n消费原理我们先看几个基本概念：\n\nbroker：每个节点运行的服务程序，功能为维护该节点的队列的增删以及转发队列操作请求。\nmaster queue：每个队列都分为一个主队列和若干个镜像队列。\nmirror queue：镜像队列，作为master queue的备份。在master queue所在节点挂掉之后，系统把mirror queue提升为master queue，负责处理客户端队列操作请求。注意，mirror queue只做镜像，设计目的不是为了承担客户端读写压力。\n\n集群中有两个节点，每个节点上有一个broker，每个broker负责本机上队列的维护，并且borker之间可以互相通信。集群中有两个队列A和B，每个队列都分为master queue和mirror queue（备份）。那么队列上的生产消费怎么实现的呢？\n\n对于消费队列，如下图有两个consumer消费队列A，这两个consumer连在了集群的不同机器上。RabbitMQ集群中的任何一个节点都拥有集群上所有队列的元信息，所以连接到集群中的任何一个节点都可以，主要区别在于有的consumer连在master queue所在节点，有的连在非master queue节点上。\n因为mirror queue要和master queue保持一致，故需要同步机制，正因为一致性的限制，导致所有的读写操作都必须都操作在master queue上（想想，为啥读也要从master queue中读？和数据库读写分离是不一样的），然后由master节点同步操作到mirror queue所在的节点。即使consumer连接到了非master queue节点，该consumer的操作也会被路由到master queue所在的节点上，这样才能进行消费。\n\n对于生成队列，原理和消费一样，如果连接到非 master queue 节点，则路由过去。\n\n\n\n\n\n\n\n\n\n\n所以，到这里小伙伴们就可以看到 RabbitMQ的不足：由于master queue单节点，导致性能瓶颈，吞吐量受限。虽然为了提高性能，内部使用了Erlang这个语言实现，但是终究摆脱不了架构设计上的致命缺陷。\n高级特性过期时间Time To Live，也就是生存时间，是一条消息在队列中的最大存活时间，单位是毫秒，下面看看RabbitMQ过期时间特性：\n\nRabbitMQ可以对消息和队列设置TTL。\nRabbitMQ支持设置消息的过期时间，在消息发送的时候可以进行指定，每条消息的过期时间可以不同。\nRabbitMQ支持设置队列的过期时间，从消息入队列开始计算，直到超过了队列的超时时间配置，那么消息会变成死信，自动清除。\n如果两种方式一起使用，则过期时间以两者中较小的那个数值为准。\n当然也可以不设置TTL，不设置表示消息不会过期；如果设置为0，则表示除非此时可以直接将消息投递到消费者，否则该消息将被立即丢弃。\n\n消息确认为了保证消息从队列可靠地到达消费者，RabbitMQ提供了消息确认机制。\n生产者发送完后，能够获取来自消息代理的确定@Component\npublic class ConfirmCallbackDemo implements RabbitTemplate.ConfirmCallback&#123;\n    /**\n     * 通过实现 ConfirmCallback 接口，消息发送到 Broker 后触发回调，确认消息是否到达 Broker 服务器，\t\t 也就是只确认是否正确到达 Exchange 中\n     * 1.如果消息没有到exchange,则 ack=false\n     * 2.如果消息到达exchange,则 ack=true\n     * @param correlationData\n     * @param ack\n     * @param cause\n     */\n    public void confirm(@Nullable CorrelationData correlationData, boolean ack, @Nullable String cause) &#123;\n        System.out.println(\"[confirm]: id=\" + correlationData.getId());\n        if(ack)&#123;// 成功接收\n            //todo 成功处理逻辑\n        &#125;else&#123;\n            // 失败原因\n            System.out.println(\"[confirm]: cause=\" + cause);\n            //todo 失败处理逻辑\n        &#125;\n    &#125;\n&#125;\n\n\n路由无法将消息投递到任何一个列队上，默认情况下会被丢弃，特定场景中，生产者需要被感知@Component\npublic class ReturnCallbackDemo implements RabbitTemplate.ReturnCallback&#123;\n    /**\n     * 当消息从交换机到队列失败时，该方法被调用。（若成功，则不调用）\n     * 需要注意的是：\n     *\t\t该方法调用后，ConfirmCallBack中的confirm方法也会被调用，且ack = true\n     * @param message\n     * @param replyCode\n     * @param replyText\n     * @param exchange\n     * @param routingKey\n     */\n    public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123;\n        System.out.println(\"消息主体 message : \"+message);\n        System.out.println(\"消息主体 message : \"+replyCode);\n        System.out.println(\"描述：\"+replyText);\n        System.out.println(\"消息使用的交换器 exchange : \"+exchange);\n        System.out.println(\"消息使用的路由键 routing : \"+routingKey);\n    &#125;\n&#125;\n\n\n消费者确定机制保证消息被消费@Component\npublic class MessageHandler &#123;\n    \t\t\t\t\t\t\t\t//为保证可靠性，不使用自动确认。\n    @RabbitListener(queues = \"directqueue\", autoAck = false)\n    public void handleMessage(Message message, Channel channel) throws Exception&#123;\n        try &#123;\n            System.out.println(\"消费消息\");\n            System.out.println(new String(message.getBody()));\n            channel.basicAck(message.getMessageProperties().getDeliveryTag(), true);\n        &#125; catch (Exception e)&#123;\n            channel.basicNack(message.getMessageProperties().getDeliveryTag(), false,true);\n        &#125;\n    &#125;\n&#125;\n\n\n消费者订阅队列的时候，可以指定autoAck参数，当autoAck为true的时候，RabbitMQ采用自动确认模式，RabbitMQ自动把发送出去的消息设置为确认，然后从内存或者硬盘中删除，而不管消费者是否真正消费到了这些消息。当autoAck为false的时候，RabbitMQ会等待消费者回复的确认信号，收到确认信号之后才从内存或者磁盘中删除消息。\n消息确认机制是RabbitMQ消息可靠性投递的基础，只要设置autoAck参数为false，消费者就有足够的时间处理消息，不用担心处理消息的过程中消费者进程挂掉后消息丢失的问题。\n持久化消息的可靠性是RabbitMQ的一大特色，那么RabbitMQ是如何保证消息可靠性的呢？答案就是消息持久化。持久化可以防止在异常情况下丢失数据。RabbitMQ的持久化分为三个部分：交换器持久化、队列持久化和消息的持久化。\n交换器持久化可以通过在声明队列时将durable参数设置为true。如果交换器不设置持久化，那么在RabbitMQ服务重启之后，相关的交换器元数据会丢失，不过消息不会丢失，只是不能将消息发送到这个交换器了。\n队列的持久化能保证其本身的元数据不会因异常情况而丢失，但是不能保证内部所存储的消息不会丢失。要确保消息不会丢失，需要将其设置为持久化。队列的持久化可以通过在声明队列时将durable参数设置为true。\n设置了队列和消息的持久化，当RabbitMQ服务重启之后，消息依然存在。如果只设置队列持久化或者消息持久化，重启之后消息都会消失。\n当然，也可以将所有的消息都设置为持久化，但是这样做会影响RabbitMQ的性能，因为磁盘的写入速度比内存的写入要慢得多。\n对于可靠性不是那么高的消息可以不采用持久化处理以提高整体的吞吐量。鱼和熊掌不可兼得，关键在于选择和取舍。在实际中，需要根据实际情况在可靠性和吞吐量之间做一个权衡。\n死信队列当消息在一个队列中变成死信之后，他能被重新发送到另一个交换器中，这个交换器成为死信交换器，与该交换器绑定的队列称为死信队列。\n消息变成死信有下面几种情况：\n\n消息被拒绝，无法被出来\n消息过期\n队列达到最大长度\n\nDLX（死信交换器）也是一个正常的交换器，和一般的交换器没有区别，他能在任何的队列上面被指定，实际上就是设置某个队列的属性。当这个队列中有死信的时候，RabbitMQ会自动将这个消息重新发送到设置的交换器上，进而被路由到另一个队列，我们可以监听这个队列中消息做相应的处理。\n死信队列有什么用？当发生异常的时候，消息不能够被消费者正常消费，被加入到了死信队列中。后续的程序可以根据死信队列中的内容分析当时发生的异常，进而改善和优化系统。\n延迟队列一般的队列，消息一旦进入队列就会被消费者立即消费。延迟队列就是进入该队列的消息会被消费者延迟消费，延迟队列中存储的对象是的延迟消息，“延迟消息”是指当消息被发送以后，等待特定的时间后，消费者才能拿到这个消息进行消费。\n延迟队列用于需要延迟工作的场景。最常见的使用场景：淘宝或者天猫我们都使用过，用户在下单之后通常有30分钟的时间进行支付，如果这30分钟之内没有支付成功，那么订单就会自动取消。\n除了延迟消费，延迟队列的典型应用场景还有延迟重试。比如消费者从队列里面消费消息失败了，可以延迟一段时间以后进行重试。\n特性分析这里才是内容的重点，不仅需要知道Rabbit的特性，还需要知道支持这些特性的原因：\n\n消息路由（支持）：RabbitMQ可以通过不同的交换器支持不同种类的消息路由；\n消息有序（不支持）：当消费消息时，如果消费失败，消息会被放回队列，然后重新消费，这样会导致消息无序；\n消息时序（非常好）：通过延时队列，可以指定消息的延时时间，过期时间TTL等；\n容错处理（非常好）：通过交付重试和死信交换器（DLX）来处理消息处理故障；\n伸缩（一般）：伸缩其实没有非常智能，因为即使伸缩了，master queue还是只有一个，负载还是只有这一个master queue去抗，所以我理解RabbitMQ的伸缩很弱（个人理解）。\n持久化（不太好）：没有消费的消息，可以支持持久化，这个是为了保证机器宕机时消息可以恢复，但是消费过的消息，就会被马上删除，因为RabbitMQ设计时，就不是为了去存储历史数据的。\n消息回溯（不支持）：因为消息不支持永久保存，所以自然就不支持回溯。\n高吞吐（中等）：因为所有的请求的执行，最后都是在master queue，它的这个设计，导致单机性能达不到十万级的标准。\n\nMQ面试篇1.为什么选择了RabbitMQ而不是其它的MQ？如图：\n\n话术：\nRabbitMQ基于面向并发的语言Erlang开发，吞吐量不如Kafka，而且消息可靠性较好，并且消息延迟极低，集群搭建比较方便。支持多种协议，并且有各种语言的客户端，比较灵活。Spring对RabbitMQ的支持也比较好，使用起来比较方便，比较符合我们公司的需求。\n2.RabbitMQ如何确保消息的不丢失？话术：\nRabbitMQ针对消息传递过程中可能发生问题的各个地方，给出了针对性的解决方案：\n\n生产者发送消息时可能因为网络问题导致消息没有到达交换机：\nRabbitMQ提供了publisher confirm机制\n生产者发送消息后，可以编写ConfirmCallback函数\n消息成功到达交换机后，RabbitMQ会调用ConfirmCallback通知消息的发送者，返回ACK\n消息如果未到达交换机，RabbitMQ也会调用ConfirmCallback通知消息的发送者，返回NACK\n消息超时未发送成功也会抛出异常\n\n\n\n\n消息到达交换机后，如果未能到达队列，也会导致消息丢失：\nRabbitMQ提供了publisher return机制\n生产者可以定义ReturnCallback函数\n消息到达交换机，未到达队列，RabbitMQ会调用ReturnCallback通知发送者，告知失败原因\n\n\n\n\n消息到达队列后，MQ宕机也可能导致丢失消息：\nRabbitMQ提供了持久化功能，集群的主从备份功能\n消息持久化，RabbitMQ会将交换机、队列、消息持久化到磁盘，宕机重启可以恢复消息\n镜像集群，仲裁队列，都可以提供主从备份功能，主节点宕机，从节点会自动切换为主，数据依然在\n\n\n\n\n消息投递给消费者后，如果消费者处理不当，也可能导致消息丢失\nSpringAMQP基于RabbitMQ提供了消费者确认机制、消费者重试机制，消费者失败处理策略：\n消费者的确认机制：\n消费者处理消息成功，未出现异常时，Spring返回ACK给RabbitMQ，消息才被移除\n消费者处理消息失败，抛出异常，宕机，Spring返回NACK或者不返回结果，消息不被异常\n\n\n消费者重试机制：\n默认情况下，消费者处理失败时，消息会再次回到MQ队列，然后投递给其它消费者。Spring提供的消费者重试机制，则是在处理失败后不返回NACK，而是直接在消费者本地重试。多次重试都失败后，则按照消费者失败处理策略来处理消息。避免了消息频繁入队带来的额外压力。\n\n\n消费者失败策略：\n当消费者多次本地重试失败时，消息默认会丢弃。\nSpring提供了Republish策略，在多次重试都失败，耗尽重试次数后，将消息重新投递给指定的异常交换机，并且会携带上异常栈信息，帮助定位问题。\n\n\n\n\n\n\n\n3.RabbitMQ如何避免消息堆积？话术：\n消息堆积问题产生的原因往往是因为消息发送的速度超过了消费者消息处理的速度。因此解决方案无外乎以下三点：\n\n提高消费者处理速度\n增加更多消费者\n增加队列消息存储上限\n\n1）提高消费者处理速度\n消费者处理速度是由业务代码决定的，所以我们能做的事情包括：\n\n尽可能优化业务代码，提高业务性能\n接收到消息后，开启线程池，并发处理多个消息\n\n优点：成本低，改改代码即可\n缺点：开启线程池会带来额外的性能开销，对于高频、低时延的任务不合适。推荐任务执行周期较长的业务。\n2）增加更多消费者\n一个队列绑定多个消费者，共同争抢任务，自然可以提供消息处理的速度。\n优点：能用钱解决的问题都不是问题。实现简单粗暴\n缺点：问题是没有钱。成本太高\n3）增加队列消息存储上限\n在RabbitMQ的1.8版本后，加入了新的队列模式：Lazy Queue\n这种队列不会将消息保存在内存中，而是在收到消息后直接写入磁盘中，理论上没有存储上限。可以解决消息堆积问题。\n优点：磁盘存储更安全；存储无上限；避免内存存储带来的Page Out问题，性能更稳定；\n缺点：磁盘存储受到IO性能的限制，消息时效性不如内存模式，但影响不大。\n4.RabbitMQ如何保证消息的有序性？话术：\n其实RabbitMQ是队列存储，天然具备先进先出的特点，只要消息的发送是有序的，那么理论上接收也是有序的。不过当一个队列绑定了多个消费者时，可能出现消息轮询投递给消费者的情况，而消费者的处理顺序就无法保证了。\n因此，要保证消息的有序性，需要做的下面几点：\n\n保证消息发送的有序性\n保证一组有序的消息都发送到同一个队列\n保证一个队列只包含一个消费者\n\n5.如何防止MQ消息被重复消费？话术：\n消息重复消费的原因多种多样，不可避免。所以只能从消费者端入手，只要能保证消息处理的幂等性就可以确保消息不被重复消费。\n而幂等性的保证又有很多方案：\n\n给每一条消息都添加一个唯一id，在本地记录消息表及消息状态，处理消息时基于数据库表的id唯一性做判断\n同样是记录消息表，利用消息状态字段实现基于乐观锁的判断，保证幂等\n基于业务本身的幂等性。比如根据id的删除、查询业务天生幂等；新增、修改等业务可以考虑基于数据库id唯一性、或者乐观锁机制确保幂等。本质与消息表方案类似。\n\n6.如何保证RabbitMQ的高可用？话术：\n要实现RabbitMQ的高可用无外乎下面两点：\n\n做好交换机、队列、消息的持久化\n搭建RabbitMQ的镜像集群，做好主从备份。当然也可以使用仲裁队列代替镜像集群。\n\n7.使用MQ可以解决那些问题？话术：\nRabbitMQ能解决的问题很多，例如：\n\n解耦合：将几个业务关联的微服务调用修改为基于MQ的异步通知，可以解除微服务之间的业务耦合。同时还提高了业务性能。\n流量削峰：将突发的业务请求放入MQ中，作为缓冲区。后端的业务根据自己的处理能力从MQ中获取消息，逐个处理任务。流量曲线变的平滑很多\n延迟队列：基于RabbitMQ的死信队列或者DelayExchange插件，可以实现消息发送后，延迟接收的效果。\n\n消息确认实战通过消息确认机制我们可以确保我们的消息可靠送达到我们的用户手中，即使消息丢失掉，我们也可以通过进行重复分发确保用户可靠收到消息\nRabbitMQ为我们提供了两种方式：\n\n通过AMQP事务机制实现，这也是AMQP协议层面提供的解决方案；\n通过将channel设置成confirm模式来实现；\n\nAMQP事务使用java原生事务RabbitMQ中与事务有关的主要有三个方法：\n\ntxSelect()\ntxCommit()\ntxRollback()\n\ntxSelect主要用于将当前channel设置成transaction模式，txCommit用于提交事务，txRollback用于回滚事务。\n生产者代码\npublic class TransactionSender1 &#123;\n\n\tprivate final static String QUEUE_NAME = \"transition\";\n\n\tpublic static void main(String[] args) throws IOException, TimeoutException &#123;\n\t\t/**\n\t\t * 创建连接连接到MabbitMQ\n\t\t */\n\t\tConnectionFactory factory = new ConnectionFactory();\n\n\t\t// 设置MabbitMQ所在主机ip或者主机名\n\t\tfactory.setUsername(\"guest\");\n\t\tfactory.setPassword(\"guest\");\n\t\tfactory.setHost(\"127.0.0.1\");\n\t\tfactory.setVirtualHost(\"/\");\n\t\tfactory.setPort(5672);\n\n\t\t// 创建一个连接\n\t\tConnection connection = factory.newConnection();\n\n\t\t// 创建一个频道\n\t\tChannel channel = connection.createChannel();\n\n\t\t// 指定一个队列\n\t\tchannel.queueDeclare(QUEUE_NAME, false, false, false, null);\n\t\t// 发送的消息\n\t\tString message = \"This is a transaction message！\";\n\n\t\ttry &#123;\n\t\t\t// 开启事务\n\t\t\tchannel.txSelect();\n\t\t\t// 往队列中发出一条消息，使用rabbitmq默认交换机\n\t\t\tchannel.basicPublish(\"\", QUEUE_NAME, null, message.getBytes());\n\t\t\t// 提交事务\n\t\t\tchannel.txCommit();\n\t\t&#125; catch (Exception e) &#123;\n\t\t\te.printStackTrace();\n\t\t\t// 事务回滚\n\t\t\tchannel.txRollback();\n\t\t&#125;\n\n\t\tSystem.out.println(\" TransactionSender1 Sent '\" + message + \"'\");\n\t\t// 关闭频道和连接\n\t\tchannel.close();\n\t\tconnection.close();\n\t&#125;\n\n&#125;\n\n使用wireshark来监听网络：\n\n我们可以清晰的看见消息的分发过程，与我们前面分析的一致。主要执行了四个步骤：\n\nClient发送Tx.Select\nBroker发送Tx.Select-Ok(在它之后，发送消息)\nClient发送Tx.Commit\nBroker发送Tx.Commit-Ok\n\n通过抛出异常来模拟发送消息错误，进行事务回滚。更改发送信息代码为：\ntry &#123;\n\t\t\t// 开启事务\n\t\t\tchannel.txSelect();\n\t\t\t// 往队列中发出一条消息，使用rabbitmq默认交换机\n\t\t\tchannel.basicPublish(\"\", QUEUE_NAME, null, message.getBytes());\n\t\t\t// 除以0，模拟异常，使用rabbitmq默认交换机\n\t\t\tint t = 1/0;\n\t\t\t// 提交事务\n\t\t\tchannel.txCommit();\n\t\t&#125; catch (Exception e) &#123;\n\t\t\te.printStackTrace();\n\t\t\t// 事务回滚\n\t\t\tchannel.txRollback();\n\t\t&#125;\n\n测试结果如下：\n\n可以看见事务进行了回滚，同时我们在接收端也没有收到消息。\n我们可以知道事务确实能够解决消息的发送者和Broker之间消息的确认，只有当消息成功被服务端Broker接收，并且接受时，事务才能提交成功，不然我们便可以在捕获异常进行事务回滚操作同时进行消息重发。\n\n\n\n\n\n\n\n\n\n消费者代码无需考虑，确认机制分为生产者确认和消费者确认两种方式。生产者确认可以确保Broker已经成功接收到该消息，消费者确认可以确保该消息被成功地消费。\n2、结合Spring Boot来使用事务一般在Spring Boot使用RabbitMQ，主要是通过封装的RabbitTemplate模板来实现消息的发送，这里主要也是分为两种情况，使用RabbitTemplate同步发送，或者异步发送。\n\n\n\n\n\n\n\n\n\n发布确认和事务。(两者不可同时使用)在channel为事务时，不可引入确认模式；同样channel为确认模式下，不可使用事务。\n所以在使用事务时，在application.properties中，需要将确认模式更改为false。\nspring.rabbitmq.publisher-confirms=false\n\n通过设置RabbitTemplate的channelTransacted为true，来设置事务环境，使得可以使用RabbitMQ事务。如下：\ntemplate.setChannelTransacted(true);\n\n在创键RabbitTemplate时，设置事务环境\n@Bean\n@Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE)\npublic RabbitTemplate rabbitTemplateNew() &#123;\n\tRabbitTemplate template = new RabbitTemplate(connectionFactory());\n\ttemplate.setChannelTransacted(true);\n\treturn template;\n&#125;\n\n\n生产者代码：\n@Component\npublic class TransactionSender2 &#123;\n\n\t@Autowired\n\tprivate AmqpTemplate rabbitTemplate;\n\n\t@Transactional(rollbackFor = Exception.class)\n\tpublic void send(String msg) &#123;\n\t\tSimpleDateFormat time = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n\t\tString sendMsg = msg + time.format(new Date()) + \" This is a transaction message！ \";\n\t\t/**\n\t\t * 这里可以执行数据库操作\n\t\t * \n\t\t **/\n\t\tSystem.out.println(\"TransactionSender2 : \" + sendMsg);\n\t\tthis.rabbitTemplate.convertAndSend(\"transition\", sendMsg);\n\t&#125;\n\n&#125;\n\n@Transactional(rollbackFor &#x3D; Exception.class)，来现实事务方法。一旦方法中抛出异常，比如执行数据库操作时，就会被捕获到，同时事务将进行回滚，并且向外发送的消息将不会发送出去。而当发送消息出现异常时，也会响应执行事务回滚。\n通过wireshark测试：\nconfirm模式生产者(Producer)的Confirm模式通过生产者的确认模式我们是要保证消息准确达到Broker端，而与AMQP事务不同的是Confirm是针对一条消息的，而事务是可以针对多条消息的。\n为了使用Confirm模式，client会发送confirm.select方法帧。通过是否设置了no-wait属性，来决定Broker端是否会以confirm.select-ok来进行应答。一旦在channel上使用confirm.select方法，channel就将处于Confirm模式。处于 transactional模式的channel不能再被设置成Confirm模式，反之亦然。\n在生产者将信道设置成Confirm模式，一旦信道进入Confirm模式，所有在该信道上面发布的消息都会被指派一个唯一的ID(以confirm.select为基础从1开始计数)，一旦消息被投递到所有匹配的队列之后，Broker就会发送一个确认给生产者（包含消息的唯一ID）,这就使得生产者知道消息已经正确到达目的队列了，如果消息和队列是可持久化的，那么确认消息会将消息写入磁盘之后发出，Broker回传给生产者的确认消息中deliver-tag域包含了确认消息的序列号，此外Broker也可以设置basic.ack的multiple域，表示到这个序列号之前的所有消息都已经得到了处理。\nConfirm模式最大的好处在于它是异步的，一旦发布一条消息，生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息，当消息最终得到确认之后，生产者应用便可以通过回调方法来处理该确认消息，如果RabbitMQ因为自身内部错误导致消息丢失，就会发送一条basic.nack来代替basic.ack的消息，在这个情形下，basic.nack中各域值的含义与basic.ack中相应各域含义是相同的，同时requeue域的值应该被忽略。通过nack一条或多条消息， Broker表明自身无法对相应消息完成处理，并拒绝为这些消息的处理负责。在这种情况下，client可以选择将消息re-publish。\n在channel 被设置成Confirm模式之后，所有被publish的后续消息都将被Confirm（即 ack）或者被nack一次。但是没有对消息被Confirm的快慢做任何保证，并且同一条消息不会既被Confirm又被nack。\n编程模式消息持久化的优化没有太好方法，用更好的物理存储（SAS, SSD, RAID卡）总会带来改善。生产者confirm这一环节的优化则主要在于客户端程序的优化之上。归纳起来，客户端实现生产者confirm有三种编程方式：\n普通Confirm模式：每发送一条消息后，调用waitForConfirms()方法，等待服务器端Confirm。实际上是一种串行Confirm了，每publish一条消息之后就等待服务端Confirm，如果服务端返回false或者超时时间内未返回，客户端进行消息重传；批量Confirm模式：批量Confirm模式，每发送一批消息之后，调用waitForConfirms()方法，等待服务端Confirm，这种批量确认的模式极大的提高了Confirm效率，但是如果一旦出现Confirm返回false或者超时的情况，客户端需要将这一批次的消息全部重发，这会带来明显的重复消息，如果这种情况频繁发生的话，效率也会不升反降；异步Confirm模式：提供一个回调方法，服务端Confirm了一条或者多条消息后Client端会回调这个方法。\n1、普通Confirm模式生产者代码：java原生方式\nimport com.rabbitmq.client.Channel;\nimport com.rabbitmq.client.Connection;\nimport com.rabbitmq.client.ConnectionFactory;\nimport com.rabbitmq.client.MessageProperties;\nimport org.apache.commons.lang.StringUtils;\nimport org.springframework.amqp.core.AmqpTemplate;\nimport org.springframework.beans.factory.annotation.Autowired;\n\nimport java.io.IOException;\nimport java.util.concurrent.TimeoutException;\n\n/**\n * 这是java原生类支持RabbitMQ，直接运行该类\n */\npublic class ConfirmSender1 &#123;\n\n    private final static String QUEUE_NAME = \"confirm\";\n\n    public static void main(String[] args) throws IOException, TimeoutException, InterruptedException &#123;\n        /**\n         * 创建连接连接到RabbitMQ\n         */\n        ConnectionFactory factory = new ConnectionFactory();\n\n        // 设置RabbitMQ所在主机ip或者主机名\n        factory.setUsername(\"guest\");\n        factory.setPassword(\"guest\");\n        factory.setHost(\"127.0.0.1\");\n        factory.setVirtualHost(\"/\");\n        factory.setPort(5672);\n\n        // 创建一个连接\n        Connection connection = factory.newConnection();\n\n        // 创建一个频道\n        Channel channel = connection.createChannel();\n\n        // 指定一个队列\n        channel.queueDeclare(QUEUE_NAME, false, false, false, null);\n        // 发送的消息\n        String message = \"This is a confirm message！\";\n\t开启confirm模式：生产者通过调用channel的confirmSelect方法将channel设置为Confirm模式\n        channel.confirmSelect();\n        final long start = System.currentTimeMillis();\n        //发送持久化消息\n        for (int i = 0; i &lt; 5; i++) &#123;\n            //第一个参数是exchangeName(默认情况下代理服务器端是存在一个\"\"名字的exchange的,\n            //因此如果不创建exchange的话我们可以直接将该参数设置成\"\",如果创建了exchange的话\n            //我们需要将该参数设置成创建的exchange的名字),第二个参数是路由键\n            channel.basicPublish(\"\", QUEUE_NAME, MessageProperties.PERSISTENT_BASIC, (\" Confirm模式， 第\" + (i + 1) + \"条消息\").getBytes());\n            if (channel.waitForConfirms()) &#123;\n                System.out.println(\"发送成功\");\n            &#125;else&#123;\n                // 进行消息重发\n            &#125;\n        &#125;\n        System.out.println(\"执行waitForConfirms耗费时间: \" + (System.currentTimeMillis() - start) + \"ms\");\n        // 关闭频道和连接\n        channel.close();\n        connection.close();\n    &#125;\n&#125;\n\n\n批量Confirm模式channel.confirmSelect();\nfor(int i=0;i&lt;5;i++)&#123;\n     channel.basicPublish(\"\", QUEUE_NAME, MessageProperties.PERSISTENT_BASIC, (\" Confirm模式， 第\" + (i + 1) + \"条消息\").getBytes());\n\t&#125;\nif(channel.waitForConfirms())&#123;\n    System.out.println(\"发送成功\");\n&#125;else&#123;\n\t// 进行消息重发\n&#125;\n\n\n在WireShark查看信息如下：\n\n可以发现这里处理的就是在批量发送信息完毕后，再进行ACK确认。同时我们发现这里只有三个Basic.Ack，这是因为Broker对信息进行了批量处理。\n我们可以发现multiple的值为true，表示确认所有将比第一个参数指定的 delivery-tag 小的消息都得到确认。\n如果我们要对每条消息进行监听处理，可以通过在channel中添加监听器来实现\nchannel.addConfirmListener(new ConfirmListener() &#123;\n\n            @Override\n            public void handleNack(long deliveryTag, boolean multiple) throws IOException &#123;\n                System.out.println(\"nack: deliveryTag = \" + deliveryTag + \" multiple: \" + multiple);\n            &#125;\n\n            @Override\n            public void handleAck(long deliveryTag, boolean multiple) throws IOException &#123;\n                System.out.println(\"ack: deliveryTag = \" + deliveryTag + \" multiple: \" + multiple);\n            &#125;\n        &#125;);\n\n\n\n当收到Broker发送过来的ack消息时就会调用handleAck方法，收到nack时就会调用handleNack方法。\n不同multiple值，ack的结果\nack: deliveryTag = 4 multiple: true\nack: deliveryTag = 5 multiple: false\n发送成功\n执行waitForConfirms耗费时间: 50ms\n\n消费者(Consumer)的Confirm模式手动确认和自动确认为了保证消息从队列可靠地到达消费者，RabbitMQ提供消息确认机制(message acknowledgment)。消费者在声明队列时，可以指定noAck参数，当noAck&#x3D;false时，RabbitMQ会等待消费者显式发回ack信号后才从内存(和磁盘，如果是持久化消息的话)中移去消息。否则，RabbitMQ会在队列中消息被消费后立即删除它。\n采用消息确认机制后，只要令noAck&#x3D;false，消费者就有足够的时间处理消息(任务)，不用担心处理消息过程中消费者进程挂掉后消息丢失的问题，因为RabbitMQ会一直持有消息直到消费者显式调用basicAck为止。\n在Consumer中Confirm模式中分为手动确认和自动确认。\n手动确认主要并使用以下方法：\nbasic.ack: 用于肯定确认，multiple参数用于多个消息确认。basic.recover：是路由不成功的消息可以使用recovery重新发送到队列中。basic.reject：是接收端告诉服务器这个消息我拒绝接收,不处理,可以设置是否放回到队列中还是丢掉，而且只能一次拒绝一个消息,官网中有明确说明不能批量拒绝消息，为解决批量拒绝消息才有了basicNack。basic.nack：可以一次拒绝N条消息，客户端可以设置basicNack方法的multiple参数为true，服务器会拒绝指定了delivery_tag的所有未确认的消息\n在自动确认模式下，消息在发送后立即被认为是发送成功。 这种模式可以提高吞吐量（只要消费者能够跟上），不过会降低投递和消费者处理的安全性。 这种模式通常被称为“发后即忘”。 与手动确认模式不同，如果消费者的TCP连接或信道在成功投递之前关闭，该消息则会丢失。\n","slug":"RabbitMQ-基础","date":"2023-06-08T14:02:31.000Z","categories_index":"","tags_index":"RabbitMQ","author_index":"大宝贝的程序员"},{"id":"fe3fb8c55b48b7e212cb33aae1728c63","title":"微服务_入门","content":"认识微服务单体架构单体架构：将业务的所有功能集中在一个项目中开发，打成一个包部署。\n\n单体架构的优缺点如下：\n优点：\n\n架构简单\n部署成本低\n\n缺点：\n\n耦合度高（维护困难、升级困难）\n\n分布式架构分布式架构：根据业务功能对系统做拆分，每个业务功能模块作为独立项目开发，称为一个服务。\n\n分布式架构的优缺点：\n优点：\n\n降低服务耦合\n有利于服务升级和拓展\n\n缺点：\n\n服务调用关系错综复杂\n\n分布式架构虽然降低了服务耦合，但是服务拆分时也有很多问题需要思考：\n\n服务拆分的粒度如何界定？\n服务之间如何调用？\n服务的调用关系如何管理？\n\n微服务微服务的架构特征：\n\n单一职责：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责\n自治：团队独立、技术独立、数据独立，独立部署和交付\n面向服务：服务提供统一标准的接口，与语言和技术无关\n隔离性强：服务调用做好隔离、容错、降级，避免出现级联问题\n\n\n微服务的上述特性其实是在给分布式架构制定一个标准，进一步降低服务之间的耦合度，提供服务的独立性和灵活性。做到高内聚，低耦合。\n因此，可以认为微服务是一种经过良好架构设计的分布式架构方案 。\nSpringCloudSpringCloud是目前国内使用最广泛的微服务框架。官网地址：https://spring.io/projects/spring-cloud。\nSpringCloud集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。\n其中常见的组件包括：\n\n另外，SpringCloud底层是依赖于SpringBoot的，并且有版本的兼容关系，如下：\n\n学习的版本是 Hoxton.SR10，因此对应的SpringBoot版本是2.3.x版本。\n总结\n\n单体架构：简单方便，高度耦合，扩展性差，适合小型项目。例如：学生管理系统\n\n分布式架构：松耦合，扩展性好，但架构复杂，难度大。适合大型互联网项目，例如：京东、淘宝\n\n微服务：一种良好的分布式架构方案\n①优点：拆分粒度更小、服务更独立、耦合度更低\n②缺点：架构非常复杂，运维、监控、部署难度提高\n\nSpringCloud是微服务架构的一站式解决方案，集成了各种优秀微服务功能组件\n\n\n服务拆分和远程调用服务拆分原则这里我总结了微服务拆分时的几个原则：\n\n不同微服务，不要重复开发相同业务\n微服务数据独立，不要访问其它微服务的数据库\n微服务可以将自己的业务暴露为接口，供其它微服务调用\n\n\n服务拆分示例以微服务cloud-demo为例，其结构如下：\n\ncloud-demo：父工程，管理依赖\n\norder-service：订单微服务，负责订单相关业务\nuser-service：用户微服务，负责用户相关业务\n\ncloud-user表中初始数据如下：\n\ncloud-order表中初始数据如下：\n\ncloud-order表中持有cloud-user表中的id字段。\n项目结构如下：\n\n实现远程调用案例在order-service服务中，有一个根据id查询订单的接口：\n\n根据id查询订单，返回值是Order对象，如图：\n\n其中的user为null\n在user-service中有一个根据id查询用户的接口：\n\n案例分析要求：\n\n订单微服务和用户微服务都必须有各自的数据库，相互独立\n订单服务和用户服务都对外暴露Restful的接口\n订单服务如果需要查询用户信息，只能调用用户服务的Restful接口，不能查询用户数据库\n\n修改order-service中的根据id查询订单业务，要求在查询订单的同时，根据订单中包含的userId查询出用户信息，一起返回。\n\n因此，我们需要在order-service中 向user-service发起一个http的请求，调用http://localhost:8081/user/{userId}这个接口。\n大概的步骤是这样的：\n\n注册一个RestTemplate的实例到Spring容器\n修改order-service服务中的OrderService类中的queryOrderById方法，根据Order对象中的userId查询User\n将查询的User填充到Order对象，一起返回\n\n注册RestTemplate首先，我们在order-service服务中的OrderApplication启动类中，注册RestTemplate实例：\nimport org.mybatis.spring.annotation.MapperScan;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.web.client.RestTemplate;\n\n@MapperScan(\"cn.itcast.order.mapper\")\n@SpringBootApplication\npublic class OrderApplication &#123;\n\n    public static void main(String[] args) &#123;\n        SpringApplication.run(OrderApplication.class, args);\n    &#125;\n\n    @Bean\n    public RestTemplate restTemplate() &#123;\n        return new RestTemplate();\n    &#125;\n&#125;\n\n实现远程调用修改order-service服务中的cn.itcast.order.service包下的OrderService类中的queryOrderById方法：\n\n提供者与消费者在服务调用关系中，会有两个不同的角色：\n服务提供者：一次业务中，被其它微服务调用的服务。（提供接口给其它微服务）\n服务消费者：一次业务中，调用其它微服务的服务。（调用其它微服务提供的接口）\n\n服务提供者与服务消费者的角色并不是绝对的，而是相对于业务而言。\nEureka注册中心假如我们的服务提供者user-service部署了多个实例，如图：\n\n思考几个问题：\n\norder-service在发起远程调用的时候，该如何得知user-service实例的ip地址和端口？\n有多个user-service实例地址，order-service调用时该如何选择？\norder-service如何得知某个user-service实例是否依然健康，是不是已经宕机？\n\nEureka的结构和作用这些问题都需要利用SpringCloud中的注册中心来解决，这里介绍Eureka，其结构如下：\n\norder-service如何得知user-service实例地址？\n获取地址信息的流程如下：\n\nuser-service服务实例启动后，将自己的信息注册到eureka-server（Eureka服务端）即服务注册\neureka-server保存服务名称到服务实例地址列表的映射关系\norder-service根据服务名称，拉取实例地址列表，即服务发现或服务拉取\n\norder-service如何从多个user-service实例中选择具体的实例？\n\norder-service从实例列表中利用负载均衡算法选中一个实例地址\n向该实例地址发起远程调用\n\norder-service如何得知某个user-service实例是否依然健康，是不是已经宕机？\n\nuser-service会每隔一段时间（默认30秒）向eureka-server发起请求，报告自己状态，称为心跳\n当超过一定时间没有发送心跳时，eureka-server会认为微服务实例故障，将该实例从服务列表中剔除\norder-service拉取服务时，就能将故障实例排除了\n\n\n\n\n\n\n\n\n\n\n注意：一个微服务，既可以是服务提供者，又可以是服务消费者，因此eureka将服务注册、服务发现等功能统一封装到了eureka-client端\n搭建eureka-server注册中心服务端：eureka-server，这必须是一个独立的微服务\n\n创建eureka-server服务在cloud-demo父工程下，创建一个子模块：\n引入eureka依赖引入SpringCloud为eureka提供的starter依赖：\n&lt;dependency>\n    &lt;groupId>org.springframework.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-netflix-eureka-server&lt;/artifactId>\n&lt;/dependency>\n\n编写启动类给eureka-server服务编写一个启动类，一定要添加一个@EnableEurekaServer注解，开启eureka的注册中心功能：\npackage cn.itcast.eureka;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;\n\n@SpringBootApplication\n@EnableEurekaServer\npublic class EurekaApplication &#123;\n    public static void main(String[] args) &#123;\n        SpringApplication.run(EurekaApplication.class, args);\n    &#125;\n&#125;\n\n编写配置文件编写一个application.yml文件，内容如下：\nserver:\n  port: 10086\nspring:\n  application:\n    name: eureka-server\neureka:\n  client:\n    service-url: \n      defaultZone: http://127.0.0.1:10086/eureka\n\n启动服务启动微服务，然后在浏览器访问：http://127.0.0.1:10086\n\n服务注册将user-service注册到eureka-server中去。\n引入依赖在user-service的pom文件中，引入下面的eureka-client依赖：\n&lt;dependency>\n    &lt;groupId>org.springframework.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-netflix-eureka-client&lt;/artifactId>\n&lt;/dependency>\n\n配置文件在user-service中，修改application.yml文件，添加服务名称、eureka地址：\nspring:\n  application:\n    name: userservice\neureka:\n  client:\n    service-url:\n      defaultZone: http://127.0.0.1:10086/eureka\n\n启动多个user-service实例添加一个SpringBoot的启动配置，再启动一个user-service。\n复制原来的user-service启动配置：\n\n然后，在弹出的窗口中，填写信息：\n\nSpringBoot窗口会出现两个user-service启动配置：\n\n启动两个user-service实例：\n\n查看eureka-server管理页面：\n\n服务发现将order-service的逻辑修改：向eureka-server拉取user-service的信息，实现服务发现。\n引入依赖服务发现、服务注册统一都封装在eureka-client依赖，因此这一步与服务注册时一致\n在order-service的pom文件中，引入下面的eureka-client依赖：\n&lt;dependency>\n    &lt;groupId>org.springframework.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-netflix-eureka-client&lt;/artifactId>\n&lt;/dependency>\n\n配置文件服务发现也需要知道eureka地址，因此第二步与服务注册一致，都是配置eureka信息：\n在order-service中，修改application.yml文件，添加服务名称、eureka地址：\nspring:\n  application:\n    name: orderservice\neureka:\n  client:\n    service-url:\n      defaultZone: http://127.0.0.1:10086/eureka\n\n服务拉取和负载均衡在order-service的OrderApplication中，给RestTemplate这个Bean添加一个@LoadBalanced注解：\n\n修改order-service服务中的queryOrderById方法。修改访问的url路径，用服务名代替ip、端口：\n\nspring会自动帮助我们从eureka-server端，根据userservice这个服务名称，获取实例列表，而后完成负载均衡。\nRibbon负载均衡添加了@LoadBalanced注解，即可实现负载均衡功能，这是什么原理呢？\n负载均衡原理SpringCloud底层其实是利用了一个名为Ribbon的组件，来实现负载均衡功能的。\n\n那么我们发出的请求明明是http://userservice/user/1，怎么变成了http://localhost:8081的呢？\n源码跟踪为什么我们只输入了service名称就可以访问了呢？之前还要获取ip和端口。\n显然有人帮我们根据service名称，获取到了服务实例的ip和端口。它就是LoadBalancerInterceptor，这个类会在对RestTemplate的请求进行拦截，然后从Eureka根据服务id获取服务列表，随后利用负载均衡算法得到真实的服务地址信息，替换服务id。\n我们进行源码跟踪：\nLoadBalancerIntercepor\n可以看到这里的intercept方法，拦截了用户的HttpRequest请求，然后做了几件事：\n\nrequest.getURI()：获取请求uri，本例中就是 http://user-service/user/8\noriginalUri.getHost()：获取uri路径的主机名，其实就是服务id，user-service\nthis.loadBalancer.execute()：处理服务id，和用户请求。\n\n这里的this.loadBalancer是LoadBalancerClient类型，我们继续跟入。\nLoadBalancerClient继续跟入execute方法：\n\n代码是这样的：\n\ngetLoadBalancer(serviceId)：根据服务id获取ILoadBalancer，而ILoadBalancer会拿着服务id去eureka中获取服务列表并保存起来。\ngetServer(loadBalancer)：利用内置的负载均衡算法，从服务列表中选择一个。\n\n负载均衡策略IRule在刚才的代码中，可以看到获取服务使通过一个getServer方法来做负载均衡:\n \n我们继续跟入：\n\n继续跟踪源码chooseServer方法，发现这么一段代码：\n \n我们看看这个rule是谁：\n \n这里的rule默认值是一个RoundRobinRule，看类的介绍：\n \n这不就是轮询的意思嘛\nSpringCloudRibbon的流程SpringCloudRibbon的底层采用了一个拦截器，拦截了RestTemplate发出的请求，对地址做了修改。用一幅图来总结一下：\n\n基本流程如下：\n\n拦截我们的RestTemplate请求http://userservice/user/1\nRibbonLoadBalancerClient会从请求url中获取服务名称，也就是user-service\nDynamicServerListLoadBalancer根据user-service到eureka拉取服务列表\neureka返回列表，localhost:8081、localhost:8082\nIRule利用内置负载均衡规则，从列表中选择一个，例如localhost:8081\nRibbonLoadBalancerClient修改请求地址，用localhost:8081替代userservice，得到http://localhost:8081/user/1，发起真实请求\n\n负载均衡策略负载均衡策略负载均衡的规则都定义在IRule接口中，而IRule有很多不同的实现类：\n\n不同规则的含义如下：\n\n\n\n内置负载均衡规则类\n规则描述\n\n\n\nRoundRobinRule\n简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。\n\n\nAvailabilityFilteringRule\n对以下两种服务器进行忽略：   （1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。  （2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的clientName.clientConfigNameSpace.ActiveConnectionsLimit属性进行配置。\n\n\nWeightedResponseTimeRule\n为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。\n\n\nZoneAvoidanceRule\n以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。而后再对Zone内的多个服务做轮询。\n\n\nBestAvailableRule\n忽略那些短路的服务器，并选择并发数较低的服务器。\n\n\nRandomRule\n随机选择一个可用的服务器。\n\n\nRetryRule\n重试机制的选择逻辑\n\n\n默认的实现就是ZoneAvoidanceRule，是一种区间轮询方案\n自定义负载均衡策略通过定义IRule实现可以修改负载均衡规则，有两种方式：\n\n代码方式：在order-service中的OrderApplication类中，定义一个新的IRule：\n\n@Bean\npublic IRule randomRule()&#123;\n    return new RandomRule();\n&#125;\n\n\n配置文件方式：在order-service的application.yml文件中，添加新的配置也可以修改规则：\n\nuserservice: # 给某个微服务配置负载均衡规则，这里是userservice服务\n  ribbon:\n    NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 负载均衡规则 \n\n\n\n\n\n\n\n\n\n\n注意，一般用默认的负载均衡规则，不做修改。\n饥饿加载Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。\n而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载：\nribbon:\n  eager-load:\n    enabled: true\n    clients: userservice\n\nNacos注册中心SpringCloudAlibaba也推出了一个名为Nacos的注册中心。\n服务注册到nacosNacos是SpringCloudAlibaba的组件，而SpringCloudAlibaba也遵循SpringCloud中定义的服务注册、服务发现规范。因此使用Nacos和使用Eureka对于微服务来说，并没有太大区别。\n引入依赖在cloud-demo父工程的pom文件中的&lt;dependencyManagement&gt;中引入SpringCloudAlibaba的依赖：\n&lt;dependency>\n    &lt;groupId>com.alibaba.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-alibaba-dependencies&lt;/artifactId>\n    &lt;version>2.2.6.RELEASE&lt;/version>\n    &lt;type>pom&lt;/type>\n    &lt;scope>import&lt;/scope>\n&lt;/dependency>\n\n然后在user-service和order-service中的pom文件中引入nacos-discovery依赖：\n&lt;dependency>\n    &lt;groupId>com.alibaba.cloud&lt;/groupId>\n    &lt;artifactId>spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId>\n&lt;/dependency>\n\n配置nacos地址在user-service和order-service的application.yml中添加nacos地址：\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n\n启动启动微服务后，登录nacos管理页面，可以看到微服务信息：\n\n服务分级存储模型一个服务可以有多个实例，例如我们的user-service，可以有:\n\n127.0.0.1:8081\n127.0.0.1:8082\n127.0.0.1:8083\n\n假如这些实例分布于全国各地的不同机房，例如：\n\n127.0.0.1:8081，在上海机房\n127.0.0.1:8082，在上海机房\n127.0.0.1:8083，在杭州机房\n\nNacos就将同一机房内的实例划分为一个集群。\n也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图：\n\n微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。\n给user-service配置集群修改user-service的application.yml文件，添加集群配置：\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: HZ # 集群名称\n\n重启两个user-service实例后，我们可以在nacos控制台看到下面结果：\n\n我们再次复制一个user-service启动配置，添加属性：\n-Dserver.port=8083 -Dspring.cloud.nacos.discovery.cluster-name=SH\n\n启动UserApplication3后再次查看nacos控制台：\n\n同集群优先的负载均衡默认的ZoneAvoidanceRule并不能实现根据同集群优先来实现负载均衡。\n因此Nacos中提供了一个NacosRule的实现，可以优先从同集群中挑选实例。\n1）给order-service配置集群信息\n修改order-service的application.yml文件，添加集群配置：\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: HZ # 集群名称\n\n2）修改负载均衡规则\n修改order-service的application.yml文件，修改负载均衡规则：\nuserservice:\n  ribbon:\n    NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 \n\n权重配置实际部署中会出现这样的场景：\n服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求。\n但默认情况下NacosRule是同集群内随机挑选，不会考虑机器的性能问题。因此，Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高。\n在nacos控制台，找到user-service的实例列表，点击编辑，即可修改权重：\n\n在弹出的编辑窗口，修改权重：\n\n\n\n\n\n\n\n\n\n\n注意：如果权重修改为0，则该实例永远不会被访问\n环境隔离Nacos提供了namespace来实现环境隔离功能。\n\nnacos中可以有多个namespace\nnamespace下可以有group、service等\n不同namespace之间相互隔离，例如不同namespace的服务互相不可见\n\n\n创建namespace默认情况下，所有service、data、group都在同一个namespace，名为public\n我们可以点击页面新增按钮，添加一个namespace：\n\n然后，填写表单：\n\n就能在页面看到一个新的namespace：\n\n给微服务配置namespace给微服务配置namespace只能通过修改配置来实现。\n例如，修改order-service的application.yml文件：\nspring:\n  cloud:\n    nacos:\n      server-addr: localhost:8848\n      discovery:\n        cluster-name: HZ\n        namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填ID\n\n\n\n重启order-service后，访问控制台，可以看到下面的结果：\n\n此时访问order-service，因为user-service的namespace和order-service不同，会导致找不到userservice，控制台会报错：\n\nNacos与Eureka的区别Nacos的服务实例分为两种类型：\n\n临时实例：如果实例宕机超过一定时间，会从服务列表剔除，默认的类型。\n\n非临时实例：如果实例宕机，不会从服务列表剔除，也可以叫永久实例。\n\n\n配置一个服务实例为永久实例：\nspring:\n  cloud:\n    nacos:\n      discovery:\n        ephemeral: false # 设置为非临时实例\n\nNacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异：\n\n\nNacos与eureka的共同点\n\n都支持服务注册和服务拉取\n都支持服务提供者心跳方式做健康检测\n\n\nNacos与Eureka的区别\n\nNacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式\n临时实例心跳不正常会被剔除，非临时实例则不会被剔除\nNacos支持服务列表变更的消息推送模式，服务列表更新更及时\nNacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式\n\n\n\n\n\n\n\n\n\n\n\n\nAP理论认为任何一个分布式系统最多只能满足以下三个特性中的两个：一致性（consistency）、可用性（availability）以及分区容错性（partition tolerance）\n","slug":"微服务-入门","date":"2023-06-08T09:44:27.000Z","categories_index":"","tags_index":"微服务","author_index":"大宝贝的程序员"},{"id":"d1863bbf8aa3ec7bc32e4a0f3476163f","title":"线程池","content":"Executor线程池在Java框架设计中，一般涉及到资源相关的，并且资源具有创建消耗大、可复用的特点时，都采用了池化技术管理资源，形成一个“资源池”，池化技术可以带来以下一般性好处：对外部隐藏了资源的创建与释放的细节、实现资源的复用减少内存或者时间性能开销。常见Java中池化技术有：数据库连接池（管理数据库连接资源）、、redis连接池（管理redis连接）等等\nExecutor线程池来自于JDK1.5的JUC包，使用线程池的目的或者好处如下：\n实现线程资源的合理复用。线程资源属于操作系统核心资源之一，创建和销毁都需要占用系统资源和大量时间。使用线程池之后，不再需要开发者管理线程，线程的创建和销毁都交给线程池控制，实现线程的复用，减少线程资源的频繁的创建和销毁。\n提升任务执行效率。当新来一个线程任务的时候，由于具有线程复用计数因此可以直接利用现有的线程去执行任务，不需要新建线程，这样一定程度上提升了执行效率。\n可以对线程和线程任务实现实时监控和管理。比如目前活动线程数、曾经的最大线程数、已完成的任务数量等功能；比如控制最大线程数，在线程任务执行前-执行完毕后-线程池停止后具有可选的回调方法、移除某个线程任务、立即停止线程池等功能，他们都可以通过线程池的相关方法调用来实现。\n\nExecutor线程池的基本结构\n线程池的核心接口以及实现类：\nExecutor        作为线程池的顶级执行接口，也是一个函数式接口。只有一个execute方法，用于执行已提交的 Runnable 任务对象。        它不仅仅是一个接口，更是代表着一种将任务与每个任务将如何运行的机制（包括线程的创建、使用、调度等）分离开来的思想。使用者只需要提交任务，不需要创建线程，执行的细节被封装到Executor中，任务的执行方法可以根据实现者自由选择，可以实现为异步（使用新线程执行任务）、也可以是同步的（在调用者的线程中立即运行已提交的任务）。ExecutorService        继承并扩展了Executor接口的执行服务接口。        新增了可为跟踪一个或多个异步任务执行状况而生成 Future 的方法，比如submit方法，作为execute方法的扩展。新增了可以关闭线程池的方法，比如shutdown和shutdownNow方法。新增了批量执行任务的方法，比如 invokeAny 和 invokeAll方法。AbstractExecutorService        实现了ExecutorService的抽象类，提供 ExecutorService 执行方法的默认实现。比如对ExecutorService返回Future，实现为返回RunnableFuture。另一个作用是作为骨干实现最大限度地减少ExecutorService的实现类的代码。ThreadPoolExecutor        继承了ExecutorService的普通类，这是JDK线程池的核心实现。        它的构造器提供了各种可配置参数，比如线程数量、任务队列、拒绝策略等，方便我们自定义自己的线程池，以及各种钩子 (hook) 方法，方便追踪线程任务的执行，这是我们学习的重点，这里不做详细介绍。ScheduledThreadPoolExecutor        继承了ThreadPoolExecutor的普通类，可以看作功能的扩展或增强。        它能够将线程任务延迟指定时间后执行，或者间隔固定时间多次执行。功能与Timer类似，但ScheduledThreadPoolExecutor功能更强大、更灵活。Timer对应的是单个后台线程，而ScheduledThreadPoolExecutor可以在构造函数中指定多个对应的后台线程数。Timer中一个任务出现异常之后会影响其他任务的执行，但是ScheduledThreadPoolExecutor不会。Timer中一个任务耗时较常会影响其他任务的执行，ScheduledThreadPoolExecutor不会。Executors        独立出来的一个普通类（没有继承和实现关系，采用组合&#x2F;聚合关系，图上没有注明），作为一个线程池工厂，提供各种实用方法。        提供了各种预定义线程池的实现，比如CachedThreadPool、FixedThreadPool等；提供了将Runnable包装、转换为Callable的方法；提供默认的ThreadFactory线程工厂的实现等功能。\nJDK内部提供了Executors这个工具类，来快速的创建线程池。固定线程数量的线程池：核心线程数与最大线程数相等\n单个线程数量的线程池\n接近无限大线程数量的线程池\n带定时调度功能的线程池\n虽然JDK提供了快速创建线程池的方法，但是其实不推荐使用Executors来创建线程池，因为从上面构造线程池可以看出，newFixedThreadPool线程池，由于使用了LinkedBlockingQueue，队列的容量默认是无限大，实际使用中出现任务过多时会导致内存溢出；newCachedThreadPool线程池由于核心线程数无限大，当任务过多的时候，会导致创建大量的线程，可能机器负载过高，可能会导致服务宕机。\nThreadPoolExecutor的概述JDK线程池的关键实现，我们常用的Executors中的预定义线程池就有这个类的实例，当然也可以通过该类的构造器来创建一个自定义的线程池，提供任务执行，线程调度，线程池管理等等服务\nThreadPoolExecutor的类层次结构\n\nCallerRunsPolicy、AbortPolicy、DiscardPolicy、DiscardOldestPolicy是四个拒绝策略类。对于正在执行的线程数大于等于maxmumPoolSize以及workQueue容量已满时提交的任务，或者线程池正在关闭时的新提交的任务，线程池将会执行拒绝策略，任务会交给RejectedExecutionHandler来处理。\nThreadPoolExecutor的主要属性使用一个ctl原子变量来来同时记录线程池的运行状态(runState，简称rs)和线程池中线程数量(workerCount，简称wc)。int类型转换为二进制之后的最高三位保存线程池的状态，低29位保存线程数量。刚初始化ctl的时候，rs为RUNNING状态，wc为0。\n还有一个ReentrantLock独占锁，当改变线程池状态，比如添加工作线程、停止线程池，或者访问线程池共享参数信息比如当前线程数量的时候，因为这涉及到多个工作线程之间的共享信息比如线程池状态、工作线程数量等参数的同步，需要获取mainLock独占锁才能进行操作。\n部分源码\n/**\n * 用来同时记录线程池的运行状态(runState，简称rs)和线程池中线程数量(workerCount，简称wc)。\n \tctl的值使用AtomicInteger原子类包装，能够保证数据是线程安全的。\n * int类型转换为二进制之后的最高三位保存线程池的状态，低29位保存线程数量。\n \t刚初始化ctl的时候，rs为RUNNING状态，wc为0\n * ReentrantReadWriteLock也是使用一个state变量保存写锁和读锁的获取信息，\n \tConcurrentHashMap中的甚至使用一个lockState保存三种锁状态\n */\nprivate final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));\n\n/**\n * 线程数量掩码位数，int类型长度－3后的剩余位数，即wc所占位数为29\n */\nprivate static final int COUNT_BITS = Integer.SIZE - 3;\n/**\n * 为了能正确保存线程数量，线程池的数量线程被限制为29位的最大值，即最大(2^29）-1个，而不是(2^31)-1个\n */\nprivate static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;\n\n\n//ThreadPoolExecutor中定义了线程池的状态，存储在ctl的高三位中，一共有五种\n\n/**\n * RUNNING状态，11100000000000000000000000000000\n */\nprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;\n/**\n * SHUTDOWN状态：00000000000000000000000000000000\n */\nprivate static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;\n/**\n * STOP状态：00100000000000000000000000000000\n */\nprivate static final int STOP = 1 &lt;&lt; COUNT_BITS;\n/**\n * TIDYING状态：01000000000000000000000000000000\n */\nprivate static final int TIDYING = 2 &lt;&lt; COUNT_BITS;\n/**\n * TERMINATED状态：01100000000000000000000000000000\n */\nprivate static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;\n\n//通过对ctl的拆解、组合，获取相关的数据\n/**\n * 获取ctl的高3位，线程池运行状态\n *\n * @param c 此时的ctl值\n * @return ctl的高3位的int值\n */\nprivate static int runStateOf(int c) &#123;\n    //将c的低29位置为0并返回结果\n    return c &amp; ~CAPACITY;\n&#125;\n\n/**\n * 获取ctl的低29位，线程数量\n *\n * @param c 此时的ctl值\n * @return ctl的低29位的int值\n */\nprivate static int workerCountOf(int c) &#123;\n    //将c的高3位置为0并返回结果\n    return c &amp; CAPACITY;\n&#125;\n\n/**\n * 组合rs和wc，计算ctl新值\n *\n * @param rs 运行状态\n * @param wc 线程数量\n * @return 新ctl的int值\n */\nprivate static int ctlOf(int rs, int wc) &#123;\n    //两者与运算\n    return rs | wc;\n&#125;\n\n/**\n * 线程任务队列，是一个阻塞队列\n * 使用isEmpty来检测队列是否为空，而不是通过poll的返回值\n */\nprivate final BlockingQueue&lt;Runnable> workQueue;\n\n/**\n * 当改变线程池状态，比如添加工作线程、停止线程池，或者访问线程池共享参数信息比如当前线程数量的时候\n * 因为这涉及到多个线程之间的共享信息比如线程池状态、工作线程数量等参数的同步，需要获取mainLock独占锁才行\n */\nprivate final ReentrantLock mainLock = new ReentrantLock();\n\n/**\n * 包含全部工作线程的set集合\n * 只有在持有mainLock锁的时候才能访问\n */\nprivate final HashSet&lt;Worker> workers = new HashSet&lt;Worker>();\n\n/**\n * 主要是为了支持awaitTermination方法，外部线程调用awaitTermination方法之后\n * 会判断线程池是否是TERMINATED状态，即终止状态，如果不是则调用线程在termination条件变量中等待，直到超时或者线程完毕\n */\nprivate final Condition termination = mainLock.newCondition();\n\n/**\n * 记录到目前为止线程池中的拥有的最大线程数量\n * 只有在持有mainLock锁的时候才能访问\n */\nprivate int largestPoolSize;\n\n/**\n * 线程池已完成的任务数量，只有在某个Worker工作线程终止时才会更新\n * 只有在持有mainLock锁的时候才能访问\n */\nprivate long completedTaskCount;\n\n\n/**\n * ThreadPoolExecutor中的工作线程统一使用线程工厂来创建\n * threadFactory用于保存传入的线程工厂实现，具有volatile的特性\n * Executors中给出了默认实现，我们可以直接使用：Executors.defaultThreadFactory()\n */\nprivate volatile ThreadFactory threadFactory;\n\n/**\n * 对于提交任务数超过maxmumPoolSize+workQueue之和时超出的任务，或者线程池正在关闭时的新提交的任务执行的拒绝策略，\n * 任务会交给RejectedExecutionHandler的handler来处理，具有volatile的特性\n * ThreadPoolExecutor中提供了默认实现：AbortPolicy、CallerRunsPolicy、DiscardPolicy、DiscardOldestPolicy\n */\nprivate volatile RejectedExecutionHandler handler;\n\n/**\n * 空闲的工作线程的等待超时时间，具有volatile的特性\n * 当存在的工作线程数量大于指定核心线程数量时，那么多余的线程会使用此超时时间，超过该时间没有工作则关闭线程\n * 或者如果允许CoreThreadTimeOut，那核心线程也会使用此超时时间，超过该时间没有任务则关闭线程；否则，核心将永远等待新的任务。\n */\nprivate volatile long keepAliveTime;\n\n/**\n * 是否允许核心线程应用空闲超时时间，具有volatile的特性\n * 如果为false，那么即使核心线程空闲也会永远保持活动状态（不会被销毁）\n * 如果为true，那么核心线程将会应用 keepAliveTime，在指定时间内等待工作，超时则被销毁（设置成功的要求是超时时间大于0）。\n */\nprivate volatile boolean allowCoreThreadTimeOut;\n\n/**\n * 线程池核心线程数量，具有volatile的特性\n * 除非设置了allowCoreThreadTimeOut=true，那么核心线程永远不会被销毁\n */\nprivate volatile int corePoolSize;\n\n/**\n * 线程池最大线程数量，具有volatile的特性\n * 不能超过(2^29）-1\n */\nprivate volatile int maximumPoolSize;\n\n\n//下面的属性涉及到Java安全模型：https://developer.ibm.com/zh/articles/j-lo-javasecurity/\n//一般人接触不到\n/**\n * 用于校验是否具有shutdown 和 shutdownNow 关闭线程池的操作权限\n */\nprivate static final RuntimePermission shutdownPerm =\n        new RuntimePermission(\"modifyThread\");\n\n/**\n * 授权上下文环境对象\n * 在GC标记ThreadPoolExecutor对象并调用finalize方法时调用，用于释放资源\n */\nprivate final AccessControlContext acc;\n\n线程池的状态线程池有5种状态：RUNNING、SHUTDOWN、STOP、TIDYING、TERMINATED。\nRUNNING &#x3D; -1 &lt;&lt; COUNT_BITS，转换为二进制就是11100000000000000000000000000000SHUTDOWN &#x3D; 0 &lt;&lt; COUNT_BITS，转换为二进制就是00000000000000000000000000000000STOP &#x3D; 1 &lt;&lt; COUNT_BITS，转换为二进制就是00100000000000000000000000000000TIDYING &#x3D; 2 &lt;&lt; COUNT_BITS，转换为二进制就是01000000000000000000000000000000TERMINATED &#x3D; 3 &lt;&lt; COUNT_BITS，转换为二进制就是01100000000000000000000000000000\n可以发现，运行状态的大小关系为：RUNNING &lt; SHUTDOWN &lt; STOP &lt; TIDYING &lt; TERMINATED，这在状态转换的时候非常有用，这样可以通过大小判断状态关系。\n类似于线程的状态，线程池的状态也可以转换。但是又有不同，线程状态可以循环转换、相互转换，而一旦发生线程池的状态的转换，那么该转换不可逆。下面来看看线程池状态的转换规则：\n\n详细说明：\n\n\n\n状态名\n说明\n转换\n\n\n\nRUNNING\n线程池处在RUNNING状态时，能够接收新任务，以及对已添加的任务进行处理。\n新建的线程池的初始状态就是RUNNING，并且线程池中的工作线程数量为0。\n\n\nSHUTDOWN\n线程池处在SHUTDOWN状态时，不接收新任务，但内部正在执行的任务和队列里等待的任务，会执行完，随后会清理全部工作线程。\nRUNNING状态的线程池，调用shutdown方法，或者隐式调用了finalize方法（里面有shutdown方法时线程池状态将变成SHUTDOWN。\n\n\nSTOP\n线程池处在STOP状态时，不接收新任务，不处理已添加的任务（丢弃），并且会中断正在处理的任务，随后会清理全部工作线程。\nRUNNING or SHUTDOWN状态的线程池，调用shutdownNow方法，线程池状态将变成 STOP。\n\n\nTIDYING\n所有的任务已执行完或被终止或被丢弃，ctl记录的workerCount工作线程数量为0，线程池会变为TIDYING状态。接着会执行钩子函数terminated()。\nSHUTDOWN状态的线程池，当任务队列为空并且线程池工作线程数workerCount为0时，线程池状态就会由 SHUTDOWN 自动转换为 TIDYING状态。   STOP状态的线程池，线程池中工作线程数workerCount为0时，线程池状态就会由STOP自动转换为TIDYING状态。\n\n\nTERMINATED\n钩子函数terminated()执行完毕，就变成TERMINATED状态，线程池彻底终止。\nTIDYING状态的线程池，在接着执行完terminated()之后，线程池状态就会由TIDYING自动转换为 TERMINATED。\n\n\nThreadPoolExecutor的构造器ThreadPoolExecutor的构造器是创建线程池的入口，JDK提供了四个构造函数。其中参数较少的的三个构造函数内部都是调用参数最多的那一个构造函数。\t\t\n/**\n * 使用给定的初始参数和默认的线程工厂及默认的拒绝策略创建新的 ThreadPoolExecutor。\n */\npublic ThreadPoolExecutor(int corePoolSize,\n                          int maximumPoolSize,\n                          long keepAliveTime,\n                          TimeUnit unit,\n                          BlockingQueue&lt;Runnable> workQueue) &#123;\n    //内部调用最多参数的构造器\n    //线程工厂传递的Executors的默认实现：Executors.defaultThreadFactory()\n    //拒绝策略传递的默认实现：defaultHandler\n    this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,\n            Executors.defaultThreadFactory(), defaultHandler);\n&#125;\n\n/**\n * 使用给定的初始参数和默认的拒绝策略创建新的 ThreadPoolExecutor。\n */\npublic ThreadPoolExecutor(int corePoolSize,\n                          int maximumPoolSize,\n                          long keepAliveTime,\n                          TimeUnit unit,\n                          BlockingQueue&lt;Runnable> workQueue,\n                          ThreadFactory threadFactory) &#123;\n    //内部调用最多参数的构造器\n    //拒绝策略传递的默认实现：defaultHandler\n    this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,\n            threadFactory, defaultHandler);\n&#125;\n\n/**\n * 使用给定的初始参数和默认的线程工厂创建新的 ThreadPoolExecutor。\n */\npublic ThreadPoolExecutor(int corePoolSize,\n                          int maximumPoolSize,\n                          long keepAliveTime,\n                          TimeUnit unit,\n                          BlockingQueue&lt;Runnable> workQueue,\n                          RejectedExecutionHandler handler) &#123;\n    //内部调用最多参数的构造器\n    //线程工厂传递的Executors的默认实现：Executors.defaultThreadFactory()\n    this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,\n            Executors.defaultThreadFactory(), handler);\n&#125;\n\n/**\n 1. 使用给定的初始参数创建新的 ThreadPoolExecutor,一共有7个参数。\n 2.  3. @param corePoolSize    核心线程数\n 3. @param maximumPoolSize 最大线程数\n 4. @param keepAliveTime   空闲线程等待超时时间\n 5. @param unit            超时时间单位\n 6. @param workQueue       阻塞任务队列\n 7. @param threadFactory   线程工厂\n 8. @param handler         拒绝策略\n */\npublic ThreadPoolExecutor(int corePoolSize,\n                          int maximumPoolSize,\n                          long keepAliveTime,\n                          TimeUnit unit,\n                          BlockingQueue&lt;Runnable> workQueue,\n                          ThreadFactory threadFactory,\n                          RejectedExecutionHandler handler) &#123;\n    //一系列参数校验\n    /*\n     * 如果核心线程数小于0\n     * 或者 如果最大线程数小于等于0\n     * 或者 如果最大线程数小于核心线程数\n     * 或者 如果空闲线程等待超时时间小于0\n     *\n     * 满足上面一项，都将抛出IllegalArgumentException异常\n     */\n    if (corePoolSize &lt; 0 ||\n            maximumPoolSize &lt;= 0 ||\n            maximumPoolSize &lt; corePoolSize ||\n            keepAliveTime &lt; 0)\n        throw new IllegalArgumentException();\n    /*\n     * 如果阻塞任务队列为null\n     * 或者 如果线程工厂为null\n     * 或者 如果拒绝策略为null\n     *\n     * 满足上面一项，都将抛出NullPointerException异常\n     */\n    if (workQueue == null || threadFactory == null || handler == null)\n        throw new NullPointerException();\n    //初始化用于安全管理器的上下文参数\n    this.acc = System.getSecurityManager() == null ?\n            null :\n            AccessController.getContext();\n    //初始化核心线程数\n    this.corePoolSize = corePoolSize;\n    //初始化最大线程数\n    this.maximumPoolSize = maximumPoolSize;\n    //初始化阻塞任务队列\n    this.workQueue = workQueue;\n    //初始化空闲线程等待超时时间\n    this.keepAliveTime = unit.toNanos(keepAliveTime);\n    //初始化线程工厂\n    this.threadFactory = threadFactory;\n    //初始化拒绝策略\n    this.handler = handler;\n&#125;\n\n\ncorePoolSize：线程池核心线程数。不能小于0。当提交一个任务到线程池时，如果此时线程池的线程数量小于核心线程数，那么线程池会新创建一个线程来执行任务，即使此时存在空闲线程也不例外。默认情况创建0个核心线程，如果调用了线程池的prestartAllCoreThreads()方法，线程池会立即创建并启动所有核心线程。\nmaximumPoolSize：线程池最大线程数。不能小于corePoolSize，不能小于等于0。当workQueue（任务队列）放不下线程任务，并且已创建的线程数小于最大线程数，则线程池会再次创建新的线程执行任务。值得注意的是，如果使用了无界的任务队列（任务队列没有上限大小）这个参数就没什么效果。\nkeepAliveTime：空闲线程等待超时时间；unit：keepAliveTime时间单位当线程数量超过corePoolSize时，多余的空闲线程等待超时时间，即如果指定时间返回没有任务执行，那么该线程将被回收，直到数量减少到corePoolSize为止。如果允许CoreThreadTimeOut（前提是keepAliveTime大于0），那核心线程也会使用此超时时间，超过该时间没有任务则关闭线程；否则，核心线程将永远等待新的任务。\nworkQueue：阻塞任务队列。当线程任务添加的速度超过所有核心线程执行速度时，新来的来不及执行的线程任务将被存放到workQueue阻塞任务队列中。任务队列一定是阻塞队列，常见的有以下四种，实际上有很多种：ArrayBlockingQueue：有界阻塞任务队列，构造函数一定要传入具体队列容量。LinkedBlockingQueu：通常作为无界阻塞任务队列（构造函数不传大小会默认为Integer.MAX_VALUE ），当有大量任务提交时，容易造成内存耗尽。SynchronousQueue：一个没有容量的阻塞队列，会将任务同步交付给工作线程。PriorityBlockingQueue : 具有优先级的无界阻塞任务队列。\nthreadFactor：线程工厂线程工厂用于创建工作线程，默认线程工厂：Executors.defaultThreadFactory。\nhandler：拒绝策略。对于正在执行的线程数等于maxmumPoolSize以及workQueue容量已满时提交的任务，或者线程池正在关闭时的新提交的任务，线程池将会执行拒绝策略，即这些任务都直接被非线程池线程处理了。ThreadPoolExecutor中提供了4种拒绝策略的实现：\n\nAbortPolicy：调用者的线程直接抛出异常，作为默认拒绝策略；\nCallerRunsPolicy：用调用者的线程执行任务；\nDiscardOldestPolicy：抛弃队列中最久的任务；\nDiscardPolicy：抛弃当前任务；\n\n线程池的运行原理线程池刚创建出来是什么样子呢，如下图\n\n不错，刚创建出来的线程池中只有一个构造时传入的阻塞队列而已，此时里面并没有的任何线程，但是如果你想要在执行之前已经创建好核心线程数，可以调用prestartAllCoreThreads方法来实现，默认是没有线程的。\n当有线程通过execute方法提交了一个任务，会发生什么呢？\n提交任务的时候，其实会去进行任务的处理\n首先会去判断当前线程池的线程数是否小于核心线程数，也就是线程池构造时传入的参数corePoolSize。\n如果小于，那么就直接通过ThreadFactory创建一个线程来执行这个任务，如图\n\n当任务执行完之后，线程不会退出，而是会去从阻塞队列中获取任务，如下图\n\n接下来如果又提交了一个任务，也会按照上述的步骤，去判断是否小于核心线程数，如果小于，还是会创建线程来执行任务，执行完之后也会从阻塞队列中获取任务。这里有个细节，就是提交任务的时候，就算有线程池里的线程从阻塞队列中获取不到任务，如果线程池里的线程数还是小于核心线程数，那么依然会继续创建线程，而不是复用已有的线程。\n如果线程池里的线程数不再小于核心线程数呢？那么此时就会尝试将任务放入阻塞队列中，入队成功之后，如图\n\n这样在阻塞的线程就可以获取到任务了。\n但是，随着任务越来越多，队列已经满了，任务放入失败了，那怎么办呢？\n此时就会判断当前线程池里的线程数是否小于最大线程数，也就是入参时的maximumPoolSize参数\n如果小于最大线程数，那么也会创建非核心线程来执行提交的任务，如图\n\n所以，从这里可以发现，就算队列中有任务，新创建的线程还是优先处理这个提交的任务，而不是从队列中获取已有的任务执行，从这可以看出，先提交的任务不一定先执行。\n但是不幸的事发生了，线程数已经达到了最大线程数量，那么此时会怎么办呢？\n此时就会执行拒绝策略，也就是构造线程池的时候，传入的RejectedExecutionHandler对象，来处理这个任务。\n\n看看execute方法代码是如何实现的\nworkerCountOf(c)&lt;corePoolSize:这行代码就是判断是否小于核心线程数，是的话就通过addWorker方法，addWorker就是添加线程来执行任务\nworkQueue.offer(command)：这行代码就表示尝试往阻塞队列中添加任务\n添加失败之后就会再次调用addWorker方法尝试添加非核心线程来执行任务\n如果还是添加非核心线程失败了，那么就会调用reject(command)来拒绝这个任务。\nctl相关方法\n\n\n\n\n\n\n\n\nrunStateOf(int c)：获取ctl的高3位，即获取线程池运行状态值；workerCountOf(int c)：获取ctl的低29位，即获取线程数量值；ctlOf(int rs, int wc)：组合rs和wc，计算ctl新值；runStateLessThan(int c, int s)：c的运行状态值是否小于指定状态值s；runStateAtLeast(int c, int s)：c的运行状态值是否大于等于指定状态值s；isRunning(int c)：c 的运行状态是否是RUNNING；compareAndIncrementWorkerCount(int expect)：尝试CAS的将ctl的WorkerCount线程数量部分自增1；compareAndDecrementWorkerCount(int expect)：尝试CAS的将ctl的WorkerCount线程数量部分自减1；decrementWorkerCount()：循环尝试CAS的将ctl的WorkerCount线程数量部分自减1，直到成功为止。只有在addWorkerFailed、processWorkerExit以及getTask方法中调用。\n线程池中线程实现复用的原理线程在线程池内部其实是被封装成一个Worker对象\n\nWorker继承了AQS，也就是有一定锁的特性。\n创建线程来执行任务的方法上面提到是通过addWorker方法创建的。在创建Worker对象的时候，会把线程和任务一起封装到Worker内部，然后调用runWorker方法来让线程执行任务，接下来我们就来看一下runWorker方法。\n\n从这张图可以看出线程执行完任务不会退出的原因，runWorker内部使用了while死循环，当第一个任务执行完之后，会不断地通过getTask方法获取任务，只要能获取到任务，就会调用run方法，继续执行任务，这就是线程能够复用的主要原因。\n但是如果从getTask获取不到方法的时候，最后就会调用finally中的processWorkerExit方法，来将线程退出。\n这里有个一个细节就是，因为Worker继承了AQS，每次在执行任务之前都会调用Worker的lock方法，执行完任务之后，会调用unlock方法，这样做的目的就可以通过Woker的加锁状态就能判断出当前线程是否正在运行任务。如果想知道线程是否正在运行任务，只需要调用Woker的tryLock方法，根据是否加锁成功就能判断，加锁成功说明当前线程没有加锁，也就没有执行任务了，在调用shutdown方法关闭线程池的时候，就用这种方式来判断线程有没有在执行任务，如果没有的话，来尝试打断没有执行任务的线程。\n线程是如何获取任务的以及如何实现超时的线程在执行完任务之后，会继续从getTask方法中获取任务，获取不到就会退出。接下来我们就来看一看getTask方法的实现。\n\ngetTask方法这里有一行代码\nboolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize;\n这行代码是判断，当前过来获取任务的线程是否可以超时退出。如果allowCoreThreadTimeOut设置为true或者线程池当前的线程数大于核心线程数，也就是corePoolSize，那么该获取任务的线程就可以超时退出。\n那是怎么做到超时退出呢，就是这行核心代码\nRunnable r = timed ?\n    workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :\n    workQueue.take();\n\n会根据是否允许超时来选择调用阻塞队列workQueue的poll方法或者take方法。如果允许超时，则会调用poll方法，传入keepAliveTime，也就是构造线程池时传入的空闲时间，这个方法的意思就是从队列中阻塞keepAliveTime时间来获取任务，获取不到就会返回null；如果不允许超时，就会调用take方法，这个方法会一直阻塞获取任务，直到从队列中获取到任务位置。从这里可以看到keepAliveTime是如何使用的了。\n程池中的线程可以做到空闲一定时间就退出，利用了阻塞队列的poll方法的实现，这个方法可以指定超时时间，一旦线程达到了keepAliveTime还没有获取到任务，那么就会返回null。\n如果将allowCoreThreadTimeOut设置为true，那么所有线程走到这个timed都是true，那么所有的线程，包括核心线程都可以做到超时退出。如果你的线程池需要将核心线程超时退出，那么可以通过allowCoreThreadTimeOut方法将allowCoreThreadTimeOut变量设置为true。\n实际项目中如何合理的自定义线程池通过上面分析提到，通过Executors这个工具类来创建的线程池其实都无法满足实际的使用场景，那么在实际的项目中，到底该如何构造线程池呢，该如何合理的设置参数？\n线程数线程数的设置主要取决于业务是IO密集型还是CPU密集型。\nCPU密集型指的是任务主要使用来进行大量的计算，没有什么导致线程阻塞。一般这种场景的线程数设置为CPU核心数+1。\nIO密集型：当执行任务需要大量的io，比如磁盘io，网络io，可能会存在大量的阻塞，所以在IO密集型任务中使用多线程可以大大地加速任务的处理。一般线程数设置为 2*CPU核心数\njava中用来获取CPU核心数的方法是：\nRuntime.getRuntime().availableProcessors();\n\n线程工厂一般建议自定义线程工厂，构建线程的时候设置线程的名称，这样就在查日志的时候就方便知道是哪个线程执行的代码。\n有界队列一般需要设置有界队列的大小，比如LinkedBlockingQueue在构造的时候就可以传入参数，来限制队列中任务数据的大小，这样就不会因为无限往队列中扔任务导致系统的oom。\n","slug":"线程池","date":"2023-06-07T11:42:20.000Z","categories_index":"","tags_index":"线程池","author_index":"大宝贝的程序员"},{"id":"19e0c7830137ba378427a6cdcb7fc021","title":"Java集合高频考点","content":"1. 常见的集合有哪些？Java集合类主要由两个根接口Collection和Map派生出来的，Collection派生出了三个子接口：List、Set、Queue（Java5新增的队列），因此Java集合大致也可分成List、Set、Queue、Map四种接口体系。\n注意：Collection是一个接口，Collections是一个工具类，Map不是Collection的子接口。\nJava集合框架图如下：\n\n\n图中，List代表了有序可重复集合，可直接根据元素的索引来访问；Set代表无序不可重复集合，只能根据元素本身来访问；Queue是队列集合。\nMap代表的是存储key-value对的集合，可根据元素的key来访问value。\n上图中淡绿色背景覆盖的是集合体系中常用的实现类，分别是ArrayList、LinkedList、ArrayQueue、HashSet、TreeSet、HashMap、TreeMap等实现类。\n2. 线程安全的集合有哪些？线程不安全的呢？线程安全的：\n\nHashtable：比HashMap多了个线程安全。\nConcurrentHashMap:是一种高效但是线程安全的集合。\nVector：比Arraylist多了个同步化机制。\nStack：栈，也是线程安全的，继承于Vector。\n\n线性不安全的：\n\nHashMap\nArraylist\nLinkedList\nHashSet\nTreeSet\nTreeMap\n\n3. Arraylist与 LinkedList 异同点？\n是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全；\n底层数据结构： Arraylist 底层使用的是Object数组；LinkedList 底层使用的是双向循环链表数据结构；\n插入和删除是否受元素位置的影响： ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位&#x2F;向前移一位的操作。 LinkedList 采用链表存储，所以插入，删除元素时间复杂度不受元素位置的影响，都是近似 O（1）而数组为近似 O（n）。\n是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而ArrayList 实现了RandmoAccess 接口，所以有随机访问功能。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。\n内存空间占用： ArrayList的空间浪费主要体现在在list列表的结尾会预留一定的容量空间；LinkedList 由于需要存储指向前驱和后继节点的指针，因此占用内存较多。\n性能方面：\nArrayList 是基于数组实现的，它在内存中是连续存储的。当我们访问 ArrayList 中的一个元素时，CPU 缓存会将这个元素所在的内存块缓存起来。由于局部性原理，我们很可能在不久的将来再次访问这个内存块中的其他元素。此时，由于这些元素已经被缓存在 CPU 缓存中，我们可以快速访问它们。 \nLinkedList 是基于链表实现的，它的节点在内存中不是连续存储的。当我们访问 LinkedList 中的一个元素时，CPU 缓存会将这个元素所在的内存块缓存起来。但由于链表节点在内存中不是连续存储的，这个内存块中的其他数据很可能与当前访问的元素无关。因此，即使这个内存块被缓存在 CPU 缓存中，我们也无法利用它来加快对其他元素的访问速度。总之，由于 LinkedList 的节点在内存中不是连续存储的，它不能像 ArrayList 那样有效地利用 CPU 缓存。\n总之，由于 LinkedList 的节点在内存中不是连续存储的，它不能像 ArrayList 那样有效地利用 CPU 缓存。\n\n\n\n4. ArrayList 与 Vector 区别？\nVector是线程安全的，ArrayList不是线程安全的。其中，Vector在关键性的方法前面都加了synchronized关键字，来保证线程的安全性。如果有多个线程会访问到集合，那最好是使用 Vector，因为不需要我们自己再去考虑和编写线程安全的代码。\nArrayList在底层数组不够用时在原来的基础上扩展0.5倍，Vector是扩展1倍，这样ArrayList就有利于节约内存空间。\n\n5. 说一说ArrayList 的扩容机制？ArrayList扩容的本质就是计算出新的扩容数组的size后实例化，并将原有数组内容复制到新数组中去。默认情况下，新的容量会是原容量的1.5倍。\n有以下扩容规则：\n\nArrayList() 会使用长度为零的数组\n\nArrayList(int initialCapacity) 会使用指定容量的数组\n\npublic ArrayList(Collection&lt;? extends E&gt; c) 会使用 c 的大小作为数组容量\n\nadd(Object o) 首次扩容为 10，再次扩容为上次容量的 1.5 倍\n\naddAll(Collection c) 没有元素时，扩容为 Math.max(10, 实际元素个数)，有元素时为 Math.max(原容量 1.5 倍, 实际元素个数)\n\n\n以JDK1.8为例说明:\npublic boolean add(E e) &#123;\n    //判断是否可以容纳e，若能，则直接添加在末尾；若不能，则进行扩容，然后再把e添加在末尾\n    ensureCapacityInternal(size + 1);  // Increments modCount!!\n    //将e添加到数组末尾\n    elementData[size++] = e;\n    return true;\n    &#125;\n\n// 每次在add()一个元素时，arraylist都需要对这个list的容量进行一个判断。通过ensureCapacityInternal()方法确保当前ArrayList维护的数组具有存储新元素的能力，经过处理之后将元素存储在数组elementData的尾部\n\nprivate void ensureCapacityInternal(int minCapacity) &#123;\n      ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));\n&#125;\n\nprivate static int calculateCapacity(Object[] elementData, int minCapacity) &#123;\n        //如果传入的是个空数组则最小容量取默认容量与minCapacity之间的最大值\n        if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;\n            return Math.max(DEFAULT_CAPACITY, minCapacity);\n        &#125;\n        return minCapacity;\n    &#125;\n    \n  private void ensureExplicitCapacity(int minCapacity) &#123;\n        modCount++;\n        // 若ArrayList已有的存储能力满足最低存储要求，则返回add直接添加元素；如果最低要求的存储能力>ArrayList已有的存储能力，这就表示ArrayList的存储能力不足，因此需要调用 grow();方法进行扩容\n        if (minCapacity - elementData.length > 0)\n            grow(minCapacity);\n    &#125;\n\n\nprivate void grow(int minCapacity) &#123;\n        // 获取elementData数组的内存空间长度\n        int oldCapacity = elementData.length;\n        // 扩容至原来的1.5倍\n        int newCapacity = oldCapacity + (oldCapacity >> 1);\n        //校验容量是否够\n        if (newCapacity - minCapacity &lt; 0)\n            newCapacity = minCapacity;\n        //若预设值大于默认的最大值，检查是否溢出\n        if (newCapacity - MAX_ARRAY_SIZE > 0)\n            newCapacity = hugeCapacity(minCapacity);\n        // 调用Arrays.copyOf方法将elementData数组指向新的内存空间\n         //并将elementData的数据复制到新的内存空间\n        elementData = Arrays.copyOf(elementData, newCapacity);\n    &#125;\n\n6. Array 和 ArrayList 有什么区别？什么时候该应 Array 而不是 ArrayList 呢？\nArray 可以包含基本类型和对象类型，ArrayList 只能包含对象类型。\nArray 是一种静态数据结构，它在创建时就确定了大小，之后不能再改变。而 ArrayList 是一种动态数据结构，它可以根据需要自动扩容，ArrayList 底层是使用 Array 实现的。\nArrayList 提供了更多的方法和特性，比如：addAll()，removeAll()，iterator() 等等。\n\n在选择使用 Array 还是 ArrayList 时，需要根据实际情况进行判断。如果你需要存储基本数据类型，或者你知道数组的大小不会改变，那么使用 Array 可能是一个更好的选择。如果你需要存储对象类型，并且数组的大小可能会改变，那么使用 ArrayList 可能更方便。\n7. HashMap1）基本数据结构\n1.7 数组 + 链表\n1.8 数组 + （链表 | 红黑树）\n\n2）树化与退化树化意义\n\n红黑树用来避免 DoS 攻击，防止链表超长时性能下降，树化应当是偶然情况，是保底策略\nhash 表的查找，更新的时间复杂度是 O(1)，而红黑树的查找，更新的时间复杂度是 $O(\\log_2 n)$ ，TreeNode 占用空间也比普通 Node 的大，如非必要，尽量还是使用链表\nhash 值如果足够随机，则在 hash 表内按泊松分布，在负载因子 0.75 的情况下，长度超过 8 的链表出现概率是 0.00000006，树化阈值选择 8 就是为了让树化几率足够小\n\n树化规则\n\n当链表长度超过树化阈值 8 时，先尝试扩容来减少链表长度，如果数组容量已经 &gt;&#x3D;64，才会进行树化\n\n退化规则\n\n情况1：在扩容时如果拆分树时，树元素个数 &lt;&#x3D; 6 则会退化链表\n情况2：remove 树节点时，若 root、root.left、root.right、root.left.left 有一个为 null ，也会退化为链表，即使退化后链表的长度仍然大于 8，链表不会再次树化。因为树化的条件是在添加元素时链表长度超过阈值，而不是在删除元素时。\n\n3）索引计算索引计算方法\n\n首先，计算对象的 hashCode()\n再进行调用 HashMap 的 hash() 方法进行二次哈希，二次 hash() 是为了综合高位数据，让哈希分布更为均匀\n将哈希值与 (capacity - 1) 进行按位与运算，得到索引\n\n数组容量为何是 2 的 n 次幂\n\n计算索引时效率更高：如果是 2 的 n 次幂可以使用位与运算代替取模\n扩容时重新计算索引效率更高。当容量扩大一倍时，原来的元素可以直接留在原来的位置，或者移动到旧位置加上旧容量的位置。\n\n注意\n\n二次 hash 是为了配合 容量是 2 的 n 次幂 这一设计前提，如果 hash 表的容量不是 2 的 n 次幂，则不必二次 hash\n容量是 2 的 n 次幂 这一设计可以提高计算索引的效率，但会降低哈希的分散性，因此需要二次哈希来作为补偿。一个没有采用这一设计的典型例子是 Hashtable。\n\n4）put 与扩容put 流程\n\n首先，检查数组是否已经创建。如果还没有创建，则创建一个新的数组。如果你使用无参构造函数创建 HashMap，那么在首次调用 put 方法时，会创建一个长度为 16 的数组。这个值是 HashMap 类中定义的默认初始容量。如果你使用带参构造函数创建 HashMap，并指定了初始容量，那么在首次调用 put 方法时，会创建一个长度为大于等于指定初始容量且最小的 2 的 n 次幂的数组。例如，如果你指定初始容量为 20，那么首次调用 put 方法时会创建一个长度为 32 的数组。\n\n计算键的哈希值，并根据哈希值计算键在数组中的索引（桶下标）。\n\n检查桶下标是否已经有元素占用。如果没有，则创建一个新的 Node 占位，并返回。\n\n如果桶下标已经有元素占用，则需要进行进一步处理。如果这个位置已经是 TreeNode，则走红黑树的添加或更新逻辑；如果这个位置是普通 Node，则走链表的添加或更新逻辑。如果链表长度超过树化阈值，则进行树化。在添加元素前，还会先检查是否需要扩容，如果需要，则进行扩容。扩容后，再将新元素添加到扩容后的HashMap 中。\n\n\n\n\n\n\n\n\n\n注意：在添加元素时，如果发现数组容量已经超过阈值，则会进行扩容。会创建一个新的数组，并将原来的元素重新分配到新的数组中，再插入新的元素。这个过程中，元素会被重新计算索引，并根据新的索引分配到新的位置。如果原来的位置是链表或红黑树，则会进行拆分，将元素分配到新的位置。\n\n\n1.7 与 1.8 的区别\n\n链表插入节点时，1.7 是头插法，1.8 是尾插法\n\n1.7 是大于等于阈值且没有空位时才扩容，而 1.8 是大于阈值就扩容\n\n1.8 在扩容计算 Node 索引时，会优化\n\n\n扩容（加载）因子为何默认是 0.75f\n\n在空间占用与查询时间之间取得较好的权衡\n大于这个值，空间节省了，但链表就会比较长影响性能\n小于这个值，冲突减少了，但扩容就会更频繁，空间占用也更多\n\n5）并发问题扩容死链（1.7 会存在）\n1.7 源码如下：\nvoid transfer(Entry[] newTable, boolean rehash) &#123;\n    int newCapacity = newTable.length;\n    for (Entry&lt;K,V> e : table) &#123;\n        while(null != e) &#123;\n            Entry&lt;K,V> next = e.next;\n            if (rehash) &#123;\n                e.hash = null == e.key ? 0 : hash(e.key);\n            &#125;\n            int i = indexFor(e.hash, newCapacity);\n            e.next = newTable[i];\n            newTable[i] = e;\n            e = next;\n        &#125;\n    &#125;\n&#125;\n\n\ne 和 next 都是局部变量，用来指向当前节点和下一个节点\n线程1（绿色）的临时变量 e 和 next 刚引用了这俩节点，还未来得及移动节点，发生了线程切换，由线程2（蓝色）完成扩容和迁移\n\n\n\n线程2 扩容完成，由于头插法，链表顺序颠倒。但线程1 的临时变量 e 和 next 还引用了这俩节点，还要再来一遍迁移\n\n\n\n第一次循环\n循环接着线程切换前运行，注意此时 e 指向的是节点 a，next 指向的是节点 b\ne 头插 a 节点，注意图中画了两份 a 节点，但事实上只有一个（为了不让箭头特别乱画了两份）\n当循环结束是 e 会指向 next 也就是 b 节点\n\n\n\n\n\n第二次循环\nnext 指向了节点 a\ne 头插节点 b\n当循环结束时，e 指向 next 也就是节点 a\n\n\n\n\n\n第三次循环\nnext 指向了 null\ne 头插节点 a，a 的 next 指向了 b（之前 a.next 一直是 null），b 的 next 指向 a，死链已成\n当循环结束时，e 指向 next 也就是 null，因此第四次循环时会正常退出\n\n\n\n\n数据错乱（1.7，1.8 都会存在）\n6）key 的设计key 的设计要求\n\nHashMap 的 key 可以为 null，但 Map 的其他实现则不然\n作为 key 的对象，必须实现 hashCode 和 equals，并且 key 的内容不能修改（不可变）\nkey 的 hashCode 应该有良好的散列性\n\n如果 key 可变，例如修改了 age 会导致再次查询时查询不到\npublic class HashMapMutableKey &#123;\n    public static void main(String[] args) &#123;\n        HashMap&lt;Student, Object> map = new HashMap&lt;>();\n        Student stu = new Student(\"张三\", 18);\n        map.put(stu, new Object());\n\n        System.out.println(map.get(stu));\n\n        stu.age = 19;\n        System.out.println(map.get(stu));\n    &#125;\n\n    static class Student &#123;\n        String name;\n        int age;\n\n        public Student(String name, int age) &#123;\n            this.name = name;\n            this.age = age;\n        &#125;\n\n        public String getName() &#123;\n            return name;\n        &#125;\n\n        public void setName(String name) &#123;\n            this.name = name;\n        &#125;\n\n        public int getAge() &#123;\n            return age;\n        &#125;\n\n        public void setAge(int age) &#123;\n            this.age = age;\n        &#125;\n\n        @Override\n        public boolean equals(Object o) &#123;\n            if (this == o) return true;\n            if (o == null || getClass() != o.getClass()) return false;\n            Student student = (Student) o;\n            return age == student.age &amp;&amp; Objects.equals(name, student.name);\n        &#125;\n\n        @Override\n        public int hashCode() &#123;\n            return Objects.hash(name, age);\n        &#125;\n    &#125;\n&#125;\n\nString 对象的 hashCode() 设计\n\n目标是达到较为均匀的散列效果，每个字符串的 hashCode 足够独特\n字符串中的每个字符都可以表现为一个数字，称为 $S_i$，其中 i 的范围是 0 ~ n - 1 \n散列公式为： $S_0∗31^{(n-1)}+ S_1∗31^{(n-2)}+ … S_i ∗ 31^{(n-1-i)}+ …S_{(n-1)}∗31^0$\n31 代入公式有较好的散列特性，并且 31 * h 可以被优化为 \n即 $32 ∗h -h $\n即 $2^5  ∗h -h$\n即 $h≪5  -h$\n\n\n\n8. 解决hash冲突的办法有哪些？HashMap用的哪种？解决Hash冲突方法有:开放定址法、再哈希法、链地址法（拉链法）、建立公共溢出区。HashMap中采用的是 链地址法 。\n\n开放定址法也称为再散列法，基本思想就是，如果p=H(key)出现冲突时，则以p为基础，再次hash，p1=H(p),如果p1再次出现冲突，则以p1为基础，以此类推，直到找到一个不冲突的哈希地址pi。 因此开放定址法所需要的hash表的长度要大于等于所需要存放的元素，而且因为存在再次hash，所以只能在删除的节点上做标记，而不能真正删除节点。\n再哈希法(双重散列，多重散列)，提供多个不同的hash函数，当R1=H1(key1)发生冲突时，再计算R2=H2(key1)，直到没有冲突为止。 这样做虽然不易产生堆集，但增加了计算的时间。\n链地址法(拉链法)，将哈希值相同的元素构成一个同义词的单链表,并将单链表的头指针存放在哈希表的第i个单元中，查找、插入和删除主要在同义词链表中进行。链表法适用于经常进行插入和删除的情况。\n建立公共溢出区，将哈希表分为公共表和溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。\n\n9. 为什么在解决 hash 冲突的时候，不直接用红黑树？而选择先用链表，再转红黑树?因为红黑树需要进行左旋，右旋，变色这些操作来保持平衡，而单链表不需要。当元素小于 8 个的时候，此时做查询操作，链表结构已经能保证查询性能。当元素大于 8 个的时候， 红黑树搜索时间复杂度是 O(logn)，而链表是 O(n)，此时需要红黑树来加快查询速度，但是新增节点的效率变慢了。\n因此，如果一开始就用红黑树结构，元素太少，新增效率又比较慢，无疑这是浪费性能的。\n10. HashMap默认加载因子为什么是 0.75，不是 0.6 或者 0.8 ？回答这个问题前，我们来先看下HashMap的默认构造函数：\nint threshold;             // 容纳键值对的最大值\nfinal float loadFactor;    // 负载因子\nint modCount;  \nint size;  \n\nNode[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳键值对的最大值。threshold &#x3D; length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。\n默认的loadFactor是0.75，0.75是对空间和时间效率的一个平衡选择，一般不要修改，除非在时间和空间比较特殊的情况下 ：\n\n如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值 。\n相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。\n\n我们来追溯下作者在源码中的注释（JDK1.7）：\nAs a general rule, the default load factor (.75) offers a good tradeoff between time and space costs. Higher values decrease the space overhead but increase the lookup cost (reflected in most of the operations of the HashMap class, including get and put). The expected number of entries in the map and its load factor should be taken into account when setting its initial capacity, so as to minimize the number of rehash operations. If the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.\n\n翻译过来大概的意思是：作为一般规则，默认负载因子（0.75）在时间和空间成本上提供了很好的折衷。较高的值会降低空间开销，但提高查找成本（体现在大多数的HashMap类的操作，包括get和put）。设置初始大小时，应该考虑预计的entry数在map及其负载系数，并且尽量减少rehash操作的次数。如果初始容量大于最大条目数除以负载因子，rehash操作将不会发生。\n11. HashMap 中 key 的存储索引是怎么计算的？首先根据key的值计算出hashcode的值，然后根据hashcode计算出hash值，最后通过hash&amp;（length-1）计算得到存储的位置。看看源码的实现：\n// jdk1.7\n//方法一：\nstatic int hash(int h) &#123;\n    int h = hashSeed;\n        if (0 != h &amp;&amp; k instanceof String) &#123;\n            return sun.misc.Hashing.stringHash32((String) k);\n        &#125;\n\n    h ^= k.hashCode(); // 为第一步：取hashCode值\n    h ^= (h >>> 20) ^ (h >>> 12); \n    return h ^ (h >>> 7) ^ (h >>> 4);\n&#125;\n//方法二：\nstatic int indexFor(int h, int length) &#123;  //jdk1.7的源码，jdk1.8没有这个方法，但实现原理一样\n     return h &amp; (length-1);  //第三步：取模运算\n&#125;\n\n\n\n// jdk1.8\nstatic final int hash(Object key) &#123;   \n     int h;\n     return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);\n    /* \n     h = key.hashCode() 为第一步：取hashCode值\n     h ^ (h >>> 16)  为第二步：高位参与运算\n    */\n&#125;\n\n\n\n这里的 Hash 算法本质上就是三步：取key的 hashCode 值、根据 hashcode 计算出hash值、通过取模计算下标。其中，JDK1.7和1.8的不同之处，就在于第二步。我们来看下详细过程，以JDK1.8为例，n为table的长度。\n\n12. HashMap 的put方法流程？简要流程如下：\n\n首先根据 key 的值计算 hash 值，找到该元素在数组中存储的下标；\n\n如果数组是空的，则调用 resize 进行初始化；\n\n如果没有哈希冲突直接放在对应的数组下标里；\n\n如果冲突了，且 key 已经存在，就覆盖掉 value；\n\n如果冲突后，发现该节点是红黑树，就将这个节点挂在树上；\n\n如果冲突后是链表，判断该链表是否大于 8 ，如果大于 8 并且数组容量小于 64，就进行扩容；如果链表节点大于 8 并且数组的容量大于 64，则将这个结构转换为红黑树；否则，链表插入键值对，若 key 存在，就覆盖掉 value。\n\n\n\n13. HashMap 的扩容方式？那扩容的具体步骤是什么？让我们看看源码。\n先来看下JDK1.7 的代码：\nvoid resize(int newCapacity) &#123;   //传入新的容量\n        Entry[] oldTable = table;    //引用扩容前的Entry数组\n        int oldCapacity = oldTable.length;\n        if (oldCapacity == MAXIMUM_CAPACITY) &#123;  //扩容前的数组大小如果已经达到最大(2^30)了\n            threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了\n            return;\n        &#125;\n\n        Entry[] newTable = new Entry[newCapacity];  //初始化一个新的Entry数组\n        transfer(newTable);                         //！！将数据转移到新的Entry数组里\n        table = newTable;                           //HashMap的table属性引用新的Entry数组\n        threshold = (int)(newCapacity * loadFactor);//修改阈值\n    &#125;\n\n这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。\nvoid transfer(Entry[] newTable) &#123;\n        Entry[] src = table;                   //src引用了旧的Entry数组\n        int newCapacity = newTable.length;\n        for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组\n            Entry&lt;K,V> e = src[j];             //取得旧Entry数组的每个元素\n            if (e != null) &#123;\n              src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象）\n                do &#123;\n                    Entry&lt;K,V> next = e.next;\n                    int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置\n                    e.next = newTable[i]; //标记[1]\n                    newTable[i] = e;      //将元素放在数组上\n                    e = next;             //访问下一个Entry链上的元素\n                &#125; while (e != null);\n            &#125;\n        &#125;\n    &#125;\n\nnewTable[i] 的引用赋给了 e.next ，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到 Entry 链的尾部(如果发生了 hash 冲突的话）。\n14. 一般用什么作为HashMap的key?一般用Integer、String 这种不可变类当 HashMap 当 key，而且 String 最为常用。\n\n因为字符串是不可变的，所以在它创建的时候 hashcode 就被缓存了，不需要重新计算。这就是 HashMap 中的键往往都使用字符串的原因。\n因为获取对象的时候要用到 equals() 和 hashCode() 方法，那么键对象正确的重写这两个方法是非常重要的,这些类已经很规范的重写了 hashCode() 以及 equals() 方法。\n缓存了-128到127之间的所有整数，这是用于提高性能的一种优化技巧。在创建 Integer 对象时，如果它表示的是缓存中的一个整数，那么会直接返回缓存中的实例，而不是新建一个对象。这意味着这些整数的对象只有一个，不同的变量都引用的是同一个对象实例，这也就保证了这些对象是不可变的。\n\n15. HashMap为什么线程不安全？\n\n多线程下扩容死循环。JDK1.7中的 HashMap 使用头插法插入元素，在多线程的环境下，扩容的时候有可能导致环形链表的出现，形成死循环。因此，JDK1.8使用尾插法插入元素，在扩容时会保持链表元素原本的顺序，不会出现环形链表的问题。\n多线程的put可能导致元素的丢失。多线程同时执行 put 操作，如果计算出来的索引位置是相同的，那会造成前一个 key 被后一个 key 覆盖，从而导致元素的丢失。此问题在JDK 1.7和 JDK 1.8 中都存在。\nput和get并发时，可能导致get为null。线程1执行put时，因为元素个数超出threshold而导致rehash，线程2此时执行get，有可能导致这个问题。此问题在JDK 1.7和 JDK 1.8 中都存在。\n\n具体分析可见大佬的这篇文章：面试官：HashMap 为什么线程不安全？\n16. ConcurrentHashMap 的实现原理是什么？ConcurrentHashMap 在 JDK1.7 和 JDK1.8 的实现方式是不同的。\n先来看下JDK1.7\nJDK1.7中的ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成，即ConcurrentHashMap 把哈希桶切分成小数组（Segment ），每个小数组有 n 个 HashEntry （哈希桶）组成。\n其中，Segment 继承了 ReentrantLock，所以 Segment 是一种可重入锁，扮演锁的角色；HashEntry 用于存储键值对数据。\n\n首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问，能够实现真正的并发访问。\n在每个 segment 中，采用类似 HashMap 的结构来存储 key-value 对，也是通过哈希值与桶的个数取模的方式来计算 key 应该存储在哪个桶中。每个桶是一个链表，存储了哈希值相同的 key-value 对。在插入 key-value 对时，如果链表已经有了该 key 则更新 value，否则就添加一个新的节点到链表的头部。\n当 ConcurrentHashMap 的 size 达到阈值时，需要进行扩容操作，也就是重新计算每个 key 的哈希值，重新插入到新的 segment 中。这个操作需要将原来的 segment 全部锁定，所以需要考虑到并发扩容时可能会发生的死锁情况。JDK1.7 对此做了优化，当扩容时，只需对需要扩容的 segment 进行锁定，因为现有的 segment 不会再插入新的元素。同时，JDK1.7 也通过调整 segment 数量来避免过于分散或集中的哈希值导致的不平衡问题。\nJDK1.8\n在数据结构上， JDK1.8 中的ConcurrentHashMap 选择了与 HashMap 相同的数组+链表+红黑树结构；在锁的实现上，抛弃了原有的 Segment 分段锁，采用CAS + synchronized实现更加低粒度的锁。\n将锁的级别控制在了更细粒度的哈希桶元素级别，也就是说只需要锁住这个链表头结点（红黑树的根节点），就不会影响其他的哈希桶元素的读写，大大提高了并发度。\n\n举个例子，假设有两个线程 A 和 B，同时要向 ConcurrentHashMap 中添加一个 key-value 对（key&#x3D;“apple”，value&#x3D;1）。在 JDK1.7 中，如果这两个 key-value 要被映射到同一个段里，那么只能依次访问，虽然锁的粒度比 Hashtable 更小，性能也有所提高，但是还是存在一定的性能瓶颈。而在 JDK1.8 中，则不需要锁的操作，它使用 CAS 技术通过一次操作就能完成修改，从而避免了线程之间的竞争和阻塞，提高了并发效率。\n17. ConcurrentHashMap 的 put 方法执行逻辑是什么？JDK1.7\n首先，会尝试获取锁，如果获取失败，利用scanAndLockForPut()自旋获取锁；如果自旋重试的次数超过 64 次，则改为阻塞获取锁。\n获取到锁后：\n\n将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry。\n遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。\n不为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。\n释放 Segment 的锁。\n\nJDK1.8\n大致可以分为以下步骤：\n\n根据 key 计算出 hash值。\n判断是否需要进行初始化。\n定位到 Node，拿到首节点 f，判断首节点 f：\n如果为 null ，则通过cas的方式尝试添加。\n如果为 f.hash = MOVED = -1 ，说明其他线程在扩容，参与一起扩容。\n如果都不满足 ，synchronized 锁住 f 节点，判断是链表还是红黑树，遍历插入。\n\n\n当在链表长度达到8的时候，数组扩容或者将链表转换为红黑树。\n\n源码分析可看这篇文章：面试 ConcurrentHashMap ，看这一篇就够了！\n18. ConcurrentHashMap 的 get 方法是否要加锁，为什么？get 方法不需要加锁。因为 Node 的元素 val 和指针 next 是用 volatile 修饰的，在多线程环境下线程A修改结点的val或者新增节点的时候是对线程B可见的。\n这也是它比其他并发集合比如 Hashtable、用 Collections.synchronizedMap()包装的 HashMap 安全效率高的原因之一。\nstatic class Node&lt;K,V> implements Map.Entry&lt;K,V> &#123;\n    final int hash;\n    final K key;\n    //可以看到这些都用了volatile修饰\n    volatile V val;\n    volatile Node&lt;K,V> next;\n&#125;\n\n\n\n19. get方法不需要加锁与volatile修饰的哈希桶有关吗？没有关系。哈希桶table用volatile修饰主要是保证在数组扩容的时候保证可见性。\nstatic final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123;\n\n    &#x2F;&#x2F; 存放数据的桶\n    transient volatile HashEntry&lt;K,V&gt;[] table;\n\n\n\n20. ConcurrentHashMap 不支持 key 或者 value 为 null 的原因？我们先来说value 为什么不能为 null ，因为ConcurrentHashMap 是用于多线程的 ，如果map.get(key)得到了 null ，无法判断，是映射的value是 null ，还是没有找到对应的key而为 null ，这就有了二义性。\n而用于单线程状态的HashMap却可以用containsKey(key) 去判断到底是否包含了这个 null 。\n我们用反证法来推理：\n假设ConcurrentHashMap 允许存放值为 null 的value，这时有A、B两个线程，线程A调用ConcurrentHashMap .get(key)方法，返回为 null ，我们不知道这个 null 是没有映射的 null ，还是存的值就是 null 。\n假设此时，返回为 null 的真实情况是没有找到对应的key。那么，我们可以用ConcurrentHashMap .containsKey(key)来验证我们的假设是否成立，我们期望的结果是返回false。\n但是在我们调用ConcurrentHashMap .get(key)方法之后，containsKey方法之前，线程B执行了ConcurrentHashMap .put(key, null )的操作。那么我们调用containsKey方法返回的就是true了，这就与我们的假设的真实情况不符合了，这就有了二义性。\n至于ConcurrentHashMap 中的key为什么也不能为 null 的问题，源码就是这样写的。可能是因为作者Doug不喜欢 null ，所以在设计之初就不允许了 null 的key存在。\n21. ConcurrentHashMap 的并发度是多少？在JDK1.7中，并发度默认是16，这个值可以在构造函数中设置。如果自己设置了并发度，ConcurrentHashMap 会使用大于等于该值的最小的2的幂指数作为实际并发度，也就是比如你设置的值是17，那么实际并发度是32。\n22. ConcurrentHashMap 迭代器是强一致性还是弱一致性？与HashMap迭代器是强一致性不同，ConcurrentHashMap 迭代器是弱一致性。\nConcurrentHashMap 的迭代器创建后，就会按照哈希表结构遍历每个元素，但在遍历过程中，内部元素可能会发生变化，如果变化发生在已遍历过的部分，迭代器就不会反映出来，而如果变化发生在未遍历过的部分，迭代器就会发现并反映出来，这就是弱一致性。\n这样迭代器线程可以使用原来老的数据，而写线程也可以并发的完成改变，更重要的，这保证了多个线程并发执行的连续性和扩展性，是性能提升的关键。\n23. JDK1.7与JDK1.8 中ConcurrentHashMap 的区别？\n数据结构：取消了Segment分段锁的数据结构，取而代之的是数组+链表+红黑树的结构。\n保证线程安全机制：JDK1.7采用Segment的分段锁机制实现线程安全，其中segment继承自ReentrantLock。JDK1.8 采用CAS+Synchronized保证线程安全。\n锁的粒度：原来是对需要进行数据操作的Segment加锁，现调整为对每个数组元素加锁（Node）。\n链表转化为红黑树:定位结点的hash算法简化会带来弊端,Hash冲突加剧,因此在链表节点数量大于8时，会将链表转化为红黑树进行存储。\n\n24. ConcurrentHashMap 和Hashtable的效率哪个更高？为什么？ConcurrentHashMap 的效率要高于Hashtable，因为Hashtable给整个哈希表加了一把大锁从而实现线程安全。而ConcurrentHashMap 的锁粒度更低，在JDK1.7中采用分段锁实现线程安全，在JDK1.8 中采用CAS+Synchronized实现线程安全。\n25. 说一下Hashtable的锁机制 ?Hashtable是使用Synchronized来实现线程安全的，给整个哈希表加了一把大锁，多线程访问时候，只要有一个线程访问或操作该对象，那其他线程只能阻塞等待需要的锁被释放，在竞争激烈的多线程场景中性能就会非常差！\n\n26. 多线程下安全的操作map还有其他方法吗？还可以使用Collections.synchronizedMap方法，对方法进行加同步锁\nprivate static class SynchronizedMap&lt;K,V>\n        implements Map&lt;K,V>, Serializable &#123;\n        private static final long serialVersionUID = 1978198479659022715L;\n\n        private final Map&lt;K,V> m;     // Backing Map\n        final Object      mutex;        // Object on which to synchronize\n\n        SynchronizedMap(Map&lt;K,V> m) &#123;\n            this.m = Objects.requireNon null (m);\n            mutex = this;\n        &#125;\n\n        SynchronizedMap(Map&lt;K,V> m, Object mutex) &#123;\n            this.m = m;\n            this.mutex = mutex;\n        &#125;\n    // 省略部分代码\n    &#125;\n\n\n\n如果传入的是 HashMap 对象，其实也是对 HashMap 做的方法做了一层包装，里面使用对象锁来保证多线程场景下，线程安全，本质也是对 HashMap 进行全表锁。在竞争激烈的多线程环境下性能依然也非常差，不推荐使用！\n27. HashSet 和 HashMap 区别?\n补充HashSet的实现：HashSet的底层其实就是HashMap，只不过我们HashSet是实现了Set接口并且把数据作为K值，而V值一直使用一个相同的虚值来保存。如源码所示：\npublic boolean add(E e) &#123;\n    return map.put(e, PRESENT)==null;// 调用HashMap的put方法,PRESENT是一个至始至终都相同的虚值\n&#125;\n\n由于HashMap的K值本身就不允许重复，并且在HashMap中如果K&#x2F;V相同时，会用新的V覆盖掉旧的V，然后返回旧的V，那么在HashSet中执行这一句话始终会返回一个false，导致插入失败，这样就保证了数据的不可重复性。\n28. Collection框架中实现比较要怎么做？第一种，实体类实现Comparable接口，并实现 compareTo(T t) 方法，称为内部比较器。\n第二种，创建一个外部比较器，这个外部比较器要实现Comparator接口的 **compare(**T t1, T t2)方法。\n29. Iterator 和 ListIterator 有什么区别？\n遍历。使用Iterator，可以遍历所有集合，如Map，List，Set；但只能在向前方向上遍历集合中的元素。\n\n使用ListIterator，只能遍历List实现的对象，但可以向前和向后遍历集合中的元素。\n\n添加元素。Iterator无法向集合中添加元素；而，ListIteror可以向集合添加元素。\n修改元素。Iterator无法修改集合中的元素；而，ListIterator可以使用set()修改集合中的元素。\n索引。Iterator无法获取集合中元素的索引；而，使用ListIterator，可以获取集合中元素的索引。\n\n30. 讲一讲快速失败(fail-fast)和安全失败(fail-safe)快速失败（fail—fast）\n\n在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。\n原理：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()&#x2F;next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。\n注意：这里异常的抛出条件是检测到 modCount！&#x3D;expectedmodCount 这个条件。如果集合发生变化时修改modCount值刚好又设置为了expectedmodCount值，则异常不会抛出。因此，不能依赖于这个异常是否抛出而进行并发操作的编程，这个异常只建议用于检测并发修改的bug。\n场景：java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改，比如HashMap、ArrayList 这些集合类。\n\n安全失败（fail—safe）\n\n采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。\n原理：由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发Concurrent Modification Exception。\n缺点：基于拷贝内容的优点是避免了Concurrent Modification Exception，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。\n场景：java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改，比如：ConcurrentHashMap。\n\n","slug":"Java集合高频考点","date":"2023-06-06T04:20:49.000Z","categories_index":"","tags_index":"Java","author_index":"大宝贝的程序员"},{"id":"37c665cf8e0e08ae6a11b540ba094729","title":"Redis实战-用户签到_UV统计","content":"用户签到用户签到-BitMap功能演示我们针对签到功能完全可以通过mysql来完成，比如说以下这张表\n\n用户一次签到，就是一条记录，假如有1000万用户，平均每人每年签到次数为10次，则这张表一年的数据量为 1亿条\n每签到一次需要使用（8 + 8 + 1 + 1 + 3 + 1）共22 字节的内存，一个月则最多需要600多字节\n我们如何能够简化一点呢？其实可以考虑小时候一个挺常见的方案，就是小时候，咱们准备一张小小的卡片，你只要签到就打上一个勾，我最后判断你是否签到，其实只需要到小卡片上看一看就知道了\n我们可以采用类似这样的方案来实现我们的签到需求。\n我们按月来统计用户签到信息，签到记录为1，未签到则记录为0.\n把每一个bit位对应当月的每一天，形成了映射关系。用0和1标示业务状态，这种思路就称为位图（BitMap）。这样我们就用极小的空间，来实现了大量数据的表示\nRedis中是利用string类型数据结构实现BitMap，因此最大上限是512M，转换为bit则是 2^32个bit位。\n\nBitMap的操作命令有：\n\nSETBIT：向指定位置（offset）存入一个0或1\nGETBIT ：获取指定位置（offset）的bit值\nBITCOUNT ：统计BitMap中值为1的bit位的数量\nBITFIELD ：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值\nBITFIELD_RO ：获取BitMap中bit数组，并以十进制形式返回\nBITOP ：将多个BitMap的结果做位运算（与 、或、异或）\nBITPOS ：查找bit数组中指定范围内第一个0或1出现的位置\n\n用户签到-实现签到功能需求：实现签到接口，将当前用户当天签到信息保存到Redis中\n思路：我们可以把年和月作为bitMap的key，然后保存到一个bitMap中，每次签到就到对应的位上把数字从0变成1，只要对应是1，就表明说明这一天已经签到了，反之则没有签到。\n我们通过接口文档发现，此接口并没有传递任何的参数，没有参数怎么确实是哪一天签到呢？这个很容易，可以通过后台代码直接获取即可，然后到对应的地址上去修改bitMap。\n\n代码\nUserController\n@PostMapping(\"/sign\")\npublic Result sign()&#123;\n   return userService.sign();\n&#125;\n\nUserServiceImpl\n@Override\npublic Result sign() &#123;\n    // 1.获取当前登录用户\n    Long userId = UserHolder.getUser().getId();\n    // 2.获取日期\n    LocalDateTime now = LocalDateTime.now();\n    // 3.拼接key\n    String keySuffix = now.format(DateTimeFormatter.ofPattern(\":yyyyMM\"));\n    String key = USER_SIGN_KEY + userId + keySuffix;\n    // 4.获取今天是本月的第几天\n    int dayOfMonth = now.getDayOfMonth();\n    // 5.写入Redis SETBIT key offset 1\n    stringRedisTemplate.opsForValue().setBit(key, dayOfMonth - 1, true);\n    return Result.ok();\n&#125;\n\n用户签到-签到统计问题1：什么叫做连续签到天数？从最后一次签到开始向前统计，直到遇到第一次未签到为止，计算总的签到次数，就是连续签到天数。\n\nJava逻辑代码：获得当前这个月的最后一次签到数据，定义一个计数器，然后不停的向前统计，直到获得第一个非0的数字即可，每得到一个非0的数字计数器+1，直到遍历完所有的数据，就可以获得当前月的签到总天数了\n问题2：如何得到本月到今天为止的所有签到数据？\n  BITFIELD key GET u[dayOfMonth] 0\n假设今天是10号，那么我们就可以从当前月的第一天开始，获得到当前这一天的位数，是10号，那么就是10位，去拿这段时间的数据，就能拿到所有的数据了，那么这10天里边签到了多少次呢？统计有多少个1即可。\n问题3：如何从后向前遍历每个bit位？\n注意：bitMap返回的数据是10进制，假如说返回一个数字8，那么我哪儿知道到底哪些是0，哪些是1呢？我们只需要让得到的10进制数字和1做与运算就可以了，因为1只有遇见1 才是1，其他数字都是0 ，我们把签到结果和1进行与操作，每与一次，就把签到结果向右移动一位，依次内推，我们就能完成逐个遍历的效果了。\n需求：实现下面接口，统计当前用户截止当前时间在本月的连续签到天数\n有用户有时间我们就可以组织出对应的key，此时就能找到这个用户截止这天的所有签到记录，再根据这套算法，就能统计出来他连续签到的次数了\n\n代码\nUserController\n@GetMapping(\"/sign/count\")\npublic Result signCount()&#123;\n    return userService.signCount();\n&#125;\n\nUserServiceImpl\n@Override\npublic Result signCount() &#123;\n    // 1.获取当前登录用户\n    Long userId = UserHolder.getUser().getId();\n    // 2.获取日期\n    LocalDateTime now = LocalDateTime.now();\n    // 3.拼接key\n    String keySuffix = now.format(DateTimeFormatter.ofPattern(\":yyyyMM\"));\n    String key = USER_SIGN_KEY + userId + keySuffix;\n    // 4.获取今天是本月的第几天\n    int dayOfMonth = now.getDayOfMonth();\n    // 5.获取本月截止今天为止的所有的签到记录，返回的是一个十进制的数字 BITFIELD sign:5:202203 GET u14 0\n    List&lt;Long> result = stringRedisTemplate.opsForValue().bitField(\n            key,\n            BitFieldSubCommands.create()\n                    .get(BitFieldSubCommands.BitFieldType.unsigned(dayOfMonth)).valueAt(0)\n    );\n    if (result == null || result.isEmpty()) &#123;\n        // 没有任何签到结果\n        return Result.ok(0);\n    &#125;\n    Long num = result.get(0);\n    if (num == null || num == 0) &#123;\n        return Result.ok(0);\n    &#125;\n    // 6.循环遍历\n    int count = 0;\n    while (true) &#123;\n        // 6.1.让这个数字与1做与运算，得到数字的最后一个bit位  // 判断这个bit位是否为0\n        if ((num &amp; 1) == 0) &#123;\n            // 如果为0，说明未签到，结束\n            break;\n        &#125;else &#123;\n            // 如果不为0，说明已签到，计数器+1\n            count++;\n        &#125;\n        // 把数字右移一位，抛弃最后一个bit位，继续下一个bit位\n        num >>>= 1;\n    &#125;\n    return Result.ok(count);\n&#125;\n\n关于使用bitmap来解决缓存穿透的方案回顾缓存穿透：\n发起了一个数据库不存在的，redis里边也不存在的数据，通常你可以把他看成一个攻击\n解决方案：\n\n判断id&lt;0\n\n如果数据库是空，那么就可以直接往redis里边把这个空数据缓存起来\n\n\n第一种解决方案：遇到的问题是如果用户访问的是id不存在的数据，则此时就无法生效\n第二种解决方案：遇到的问题是：如果是不同的id那就可以防止下次过来直击数据\n所以我们如何解决呢？\n我们可以将数据库的数据，所对应的id写入到一个list集合中，当用户过来访问的时候，我们直接去判断list中是否包含当前的要查询的数据，如果说用户要查询的id数据并不在list集合中，则直接返回，如果list中包含对应查询的id数据，则说明不是一次缓存穿透数据，则直接放行。\n\n现在的问题是这个主键其实并没有那么短，而是很长的一个 主键\n哪怕你单独去提取这个主键，但是在11年左右，淘宝的商品总量就已经超过10亿个\n所以如果采用以上方案，这个list也会很大，所以我们可以使用bitmap来减少list的存储空间\n我们可以把list数据抽象成一个非常大的bitmap，我们不再使用list，而是将db中的id数据利用哈希思想，比如：\nid % bitmap.size  &#x3D; 算出当前这个id对应应该落在bitmap的哪个索引上，然后将这个值从0变成1，然后当用户来查询数据时，此时已经没有了list，让用户用他查询的id去用相同的哈希算法， 算出来当前这个id应当落在bitmap的哪一位，然后判断这一位是0，还是1，如果是0则表明这一位上的数据一定不存在，  采用这种方式来处理，需要重点考虑一个事情，就是误差率，所谓的误差率就是指当发生哈希冲突的时候，产生的误差。\n\nUV统计UV统计-HyperLogLog首先我们搞懂两个概念：\n\nUV：全称Unique Visitor，也叫独立访客量，是指通过互联网访问、浏览这个网页的自然人。1天内同一个用户多次访问该网站，只记录1次。\nPV：全称Page View，也叫页面访问量或点击量，用户每访问网站的一个页面，记录1次PV，用户多次打开页面，则记录多次PV。往往用来衡量网站的流量。\n\n通常来说UV会比PV大很多，所以衡量同一个网站的访问量，我们需要综合考虑很多因素，所以我们只是单纯的把这两个值作为一个参考值\nUV统计在服务端做会比较麻烦，因为要判断该用户是否已经统计过了，需要将统计过的用户信息保存。但是如果每个访问的用户都保存到Redis中，数据量会非常恐怖，那怎么处理呢？\nHyperloglog(HLL)是从Loglog算法派生的概率算法，用于确定非常大的集合的基数，而不需要存储其所有值。相关算法原理大家可以参考：https://juejin.cn/post/6844903785744056333#heading-0Redis中的HLL是基于string结构实现的，单个HLL的内存永远小于16kb，内存占用低的令人发指！作为代价，其测量结果是概率性的，有小于0.81％的误差。不过对于UV统计来说，这完全可以忽略。\n\n12.2 UV统计-测试百万数据的统计测试思路：我们直接利用单元测试，向HyperLogLog中添加100万条数据，看看内存占用和统计效果如何\n\n经过测试：我们会发生他的误差是在允许范围内，并且内存占用极小\n","slug":"Redis实战-用户签到","date":"2023-06-06T02:48:05.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"9da38b0fade8e3816513eac32a87f012","title":"Redis实战-附近商户","content":"附近商户附近商户-GEO数据结构的基本用法GEO就是Geolocation的简写形式，代表地理坐标。Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。常见的命令有：\n\nGEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member）\nGEODIST：计算指定的两个点之间的距离并返回\nGEOHASH：将指定member的坐标转为hash字符串形式并返回\nGEOPOS：返回指定member的坐标\nGEORADIUS：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.以后已废弃\nGEOSEARCH：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能\nGEOSEARCHSTORE：与GEOSEARCH功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能\n\n附近商户-导入店铺数据到GEO具体场景说明：\n\n当我们点击美食之后，会出现一系列的商家，商家中可以按照多种排序方式，我们此时关注的是距离，这个地方就需要使用到我们的GEO，向后台传入当前app收集的地址(我们此处是写死的) ，以当前坐标作为圆心，同时绑定相同的店家类型type，以及分页信息，把这几个条件传入后台，后台查询出对应的数据再返回。\n\n我们要做的事情是：将数据库表中的数据导入到redis中去，redis中的GEO，GEO在redis中就一个menber和一个经纬度，我们把x和y轴传入到redis做的经纬度位置去，但我们不能把所有的数据都放入到menber中去，毕竟作为redis是一个内存级数据库，如果存海量数据，redis还是力不从心，所以我们在这个地方存储他的id即可。\n但是这个时候还有一个问题，就是在redis中并没有存储type，所以我们无法根据type来对数据进行筛选，所以我们可以按照商户类型做分组，类型相同的商户作为同一组，以typeId为key存入同一个GEO集合中即可\n代码\nHmDianPingApplicationTests\n@Test\nvoid loadShopData() &#123;\n    // 1.查询店铺信息\n    List&lt;Shop> list = shopService.list();\n    // 2.把店铺分组，按照typeId分组，typeId一致的放到一个集合\n    Map&lt;Long, List&lt;Shop>> map = list.stream().collect(Collectors.groupingBy(Shop::getTypeId));\n    // 3.分批完成写入Redis\n    for (Map.Entry&lt;Long, List&lt;Shop>> entry : map.entrySet()) &#123;\n        // 3.1.获取类型id\n        Long typeId = entry.getKey();\n        String key = SHOP_GEO_KEY + typeId;\n        // 3.2.获取同类型的店铺的集合\n        List&lt;Shop> value = entry.getValue();\n        List&lt;RedisGeoCommands.GeoLocation&lt;String>> locations = new ArrayList&lt;>(value.size());\n        // 3.3.写入redis GEOADD key 经度 纬度 member\n        for (Shop shop : value) &#123;\n            // stringRedisTemplate.opsForGeo().add(key, new Point(shop.getX(), shop.getY()), shop.getId().toString());\n            locations.add(new RedisGeoCommands.GeoLocation&lt;>(\n                    shop.getId().toString(),\n                    new Point(shop.getX(), shop.getY())\n            ));\n        &#125;\n        stringRedisTemplate.opsForGeo().add(key, locations);\n    &#125;\n&#125;\n\n附近商户-实现附近商户功能SpringDataRedis的2.3.9版本并不支持Redis 6.2提供的GEOSEARCH命令，因此我们需要提示其版本，修改自己的POM\n第一步：导入pom\n&lt;dependency>\n    &lt;groupId>org.springframework.boot&lt;/groupId>\n    &lt;artifactId>spring-boot-starter-data-redis&lt;/artifactId>\n    &lt;exclusions>\n        &lt;exclusion>\n            &lt;artifactId>spring-data-redis&lt;/artifactId>\n            &lt;groupId>org.springframework.data&lt;/groupId>\n        &lt;/exclusion>\n        &lt;exclusion>\n            &lt;artifactId>lettuce-core&lt;/artifactId>\n            &lt;groupId>io.lettuce&lt;/groupId>\n        &lt;/exclusion>\n    &lt;/exclusions>\n&lt;/dependency>\n&lt;dependency>\n    &lt;groupId>org.springframework.data&lt;/groupId>\n    &lt;artifactId>spring-data-redis&lt;/artifactId>\n    &lt;version>2.6.2&lt;/version>\n&lt;/dependency>\n&lt;dependency>\n    &lt;groupId>io.lettuce&lt;/groupId>\n    &lt;artifactId>lettuce-core&lt;/artifactId>\n    &lt;version>6.1.6.RELEASE&lt;/version>\n&lt;/dependency>\n\n第二步：\nShopController\n@GetMapping(\"/of/type\")\npublic Result queryShopByType(\n        @RequestParam(\"typeId\") Integer typeId,\n        @RequestParam(value = \"current\", defaultValue = \"1\") Integer current,\n        @RequestParam(value = \"x\", required = false) Double x,\n        @RequestParam(value = \"y\", required = false) Double y\n) &#123;\n   return shopService.queryShopByType(typeId, current, x, y);\n&#125;\n\nShopServiceImpl\n@Override\n    public Result queryShopByType(Integer typeId, Integer current, Double x, Double y) &#123;\n        // 1.判断是否需要根据坐标查询\n        if (x == null || y == null) &#123;\n            // 不需要坐标查询，按数据库查询\n            Page&lt;Shop> page = query()\n                    .eq(\"type_id\", typeId)\n                    .page(new Page&lt;>(current, SystemConstants.DEFAULT_PAGE_SIZE));\n            // 返回数据\n            return Result.ok(page.getRecords());\n        &#125;\n\n        // 2.计算分页参数\n        int from = (current - 1) * SystemConstants.DEFAULT_PAGE_SIZE;\n        int end = current * SystemConstants.DEFAULT_PAGE_SIZE;\n\n        // 3.查询redis、按照距离排序、分页。结果：shopId、distance\n        String key = SHOP_GEO_KEY + typeId;\n        GeoResults&lt;RedisGeoCommands.GeoLocation&lt;String>> results = stringRedisTemplate.opsForGeo() // GEOSEARCH key BYLONLAT x y BYRADIUS 10 WITHDISTANCE\n                .search(\n                        key,\n                        GeoReference.fromCoordinate(x, y),\n                        new Distance(5000),\n                        RedisGeoCommands.GeoSearchCommandArgs.newGeoSearchArgs().includeDistance().limit(end)\n                );\n        // 4.解析出id\n        if (results == null) &#123;\n            return Result.ok(Collections.emptyList());\n        &#125;\n        List&lt;GeoResult&lt;RedisGeoCommands.GeoLocation&lt;String>>> list = results.getContent();\n        if (list.size() &lt;= from) &#123;\n            // 没有下一页了，结束\n            return Result.ok(Collections.emptyList());\n        &#125;\n        // 4.1.截取 from ~ end的部分\n        List&lt;Long> ids = new ArrayList&lt;>(list.size());\n        Map&lt;String, Distance> distanceMap = new HashMap&lt;>(list.size());\n        list.stream().skip(from).forEach(result -> &#123;\n            // 4.2.获取店铺id\n            String shopIdStr = result.getContent().getName();\n            ids.add(Long.valueOf(shopIdStr));\n            // 4.3.获取距离\n            Distance distance = result.getDistance();\n            distanceMap.put(shopIdStr, distance);\n        &#125;);\n        // 5.根据id查询Shop\n        String idStr = StrUtil.join(\",\", ids);\n        List&lt;Shop> shops = query().in(\"id\", ids).last(\"ORDER BY FIELD(id,\" + idStr + \")\").list();\n        for (Shop shop : shops) &#123;\n            shop.setDistance(distanceMap.get(shop.getId().toString()).getValue());\n        &#125;\n        // 6.返回\n        return Result.ok(shops);\n    &#125;\n\n","slug":"Redis实战-附近商户","date":"2023-06-06T02:39:30.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"aa18ca8bbe4604cca91aa58ef474fe02","title":"Redis实战-好友关注","content":"好友关注好友关注-关注和取消关注针对用户的操作：可以对用户进行关注和取消关注功能。\n\n实现思路：\n需求：基于该表数据结构，实现两个接口：\n\n关注和取关接口\n判断是否关注的接口\n\n关注是User之间的关系，是博主与粉丝的关系，数据库中有一张tb_follow表来标示：\n\n注意: 这里需要把主键修改为自增长，简化开发。\nFollowController\n//关注\n@PutMapping(\"/&#123;id&#125;/&#123;isFollow&#125;\")\npublic Result follow(@PathVariable(\"id\") Long followUserId, @PathVariable(\"isFollow\") Boolean isFollow) &#123;\n    return followService.follow(followUserId, isFollow);\n&#125;\n//取消关注\n@GetMapping(\"/or/not/&#123;id&#125;\")\npublic Result isFollow(@PathVariable(\"id\") Long followUserId) &#123;\n      return followService.isFollow(followUserId);\n&#125;\n\nFollowService\n//取消关注service\n@Override\npublic Result isFollow(Long followUserId) &#123;\n        // 1.获取登录用户\n        Long userId = UserHolder.getUser().getId();\n        // 2.查询是否关注 select count(*) from tb_follow where user_id = ? and follow_user_id = ?\n        Integer count = query().eq(\"user_id\", userId).eq(\"follow_user_id\", followUserId).count();\n        // 3.判断\n        return Result.ok(count > 0);\n    &#125;\n\n //关注service\n @Override\n    public Result follow(Long followUserId, Boolean isFollow) &#123;\n        // 1.获取登录用户\n        Long userId = UserHolder.getUser().getId();\n        String key = \"follows:\" + userId;\n        // 1.判断到底是关注还是取关\n        if (isFollow) &#123;\n            // 2.关注，新增数据\n            Follow follow = new Follow();\n            follow.setUserId(userId);\n            follow.setFollowUserId(followUserId);\n            boolean isSuccess = save(follow);\n\n        &#125; else &#123;\n            // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ?\n            remove(new QueryWrapper&lt;Follow>()\n                    .eq(\"user_id\", userId).eq(\"follow_user_id\", followUserId));\n\n        &#125;\n        return Result.ok();\n    &#125;\n\n好友关注-共同关注共同关注如何实现：\n需求：利用Redis中恰当的数据结构，实现共同关注功能。在博主个人页面展示出当前用户与博主的共同关注呢。\n当然是使用我们之前学习过的set集合咯，在set集合中，有交集并集补集的api，我们可以把两人的关注的人分别放入到一个set集合中，然后再通过api去查看这两个set集合中的交集数据。\n\n我们先来改造当前的关注列表\n改造原因是因为我们需要在用户关注了某位用户后，需要将数据放入到set集合中，方便后续进行共同关注，同时当取消关注时，也需要从set集合中进行删除\nFollowServiceImpl\n@Override\npublic Result follow(Long followUserId, Boolean isFollow) &#123;\n    // 1.获取登录用户\n    Long userId = UserHolder.getUser().getId();\n    String key = \"follows:\" + userId;\n    // 1.判断到底是关注还是取关\n    if (isFollow) &#123;\n        // 2.关注，新增数据\n        Follow follow = new Follow();\n        follow.setUserId(userId);\n        follow.setFollowUserId(followUserId);\n        boolean isSuccess = save(follow);\n        if (isSuccess) &#123;\n            // 把关注用户的id，放入redis的set集合 sadd userId followerUserId\n            stringRedisTemplate.opsForSet().add(key, followUserId.toString());\n        &#125;\n    &#125; else &#123;\n        // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ?\n        boolean isSuccess = remove(new QueryWrapper&lt;Follow>()\n                .eq(\"user_id\", userId).eq(\"follow_user_id\", followUserId));\n        if (isSuccess) &#123;\n            // 把关注用户的id从Redis集合中移除\n            stringRedisTemplate.opsForSet().remove(key, followUserId.toString());\n        &#125;\n    &#125;\n    return Result.ok();\n&#125;\n\n具体的关注代码：\nFollowServiceImpl\n@Override\npublic Result followCommons(Long id) &#123;\n    // 1.获取当前用户\n    Long userId = UserHolder.getUser().getId();\n    String key = \"follows:\" + userId;\n    // 2.求交集\n    String key2 = \"follows:\" + id;\n    Set&lt;String> intersect = stringRedisTemplate.opsForSet().intersect(key, key2);\n    if (intersect == null || intersect.isEmpty()) &#123;\n        // 无交集\n        return Result.ok(Collections.emptyList());\n    &#125;\n    // 3.解析id集合\n    List&lt;Long> ids = intersect.stream().map(Long::valueOf).collect(Collectors.toList());\n    // 4.查询用户\n    List&lt;UserDTO> users = userService.listByIds(ids)\n            .stream()\n            .map(user -> BeanUtil.copyProperties(user, UserDTO.class))\n            .collect(Collectors.toList());\n    return Result.ok(users);\n&#125;\n\n好友关注-Feed流实现方案当我们关注了用户后，这个用户发了动态，那么我们应该把这些数据推送给用户，这个需求，其实我们又把他叫做Feed流，关注推送也叫做Feed流，直译为投喂。为用户持续的提供“沉浸式”的体验，通过无限下拉刷新获取新的信息。\n对于传统的模式的内容解锁：我们是需要用户去通过搜索引擎或者是其他的方式去解锁想要看的内容\n\n对于新型的Feed流的的效果：不需要我们用户再去推送信息，而是系统分析用户到底想要什么，然后直接把内容推送给用户，从而使用户能够更加的节约时间，不用主动去寻找。\n\nFeed流的实现有两种模式：\nFeed流产品有两种常见模式：Timeline：不做内容筛选，简单的按照内容发布时间排序，常用于好友或关注。例如朋友圈\n\n优点：信息全面，不会有缺失。并且实现也相对简单\n缺点：信息噪音较多，用户不一定感兴趣，内容获取效率低\n\n智能排序：利用智能算法屏蔽掉违规的、用户不感兴趣的内容。推送用户感兴趣信息来吸引用户\n\n优点：投喂用户感兴趣信息，用户粘度很高，容易沉迷\n缺点：如果算法不精准，可能起到反作用本例中的个人页面，是基于关注的好友来做Feed流，因此采用Timeline的模式。该模式的实现方案有三种：\n\n我们本次针对好友的操作，采用的就是Timeline的方式，只需要拿到我们关注用户的信息，然后按照时间排序即可\n，因此采用Timeline的模式。该模式的实现方案有三种：\n\n拉模式\n推模式\n推拉结合\n\n拉模式：也叫做读扩散\n该模式的核心含义就是：当张三和李四和王五发了消息后，都会保存在自己的邮箱中，假设赵六要读取信息，那么他会从读取他自己的收件箱，此时系统会从他关注的人群中，把他关注人的信息全部都进行拉取，然后在进行排序\n优点：比较节约空间，因为赵六在读信息时，并没有重复读取，而且读取完之后可以把他的收件箱进行清除。\n缺点：比较延迟，当用户读取数据时才去关注的人里边去读取数据，假设用户关注了大量的用户，那么此时就会拉取海量的内容，对服务器压力巨大。\n\n推模式：也叫做写扩散。\n推模式是没有写邮箱的，当张三写了一个内容，此时会主动的把张三写的内容发送到他的粉丝收件箱中去，假设此时李四再来读取，就不用再去临时拉取了\n优点：时效快，不用临时拉取\n缺点：内存压力大，假设一个大V写信息，很多人关注他， 就会写很多分数据到粉丝那边去\n\n推拉结合模式：也叫做读写混合，兼具推和拉两种模式的优点。\n推拉模式是一个折中的方案，站在发件人这一段，如果是个普通的人，那么我们采用写扩散的方式，直接把数据写入到他的粉丝中去，因为普通的人他的粉丝关注量比较小，所以这样做没有压力，如果是大V，那么他是直接将数据先写入到一份到发件箱里边去，然后再直接写一份到活跃粉丝收件箱里边去，现在站在收件人这端来看，如果是活跃粉丝，那么大V和普通的人发的都会直接写入到自己收件箱里边来，而如果是普通的粉丝，由于他们上线不是很频繁，所以等他们上线时，再从发件箱里边去拉信息。\n\n好友关注-推送到粉丝收件箱需求：\n\n修改新增探店笔记的业务，在保存blog到数据库的同时，推送到粉丝的收件箱\n收件箱满足可以根据时间戳排序，必须用Redis的数据结构实现\n查询收件箱数据时，可以实现分页查询\n\nFeed流中的数据会不断更新，所以数据的角标也在变化，因此不能采用传统的分页模式。\n传统了分页在feed流是不适用的，因为我们的数据会随时发生变化\n假设在t1 时刻，我们去读取第一页，此时page &#x3D; 1 ，size &#x3D; 5 ，那么我们拿到的就是106 这几条记录，假设现在t2时候又发布了一条记录，此时t3 时刻，我们来读取第二页，读取第二页传入的参数是page&#x3D;2 ，size&#x3D;5 ，那么此时读取到的第二页实际上是从6 开始，然后是62 ，那么我们就读取到了重复的数据，所以feed流的分页，不能采用原始方案来做。\n\nFeed流的滚动分页\n我们需要记录每次操作的最后一条，然后从这个位置开始去读取数据 \n举个例子：我们从t1时刻开始，拿第一页数据，拿到了10~6，然后记录下当前最后一次拿取的记录，就是6，t2时刻发布了新的记录，此时这个11放到最顶上，但是不会影响我们之前记录的6，此时t3时刻来拿第二页，第二页这个时候拿数据，还是从6后一点的5去拿，就拿到了5-1的记录。我们这个地方可以采用sortedSet来做，可以进行范围查询，并且还可以记录当前获取数据时间戳最小值，就可以实现滚动分页了\n\n核心的意思：就是我们在保存完探店笔记后，获得到当前笔记的粉丝，然后把数据推送到粉丝的redis中去。\n@Override\npublic Result saveBlog(Blog blog) &#123;\n    // 1.获取登录用户\n    UserDTO user = UserHolder.getUser();\n    blog.setUserId(user.getId());\n    // 2.保存探店笔记\n    boolean isSuccess = save(blog);\n    if(!isSuccess)&#123;\n        return Result.fail(\"新增笔记失败!\");\n    &#125;\n    // 3.查询笔记作者的所有粉丝 select * from tb_follow where follow_user_id = ?\n    List&lt;Follow> follows = followService.query().eq(\"follow_user_id\", user.getId()).list();\n    // 4.推送笔记id给所有粉丝\n    for (Follow follow : follows) &#123;\n        // 4.1.获取粉丝id\n        Long userId = follow.getUserId();\n        // 4.2.推送\n        String key = FEED_KEY + userId;\n        stringRedisTemplate.opsForZSet().add(key, blog.getId().toString(), System.currentTimeMillis());\n    &#125;\n    // 5.返回id\n    return Result.ok(blog.getId());\n&#125;\n\n好友关注-实现分页查询收邮箱需求：在个人主页的“关注”卡片中，查询并展示推送的Blog信息：\n具体操作如下：\n1、每次查询完成后，我们要分析出查询出数据的最小时间戳，这个值会作为下一次查询的条件\n2、我们需要找到与上一次查询相同的查询个数作为偏移量，下次查询时，跳过这些查询过的数据，拿到我们需要的数据\n综上：我们的请求参数中就需要携带 lastId：上一次查询的最小时间戳 和偏移量这两个参数。\n这两个参数第一次会由前端来指定，以后的查询就根据后台结果作为条件，再次传递到后台。\n\n一、定义出来具体的返回值实体类\n@Data\npublic class ScrollResult &#123;\n    private List&lt;?> list;\n    private Long minTime;\n    private Integer offset;\n&#125;\n\nBlogController\n注意：RequestParam 表示接受url地址栏传参的注解，当方法上参数的名称和url地址栏不相同时，可以通过RequestParam 来进行指定\n@GetMapping(\"/of/follow\")\npublic Result queryBlogOfFollow(\n    @RequestParam(\"lastId\") Long max, @RequestParam(value = \"offset\", defaultValue = \"0\") Integer offset)&#123;\n    return blogService.queryBlogOfFollow(max, offset);\n&#125;\n\nBlogServiceImpl\n@Override\npublic Result queryBlogOfFollow(Long max, Integer offset) &#123;\n    // 1.获取当前用户\n    Long userId = UserHolder.getUser().getId();\n    // 2.查询收件箱 ZREVRANGEBYSCORE key Max Min LIMIT offset count\n    String key = FEED_KEY + userId;\n    Set&lt;ZSetOperations.TypedTuple&lt;String>> typedTuples = stringRedisTemplate.opsForZSet()\n        .reverseRangeByScoreWithScores(key, 0, max, offset, 2);\n    // 3.非空判断\n    if (typedTuples == null || typedTuples.isEmpty()) &#123;\n        return Result.ok();\n    &#125;\n    // 4.解析数据：blogId、minTime（时间戳）、offset\n    List&lt;Long> ids = new ArrayList&lt;>(typedTuples.size());\n    long minTime = 0; // 2\n    int os = 1; // 2\n    for (ZSetOperations.TypedTuple&lt;String> tuple : typedTuples) &#123; // 5 4 4 2 2\n        // 4.1.获取id\n        ids.add(Long.valueOf(tuple.getValue()));\n        // 4.2.获取分数(时间戳）\n        long time = tuple.getScore().longValue();\n        if(time == minTime)&#123;\n            os++;\n        &#125;else&#123;\n            minTime = time;\n            os = 1;\n        &#125;\n    &#125;\n\tos = minTime == max ? os : os + offset;\n    // 5.根据id查询blog\n    String idStr = StrUtil.join(\",\", ids);\n    List&lt;Blog> blogs = query().in(\"id\", ids).last(\"ORDER BY FIELD(id,\" + idStr + \")\").list();\n\n    for (Blog blog : blogs) &#123;\n        // 5.1.查询blog有关的用户\n        queryBlogUser(blog);\n        // 5.2.查询blog是否被点赞\n        isBlogLiked(blog);\n    &#125;\n\n    // 6.封装并返回\n    ScrollResult r = new ScrollResult();\n    r.setList(blogs);\n    r.setOffset(os);\n    r.setMinTime(minTime);\n\n    return Result.ok(r);\n&#125;\n","slug":"Redis实战-好友关注","date":"2023-06-06T01:49:53.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"6b1fcf8efc840ca23645e7afc15b96ff","title":"Redis实战-点赞排行榜","content":"点赞排行榜点赞功能初始代码\n@GetMapping(\"/likes/&#123;id&#125;\")\npublic Result queryBlogLikes(@PathVariable(\"id\") Long id) &#123;\n    //修改点赞数量\n    blogService.update().setSql(\"liked = liked +1 \").eq(\"id\",id).update();\n    return Result.ok();\n&#125;\n\n问题分析：这种方式会导致一个用户无限点赞，明显是不合理的\n造成这个问题的原因是，我们现在的逻辑，发起请求只是给数据库+1，所以才会出现这个问题\n\n完善点赞功能\n需求：\n\n同一个用户只能点赞一次，再次点击则取消点赞\n如果当前用户已经点赞，则点赞按钮高亮显示（前端已实现，判断字段Blog类的isLike属性）\n\n实现步骤：\n\n给Blog类中添加一个isLike字段，标示是否被当前用户点赞\n修改点赞功能，利用Redis的set集合判断是否点赞过，未点赞过则点赞数+1，已点赞过则点赞数-1\n修改根据id查询Blog的业务，判断当前登录用户是否点赞过，赋值给isLike字段\n修改分页查询Blog业务，判断当前登录用户是否点赞过，赋值给isLike字段\n\n为什么采用set集合：\n因为我们的数据是不能重复的，当用户操作过之后，无论他怎么操作，都是\n具体步骤：\n1、在Blog 添加一个字段\n@TableField(exist = false)\nprivate Boolean isLike;\n\n2、修改代码\n@Override\n   public Result likeBlog(Long id)&#123;\n       // 1.获取登录用户\n       Long userId = UserHolder.getUser().getId();\n       // 2.判断当前登录用户是否已经点赞\n       String key = BLOG_LIKED_KEY + id;\n       Boolean isMember = stringRedisTemplate.opsForSet().isMember(key, userId.toString());\n       if(BooleanUtil.isFalse(isMember))&#123;\n            //3.如果未点赞，可以点赞\n           //3.1 数据库点赞数+1\n           boolean isSuccess = update().setSql(\"liked = liked + 1\").eq(\"id\", id).update();\n           //3.2 保存用户到Redis的set集合\n           if(isSuccess)&#123;\n               stringRedisTemplate.opsForSet().add(key,userId.toString());\n           &#125;\n       &#125;else&#123;\n            //4.如果已点赞，取消点赞\n           //4.1 数据库点赞数-1\n           boolean isSuccess = update().setSql(\"liked = liked - 1\").eq(\"id\", id).update();\n           //4.2 把用户从Redis的set集合移除\n           if(isSuccess)&#123;\n               stringRedisTemplate.opsForSet().remove(key,userId.toString());\n           &#125;\n       &#125;\n\n点赞排行榜在探店笔记的详情页面，应该把给该笔记点赞的人显示出来，比如最早点赞的TOP5，形成点赞排行榜：\n之前的点赞是放到set集合，但是set集合是不能排序的，所以这个时候，咱们可以采用一个可以排序的set集合，就是咱们的sortedSet\n\n我们接下来来对比一下这些集合的区别是什么\n所有点赞的人，需要是唯一的，所以我们应当使用set或者是sortedSet\n其次我们需要排序，就可以直接锁定使用sortedSet啦\n\n修改代码\nBlogServiceImpl\n点赞逻辑代码\n@Override\n public Result likeBlog(Long id) &#123;\n     // 1.获取登录用户\n     Long userId = UserHolder.getUser().getId();\n     // 2.判断当前登录用户是否已经点赞\n     String key = BLOG_LIKED_KEY + id;\n     Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString());\n     if (score == null) &#123;\n         // 3.如果未点赞，可以点赞\n         // 3.1.数据库点赞数 + 1\n         boolean isSuccess = update().setSql(\"liked = liked + 1\").eq(\"id\", id).update();\n         // 3.2.保存用户到Redis的set集合  zadd key value score\n         if (isSuccess) &#123;\n             stringRedisTemplate.opsForZSet().add(key, userId.toString(), System.currentTimeMillis());\n         &#125;\n     &#125; else &#123;\n         // 4.如果已点赞，取消点赞\n         // 4.1.数据库点赞数 -1\n         boolean isSuccess = update().setSql(\"liked = liked - 1\").eq(\"id\", id).update();\n         // 4.2.把用户从Redis的set集合移除\n         if (isSuccess) &#123;\n             stringRedisTemplate.opsForZSet().remove(key, userId.toString());\n         &#125;\n     &#125;\n     return Result.ok();\n &#125;\n\n\n private void isBlogLiked(Blog blog) &#123;\n     // 1.获取登录用户\n     UserDTO user = UserHolder.getUser();\n     if (user == null) &#123;\n         // 用户未登录，无需查询是否点赞\n         return;\n     &#125;\n     Long userId = user.getId();\n     // 2.判断当前登录用户是否已经点赞\n     String key = \"blog:liked:\" + blog.getId();\n     Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString());\n     blog.setIsLike(score != null);\n &#125;\n\n点赞列表查询列表\nBlogController\n@GetMapping(\"/likes/&#123;id&#125;\")\npublic Result queryBlogLikes(@PathVariable(\"id\") Long id) &#123;\n\n    return blogService.queryBlogLikes(id);\n&#125;\n\nBlogService\n@Override\npublic Result queryBlogLikes(Long id) &#123;\n    String key = BLOG_LIKED_KEY + id;\n    // 1.查询top5的点赞用户 zrange key 0 4\n    Set&lt;String> top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4);\n    if (top5 == null || top5.isEmpty()) &#123;\n        return Result.ok(Collections.emptyList());\n    &#125;\n    // 2.解析出其中的用户id\n    List&lt;Long> ids = top5.stream().map(Long::valueOf).collect(Collectors.toList());\n    String idStr = StrUtil.join(\",\", ids);\n    // 3.根据用户id查询用户 WHERE id IN ( 5 , 1 ) ORDER BY FIELD(id, 5, 1)\n    List&lt;UserDTO> userDTOS = userService.query()\n            .in(\"id\", ids).last(\"ORDER BY FIELD(id,\" + idStr + \")\").list()\n            .stream()\n            .map(user -> BeanUtil.copyProperties(user, UserDTO.class))\n            .collect(Collectors.toList());\n    // 4.返回\n    return Result.ok(userDTOS);\n&#125;\n\n","slug":"Redis实战-点赞排行榜","date":"2023-06-06T01:30:08.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"4b50e524d6593265b35d48bd112d4791","title":"Redis实战-消息队列","content":"Redis消息队列认识消息队列什么是消息队列：字面意思就是存放消息的队列。最简单的消息队列模型包括3个角色：\n\n消息队列：存储和管理消息，也被称为消息代理（Message Broker）\n生产者：发送消息到消息队列\n消费者：从消息队列获取消息并处理消息\n\n\n使用队列的好处在于 解耦：所谓解耦，举一个生活中的例子就是：快递员(生产者)把快递放到快递柜里边(Message Queue)去，我们(消费者)从快递柜里边去拿东西，这就是一个异步，如果耦合，那么这个快递员相当于直接把快递交给你，万一你不在家，那么快递员就会一直等你，这就浪费了快递员的时间，所以这种思想在我们日常开发中，是非常有必要的。\n这种场景在我们秒杀中就变成了：我们下单之后，利用redis去进行校验下单条件，再通过队列把消息发送出去，然后再启动一个线程去消费这个消息，完成解耦，同时也加快我们的响应速度。\n这里我们可以使用一些现成的mq，比如kafka，rabbitmq等等，也可以直接使用redis提供的mq方案\n基于List实现消息队列基于List结构模拟消息队列\n消息队列（Message Queue），字面意思就是存放消息的队列。而Redis的list数据结构是一个双向链表，很容易模拟出队列效果。\n队列是入口和出口不在一边，因此我们可以利用：LPUSH 结合 RPOP、或者 RPUSH 结合 LPOP来实现。不过要注意的是，当队列中没有消息时RPOP或LPOP操作会返回null，并不像JVM的阻塞队列那样会阻塞并等待消息。因此这里应该使用BRPOP或者BLPOP来实现阻塞效果。\n\n基于List的消息队列有哪些优缺点？优点：\n\n利用Redis存储，不受限于JVM内存上限\n基于Redis的持久化机制，数据安全性有保证\n可以满足消息有序性\n\n缺点：\n\n无法避免消息丢失\n只支持单消费者\n\n基于PubSub的消息队列PubSub（发布订阅）是Redis2.0版本引入的消息传递模型。顾名思义，消费者可以订阅一个或多个channel，生产者向对应channel发送消息后，所有订阅者都能收到相关消息。\n SUBSCRIBE channel [channel] ：订阅一个或多个频道 PUBLISH channel msg ：向一个频道发送消息 PSUBSCRIBE pattern[pattern] ：订阅与pattern格式匹配的所有频道\n\n基于PubSub的消息队列有哪些优缺点？优点：\n\n采用发布订阅模型，支持多生产、多消费\n\n缺点：\n\n不支持数据持久化\n无法避免消息丢失\n消息堆积有上限，超出时数据丢失\n\n基于Stream的消息队列Stream 是 Redis 5.0 引入的一种新数据类型，可以实现一个功能非常完善的消息队列。\n发送消息的命令：\n\n例如：\n\n读取消息的方式之一：XREAD\n\n例如，使用XREAD读取第一个消息：\n\nXREAD阻塞方式，读取最新的消息：\n\n在业务开发中，可以循环的调用XREAD阻塞方式来查询最新消息，从而实现持续监听队列的效果，伪代码如下\n\n注意：当我们指定起始ID为$时，代表读取最新的消息，如果我们处理一条消息的过程中，又有超过1条以上的消息到达队列，则下次获取时也只能获取到最新的一条，会出现漏读消息的问题\nSTREAM类型消息队列的XREAD命令特点：\n\n消息可回溯\n一个消息可以被多个消费者读取\n可以阻塞读取\n有消息漏读的风险\n\n基于Stream的消息队列-消费者组消费者组（Consumer Group）：将多个消费者划分到一个组中，监听同一个队列。具备下列特点：\n\n创建消费者组：key：队列名称groupName：消费者组名称ID：起始ID标示，$代表队列中最后一个消息，0则代表队列中第一个消息MKSTREAM：队列不存在时自动创建队列其它常见命令：\n 删除指定的消费者组\nXGROUP DESTORY key groupName\n\n 给指定的消费者组添加消费者\nXGROUP CREATECONSUMER key groupname consumername\n\n 删除消费者组中的指定消费者\nXGROUP DELCONSUMER key groupname consumername\n\n从消费者组读取消息：\nXREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...]\n\n\ngroup：消费组名称\nconsumer：消费者名称，如果消费者不存在，会自动创建一个消费者\ncount：本次查询的最大数量\nBLOCK milliseconds：当没有消息时最长等待时间\nNOACK：无需手动ACK，获取到消息后自动确认\nSTREAMS key：指定队列名称\nID：获取消息的起始ID：\n\n“&gt;”：从下一个未消费的消息开始其它：根据指定id从pending-list中获取已消费但未确认的消息，例如0，是从pending-list中的第一个消息开始\n消费者监听消息的基本思路：\nSTREAM类型消息队列的XREADGROUP命令特点：\n\n消息可回溯\n可以多消费者争抢消息，加快消费速度\n可以阻塞读取\n没有消息漏读的风险\n有消息确认机制，保证消息至少被消费一次\n\n最后我们来个小对比\n\n基于Redis的Stream结构作为消息队列，实现异步秒杀下单需求：\n\n创建一个Stream类型的消息队列，名为stream.orders\n修改之前的秒杀下单Lua脚本，在认定有抢购资格后，直接向stream.orders中添加消息，内容包含voucherId、userId、orderId\n项目启动时，开启一个线程任务，尝试获取stream.orders中的消息，完成下单\n\n修改lua表达式,新增3.6 \n\nVoucherOrderServiceImpl\nprivate class VoucherOrderHandler implements Runnable &#123;\n\n    @Override\n    public void run() &#123;\n        while (true) &#123;\n            try &#123;\n // 1.获取消息队列中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 >\n                List&lt;MapRecord&lt;String, Object, Object>> list = stringRedisTemplate.opsForStream().read(\n                    Consumer.from(\"g1\", \"c1\"),\n                    StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)),\n                    StreamOffset.create(\"stream.orders\", ReadOffset.lastConsumed())\n                );\n                // 2.判断订单信息是否为空\n                if (list == null || list.isEmpty()) &#123;\n                    // 如果为null，说明没有消息，继续下一次循环\n                    continue;\n                &#125;\n                // 解析数据\n                MapRecord&lt;String, Object, Object> record = list.get(0);\n                Map&lt;Object, Object> value = record.getValue();\n                VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true);\n                // 3.创建订单\n                createVoucherOrder(voucherOrder);\n                // 4.确认消息 XACK\n                stringRedisTemplate.opsForStream().acknowledge(\"s1\", \"g1\", record.getId());\n            &#125; catch (Exception e) &#123;\n                log.error(\"处理订单异常\", e);\n                //处理异常消息\n                handlePendingList();\n            &#125;\n        &#125;\n    &#125;\n\n    private void handlePendingList() &#123;\n        while (true) &#123;\n            try &#123;\n                // 1.获取pending-list中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 0\n                List&lt;MapRecord&lt;String, Object, Object>> list = stringRedisTemplate.opsForStream().read(\n                    Consumer.from(\"g1\", \"c1\"),\n                    StreamReadOptions.empty().count(1),\n                    StreamOffset.create(\"stream.orders\", ReadOffset.from(\"0\"))\n                );\n                // 2.判断订单信息是否为空\n                if (list == null || list.isEmpty()) &#123;\n                    // 如果为null，说明没有异常消息，结束循环\n                    break;\n                &#125;\n                // 解析数据\n                MapRecord&lt;String, Object, Object> record = list.get(0);\n                Map&lt;Object, Object> value = record.getValue();\n                VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true);\n                // 3.创建订单\n                createVoucherOrder(voucherOrder);\n                // 4.确认消息 XACK\n                stringRedisTemplate.opsForStream().acknowledge(\"s1\", \"g1\", record.getId());\n            &#125; catch (Exception e) &#123;\n                log.error(\"处理pendding订单异常\", e);\n                try&#123;\n                    Thread.sleep(20);\n                &#125;catch(Exception e)&#123;\n                    e.printStackTrace();\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n\n","slug":"Redis实战-消息队列","date":"2023-06-05T14:00:00.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"b241352e87f784e0f5d32e5898f90895","title":"Redis实战-Redission分布式锁","content":"分布式锁-redission分布式锁-redission的功能基于setnx实现的分布式锁存在下面的问题：\n重入问题：重入问题是指 获得锁的线程可以再次进入到相同的锁的代码块中，可重入锁的意义在于防止死锁\n不可重试：是指目前的分布式只能尝试一次，合理的情况是：当线程在获得锁失败后，他应该能再次尝试获得锁。\n超时释放：我们在加锁时增加了过期时间，这样的我们可以防止死锁，但是如果卡顿的时间超长，虽然我们采用了lua表达式防止删锁的时候，误删别人的锁，但是毕竟存在没有锁住，有安全隐患\n主从一致性： 如果Redis提供了主从集群，当我们向集群写数据时，主机需要异步的将数据同步给从机，而万一在同步过去之前，主机宕机了，就会出现死锁问题。\n\nRedisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现。\nRedission提供了分布式锁的多种多样的功能\n\n分布式锁-redission可重入锁原理​\t\t参考jdk ReentrantLock原理（可重入锁） ：当判断这个锁已经有人的情况下，再判断获取锁的是不是自己，使用一个记录器count记录重入的次数，基于redis实现简单的可重入锁。\n\n获取锁的Lua脚本（保证操作的原子性）\n\n\nlocal key = KEYS[1];--锁的key\n\nlocal threadId = ARGV[1];--线程唯一标识\n\nlocal releaseTime = ARVG[2];--锁的自动释放时间\n\n--判断锁是否存在\n\nif(redis.call('exists',key) == 0) then\n\n--不存在，获取锁\n\nredis.call('hset', key, threadId,'1');\n\n--设置有效时间\n\nredis.call('expire', key, releaseTime);\n\nreturn 1;--放回结果\n\nend;\n\n锁已存在，判断threadId是否是自己\n\nif(redis.call('hexists', key, threadId) == 1) then\n\n--是自己，获取锁，重入次数+1\n\nredis.call('hincrby', key, threadId, '1');\n\n--设置有效期\n\nredis.call('expire', key, releaseTime);\n\nreturn 1;--返回结果\n\nend;\n\nreturn 0;--代码走到这说明获取的锁不是自己的，获取锁失败\n\n释放锁的Lua脚本\nlocal key = KEYS[1];\n\nlocal threadId = ARGV[1];\n\nlocal releaseTime = ARGV[2];\n\n--判断是否是自己的锁\n\nif(redis.call('hexists', key, threadId) == 0) then\n\n--\n\n不是自己的锁，直接返回\n\nreturn nil;\n\nend;\n\n--是自己的锁，则锁重入次数 -1；\n\nlocal count = redis.call('hincrby', key, threadId, -1);\n\n--判断锁的重入次数是否为0\n\nif(count > 0)  then\n\n--不为0，重置过期时间\n\nredis.call('expire', key, releaseTime);\n\nreturn nil;\n\nelse\n\n--为0则删除锁\n\nredis.call('del', key)\n\nreturn nil;\n\nend;\n\n在Lock锁中，他是借助于底层的一个voaltile的一个state变量来记录重入的状态的，比如当前没有人持有这把锁，那么state&#x3D;0，假如有人持有这把锁，那么state&#x3D;1，如果持有这把锁的人再次持有这把锁，那么state就会+1 ，如果是对于synchronized而言，他在c语言代码中会有一个count，原理和state类似，也是重入一次就加一，释放一次就-1 ，直到减少成0 时，表示当前这把锁没有被人持有。  \n在分布式锁中，采用hash结构用来存储锁，其中大key表示这把锁是否存在，用小key表示当前这把锁被哪个线程持有，所以接下来我们一起分析一下当前的这个lua表达式\n这个地方一共有3个参数\nKEYS[1] ： 锁名称\nARGV[1]：  锁失效时间\nARGV[2]：  id + “:” + threadId; \t锁的小key\nexists: 判断数据是否存在  name是lock,如果&#x3D;&#x3D;0，就表示当前这把锁不存在\nredis.call(‘hset’, KEYS[1], ARGV[2], 1);此时他就开始往redis里边去写数据 ，写成一个hash结构\nLock{\n​    id + “:” + threadId :  1\n}\n如果当前这把锁存在，则第一个条件不满足，再判断\nredis.call(‘hexists’, KEYS[1], ARGV[2]) &#x3D;&#x3D; 1\n此时需要通过大key+小key判断当前这把锁是否是属于自己的，如果是自己的，则进行\nredis.call(‘hincrby’, KEYS[1], ARGV[2], 1)\n将当前这个锁的value进行+1 ，redis.call(‘pexpire’, KEYS[1], ARGV[1]); 然后再对其设置过期时间，如果以上两个条件都不满足，则表示当前这把锁抢锁失败，最后返回pttl，即为当前这把锁的失效时间\nredission实现可重入锁：\ntrylock()\n\n\n\ntryAcquire()\n\n\n获取锁\n\n释放锁\n\n获取锁成功返回nil，获取锁失败返回锁的pttl(毫秒)剩余有效期\nRFuture接收：因为函数tryLockInnerAsync是一个异步函数：函数执行完，只代表命令发出去了。\ntryAcquire阻塞并等待tryAcquireAsync返回的剩余有效期\nredisson解决锁重试问题：获取尝试最大等待时间；获取当前时间；获取线程id；尝试获取（返回null代表获取锁成功，返回有效期代表获取锁失败）判断是否是null,是null返回true，表示获取锁成功。否则就用最大尝试等待时间-（当前时间-获取锁操作的时间） 然后判断如果time是小于等于0，表示已经超过尝试最大等待时间，返回false表示获取所失败。如果大于0，再次获取当前时间，准备进行下一次尝试（！不是马上尝试，因为其他线程获取锁，要执行操作，立马尝试失败的概率很大）：先订阅别人释放锁的信号（unLockInnerAsync释放锁的时候，“redis.call(‘publish’, KEYS[2], ARGV[1]);”会发布通知。\n下一次尝试的结果也是不确定的,所以用RFutre接收，具体等待时间为剩余的尝试最大等待时间，如果超时则取消订阅，返回false,获取锁失败；如果在未超过尝试最大等待时间，获取了别人发布的释放锁的信号，又一次计算剩余的尝试最大等待时间，依然有剩余的话，才是真正的尝试获取锁。如果再次获取锁失败并且剩余最大尝试等待时间还大于0，会再次准备进行下一轮尝试（两次尝试的不同：第二次尝试会使用信号量的方式getLatch()获取，第二轮尝试会在while(true)内进行。\nReddisson解决锁超时：tryAcquireAsync传入的参数中，未设置过期时间时，会开启一个固定的时间（看门狗时间：30秒）不停的更新有效期\n优点：精确的时间计算，不是盲目的等待，不会无故的占用cpu\n分布式锁-redission锁重试和WatchDog机制抢锁过程中，获得当前线程，通过tryAcquire进行抢锁，该抢锁逻辑和之前逻辑相同\n1、先判断当前这把锁是否存在，如果不存在，插入一把锁，返回null\n2、判断当前这把锁是否是属于当前线程，如果是，则返回null\n所以如果返回是null，则代表着当前已经抢锁完毕，或者可重入完毕，但是如果以上两个条件都不满足，则进入到第三个条件，返回的是锁的失效时间，你能发现有个while( true) 再次进行tryAcquire进行抢锁\nlong threadId = Thread.currentThread().getId();\nLong ttl = tryAcquire(-1, leaseTime, unit, threadId);\n// lock acquired\nif (ttl == null) &#123;\n    return;\n&#125;\n\n接下来会有一个条件分支，因为lock方法有重载方法，一个是带参数，一个是不带参数，如果带参数传入的值是-1，则leaseTime是他本身，所以如果传入了其他参数，此时leaseTime !&#x3D; -1 则会进去抢锁，抢锁的逻辑就是之前说的那三个逻辑\nif (leaseTime != -1) &#123;\n    return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG);\n&#125;\n\n如果是没有传入时间，则此时也会进行抢锁， 而且抢锁时间是默认看门狗时间 commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout()\nttlRemainingFuture.onComplete((ttlRemaining, e) 这句话相当于对以上抢锁进行了监听，也就是说当上边抢锁完毕后，此方法会被调用，具体调用的逻辑就是去后台开启一个线程，进行续约逻辑，也就是看门狗线程\nRFuture&lt;Long> ttlRemainingFuture = tryLockInnerAsync(waitTime,\n                                        commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(),\n                                        TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG);\nttlRemainingFuture.onComplete((ttlRemaining, e) -> &#123;\n    if (e != null) &#123;\n        return;\n    &#125;\n\n    // lock acquired\n    if (ttlRemaining == null) &#123;\n        scheduleExpirationRenewal(threadId);\n    &#125;\n&#125;);\nreturn ttlRemainingFuture;\n\n此逻辑就是续约逻辑，注意看commandExecutor.getConnectionManager().newTimeout（） 此方法\nMethod(  new TimerTask() {},参数2 ，参数3  )\n指的是：通过参数2，参数3 去描述什么时候去做参数1的事情，现在的情况是：10s之后去做参数一的事情\n因为锁的失效时间是30s，当10s之后，此时这个timeTask 就触发了，他就去进行续约，把当前这把锁续约成30s，如果操作成功，那么此时就会递归调用自己，再重新设置一个timeTask()，于是再过10s后又再设置一个timerTask，完成不停的续约\n那么大家可以想一想，假设我们的线程出现了宕机他还会续约吗？当然不会，因为没有人再去调用renewExpiration这个方法，所以等到时间之后自然就释放了。\nprivate void renewExpiration() &#123;\n    ExpirationEntry ee = EXPIRATION_RENEWAL_MAP.get(getEntryName());\n    if (ee == null) &#123;\n        return;\n    &#125;\n    \n    Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() &#123;\n        @Override\n        public void run(Timeout timeout) throws Exception &#123;\n            ExpirationEntry ent = EXPIRATION_RENEWAL_MAP.get(getEntryName());\n            if (ent == null) &#123;\n                return;\n            &#125;\n            Long threadId = ent.getFirstThreadId();\n            if (threadId == null) &#123;\n                return;\n            &#125;\n            \n            RFuture&lt;Boolean> future = renewExpirationAsync(threadId);\n            future.onComplete((res, e) -> &#123;\n                if (e != null) &#123;\n                    log.error(\"Can't update lock \" + getName() + \" expiration\", e);\n                    return;\n                &#125;\n                \n                if (res) &#123;\n                    // reschedule itself\n                    renewExpiration();\n                &#125;\n            &#125;);\n        &#125;\n    &#125;, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS);\n    \n    ee.setTimeout(task);\n&#125;\n\nredission锁的MutiLock(联锁)原理为了提高redis的可用性，我们会搭建集群或者主从，现在以主从为例\n此时我们去写命令，写在主机上， 主机会将数据同步给从机，但是假设在主机还没有来得及把数据写入到从机去的时候，此时主机宕机，哨兵会发现主机宕机，并且选举一个slave变成master，而此时新的master中实际上并没有锁信息，此时锁信息就已经丢掉了。\n\n为了解决这个问题，redission提出来了MutiLock锁，使用这把锁咱们就不使用主从了，每个节点的地位都是一样的， 这把锁加锁的逻辑需要写入到每一个主丛节点上，只有所有的服务器都写入成功，此时才是加锁成功，假设现在某个节点挂了，那么他去获得锁的时候，只要有一个节点拿不到，都不能算是加锁成功，就保证了加锁的可靠性。\n\nMutiLock 加锁原理\n当我们去设置了多个锁时，redission会将多个锁添加到一个集合中，然后用while循环去不停去尝试拿锁，但是会有一个总共的加锁时间，这个时间是用需要加锁的个数 * 1500ms ，假设有3个锁，那么时间就是4500ms，假设在这4500ms内，所有的锁都加锁成功， 那么此时才算是加锁成功，如果在4500ms有线程加锁失败，则会再次去进行重试。\n\n秒杀优化秒杀优化-异步秒杀思路当用户发起请求，此时会请求nginx，nginx会访问到tomcat，而tomcat中的程序，会进行串行操作，分成如下几个步骤\n1、查询优惠卷\n2、判断秒杀库存是否足够\n3、查询订单\n4、校验是否是一人一单\n5、扣减库存\n6、创建订单\n在这六步操作中，又有很多操作是要去操作数据库的，而且还是一个线程串行执行， 这样就会导致我们的程序执行的很慢，所以需要异步程序执行。\n可以使用异步吗？比如，我们可不可以使用异步编排来做，或者说我开启N多线程，N多个线程，一个线程执行查询优惠卷，一个执行判断扣减库存，一个去创建订单等等，然后再统一做返回。\n如果你采用异步的方式，如果访问的人很多，那么线程池中的线程可能一下子就被消耗完了，而且你使用异步方案，最大的特点在于，你觉得时效性会非常重要，但是你想想是吗？并不是，比如我只要确定他能做这件事，然后我后边慢慢做就可以了，我并不需要他一口气做完这件事，所以我们不应当采用异步。使用类似消息队列的方式来完成我们的需求，而不是使用线程池或者是异步编排的方式来完成这个需求\n\n优化方案：我们将耗时比较短的逻辑判断放入到redis中，比如是否库存足够，比如是否一人一单，这样的操作，只要这种逻辑可以完成，就意味着我们是一定可以下单完成的，我们只需要进行快速的逻辑判断，根本就不用等下单逻辑走完，我们直接给用户返回成功， 再在后台开一个线程，后台线程慢慢的去执行queue里边的消息，这样程序不就超级快了吗？而且也不用担心线程池消耗殆尽的问题，因为这里我们的程序中并没有手动使用任何线程池，当然这里边有两个难点\n第一个难点是我们怎么在redis中去快速校验一人一单，还有库存判断\n第二个难点是由于我们校验和tomcat下单是两个线程，那么我们如何知道到底哪个单他最后是否成功，或者是下单完成，为了完成这件事我们在redis操作完之后，我们会将一些信息返回给前端，同时也会把这些信息丢到异步queue中去，后续操作中，可以通过这个id来查询我们tomcat中的下单逻辑是否完成了。\n\n我们现在来看看整体思路：当用户下单之后，判断库存是否充足只需要导redis中去根据key找对应的value是否大于0即可，如果不充足，则直接结束，如果充足，继续在redis中判断用户是否可以下单，如果set集合中没有这条数据，说明他可以下单，如果set集合中没有这条记录，则将userId和优惠卷存入到redis中，并且返回0，整个过程需要保证是原子性的，我们可以使用lua来操作\n当以上判断逻辑走完之后，我们可以判断当前redis中返回的结果是否是0 ，如果是0，则表示可以下单，则将之前说的信息存入到到queue中去，然后返回，然后再来个线程异步的下单，前端可以通过返回的订单id来判断是否下单成功。\n\n秒杀优化-Redis完成秒杀资格判断需求：\n\n新增秒杀优惠券的同时，将优惠券信息保存到Redis中\n\n基于Lua脚本，判断秒杀库存、一人一单，决定用户是否抢购成功\n\n如果抢购成功，将优惠券id和用户id封装后存入阻塞队列\n\n开启线程任务，不断从阻塞队列中获取信息，实现异步下单功能\n\n\n\nVoucherServiceImpl\n@Override\n@Transactional\npublic void addSeckillVoucher(Voucher voucher) &#123;\n    // 保存优惠券\n    save(voucher);\n    // 保存秒杀信息\n    SeckillVoucher seckillVoucher = new SeckillVoucher();\n    seckillVoucher.setVoucherId(voucher.getId());\n    seckillVoucher.setStock(voucher.getStock());\n    seckillVoucher.setBeginTime(voucher.getBeginTime());\n    seckillVoucher.setEndTime(voucher.getEndTime());\n    seckillVoucherService.save(seckillVoucher);\n    // 保存秒杀库存到Redis中\n    //SECKILL_STOCK_KEY 这个变量定义在RedisConstans中\n    //private static final String SECKILL_STOCK_KEY =\"seckill:stock:\"\n    stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString());\n&#125;\n\n完整lua表达式\n-- 1.参数列表\n-- 1.1.优惠券id\nlocal voucherId = ARGV[1]\n-- 1.2.用户id\nlocal userId = ARGV[2]\n-- 1.3.订单id\nlocal orderId = ARGV[3]\n\n-- 2.数据key\n-- 2.1.库存key\nlocal stockKey = 'seckill:stock:' .. voucherId\n-- 2.2.订单key\nlocal orderKey = 'seckill:order:' .. voucherId\n\n-- 3.脚本业务\n-- 3.1.判断库存是否充足 get stockKey\nif(tonumber(redis.call('get', stockKey)) &lt;= 0) then\n    -- 3.2.库存不足，返回1\n    return 1\nend\n-- 3.2.判断用户是否下单 SISMEMBER orderKey userId\nif(redis.call('sismember', orderKey, userId) == 1) then\n    -- 3.3.存在，说明是重复下单，返回2\n    return 2\nend\n-- 3.4.扣库存 incrby stockKey -1\nredis.call('incrby', stockKey, -1)\n-- 3.5.下单（保存用户）sadd orderKey userId\nredis.call('sadd', orderKey, userId)\n-- 3.6.发送消息到队列中， XADD stream.orders * k1 v1 k2 v2 ...\nredis.call('xadd', 'stream.orders', '*', 'userId', userId, 'voucherId', voucherId, 'id', orderId)\nreturn 0\n\n当以上lua表达式执行完毕后，剩下的就是根据步骤3,4来执行我们接下来的任务了\nVoucherOrderServiceImpl\n@Override\npublic Result seckillVoucher(Long voucherId) &#123;\n    //获取用户\n    Long userId = UserHolder.getUser().getId();\n    long orderId = redisIdWorker.nextId(\"order\");\n    // 1.执行lua脚本\n    Long result = stringRedisTemplate.execute(\n            SECKILL_SCRIPT,\n            Collections.emptyList(),\n            voucherId.toString(), userId.toString(), String.valueOf(orderId)\n    );\n    int r = result.intValue();\n    // 2.判断结果是否为0\n    if (r != 0) &#123;\n        // 2.1.不为0 ，代表没有购买资格\n        return Result.fail(r == 1 ? \"库存不足\" : \"不能重复下单\");\n    &#125;\n    //TODO 保存阻塞队列\n    // 3.返回订单id\n    return Result.ok(orderId);\n&#125;\n\n秒杀优化-基于阻塞队列实现秒杀优化VoucherOrderServiceImpl\n修改下单动作，现在我们去下单时，是通过lua表达式去原子执行判断逻辑，如果判断我出来不为0 ，则要么是库存不足，要么是重复下单，返回错误信息，如果是0，则把下单的逻辑保存到队列中去，然后异步执行\n//异步处理线程池\nprivate static final ExecutorService SECKILL_ORDER_EXECUTOR = Executors.newSingleThreadExecutor();\n\n//在类初始化之后执行，因为当这个类初始化好了之后，随时都是有可能要执行的\n@PostConstruct\nprivate void init() &#123;\n   SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler());\n&#125;\n// 用于线程池处理的任务\n// 当初始化完毕后，就会去从对列中去拿信息\n private class VoucherOrderHandler implements Runnable&#123;\n\n        @Override\n        public void run() &#123;\n            while (true)&#123;\n                try &#123;\n                    // 1.获取队列中的订单信息\n                    VoucherOrder voucherOrder = orderTasks.take();\n                    // 2.创建订单\n                    handleVoucherOrder(voucherOrder);\n                &#125; catch (Exception e) &#123;\n                    log.error(\"处理订单异常\", e);\n                &#125;\n          \t &#125;\n        &#125;\n     \n       private void handleVoucherOrder(VoucherOrder voucherOrder) &#123;\n            //1.获取用户\n            Long userId = voucherOrder.getUserId();\n            // 2.创建锁对象\n            RLock redisLock = redissonClient.getLock(\"lock:order:\" + userId);\n            // 3.尝试获取锁\n            boolean isLock = redisLock.lock();\n            // 4.判断是否获得锁成功\n            if (!isLock) &#123;\n                // 获取锁失败，直接返回失败或者重试\n                log.error(\"不允许重复下单！\");\n                return;\n            &#125;\n            try &#123;\n\t\t\t\t//注意：由于是spring的事务是放在threadLocal中，此时的是多线程，事务会失效\n                proxy.createVoucherOrder(voucherOrder);\n            &#125; finally &#123;\n                // 释放锁\n                redisLock.unlock();\n            &#125;\n    &#125;\n     //a\n\tprivate BlockingQueue&lt;VoucherOrder> orderTasks =new  ArrayBlockingQueue&lt;>(1024 * 1024);\n\n    @Override\n    public Result seckillVoucher(Long voucherId) &#123;\n        Long userId = UserHolder.getUser().getId();\n        long orderId = redisIdWorker.nextId(\"order\");\n        // 1.执行lua脚本\n        Long result = stringRedisTemplate.execute(\n                SECKILL_SCRIPT,\n                Collections.emptyList(),\n                voucherId.toString(), userId.toString(), String.valueOf(orderId)\n        );\n        int r = result.intValue();\n        // 2.判断结果是否为0\n        if (r != 0) &#123;\n            // 2.1.不为0 ，代表没有购买资格\n            return Result.fail(r == 1 ? \"库存不足\" : \"不能重复下单\");\n        &#125;\n        VoucherOrder voucherOrder = new VoucherOrder();\n        // 2.3.订单id\n        long orderId = redisIdWorker.nextId(\"order\");\n        voucherOrder.setId(orderId);\n        // 2.4.用户id\n        voucherOrder.setUserId(userId);\n        // 2.5.代金券id\n        voucherOrder.setVoucherId(voucherId);\n        // 2.6.放入阻塞队列\n        orderTasks.add(voucherOrder);\n        //3.获取代理对象\n         proxy = (IVoucherOrderService)AopContext.currentProxy();\n        //4.返回订单id\n        return Result.ok(orderId);\n    &#125;\n     \n      @Transactional\n    public  void createVoucherOrder(VoucherOrder voucherOrder) &#123;\n        Long userId = voucherOrder.getUserId();\n        // 5.1.查询订单\n        int count = query().eq(\"user_id\", userId).eq(\"voucher_id\", voucherOrder.getVoucherId()).count();\n        // 5.2.判断是否存在\n        if (count > 0) &#123;\n            // 用户已经购买过了\n           log.error(\"用户已经购买过了\");\n           return ;\n        &#125;\n\n        // 6.扣减库存\n        boolean success = seckillVoucherService.update()\n                .setSql(\"stock = stock - 1\") // set stock = stock - 1\n                .eq(\"voucher_id\", voucherOrder.getVoucherId()).gt(\"stock\", 0) // where id = ? and stock > 0\n                .update();\n        if (!success) &#123;\n            // 扣减失败\n            log.error(\"库存不足\");\n            return ;\n        &#125;\n        save(voucherOrder);\n \n    &#125;\n\n\n小总结：秒杀业务的优化思路是什么？\n\n先利用Redis完成库存余量、一人一单判断，完成抢单业务\n再将下单业务放入阻塞队列，利用独立线程异步下单\n基于阻塞队列的异步秒杀存在哪些问题？\n内存限制问题\n数据安全问题\n\n\n\n","slug":"Redis实战-Redission分布式锁","date":"2023-06-04T14:29:14.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"ccca64486565a23a9fbb5012df3ad7c1","title":"Redis实战-分布式锁","content":"4、分布式锁基本原理和实现方式对比分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。\n分布式锁的核心思想就是让大家都使用同一把锁，只要大家使用的是同一把锁，那么我们就能锁住线程，不让线程进行，让程序串行执行，这就是分布式锁的核心思路\n\n分布式锁应该满足以下条件：\n可见性：多个线程都能看到相同的结果，注意：这个地方说的可见性并不是并发编程中指的内存可见性，只是说多个进程之间都能感知到变化的意思\n互斥：互斥是分布式锁的最基本的条件，使得程序串行执行\n高可用：程序不易崩溃，时时刻刻都保证较高的可用性\n高性能：由于加锁本身就让性能降低，所有对于分布式锁本身需要他就较高的加锁性能和释放锁性能\n安全性：安全也是程序中必不可少的一环\n\n常见的分布式锁有三种\nMysql：mysql本身就带有锁机制，但是由于mysql性能本身一般，所以采用分布式锁的情况下，其实使用mysql作为分布式锁比较少见\nRedis：redis作为分布式锁是非常常见的一种使用方式，现在企业级开发中基本都使用redis或者zookeeper作为分布式锁，利用setnx这个方法，如果插入key成功，则表示获得到了锁，如果有人插入成功，其他人插入失败则表示无法获得到锁，利用这套逻辑来实现分布式锁\nZookeeper：zookeeper也是企业级开发中较好的一个实现分布式锁的方案\nRedis分布式锁的实现核心思路实现分布式锁时需要实现的两个基本方法：\n\n获取锁：\n\n互斥：确保只能有一个线程获取锁\n非阻塞：尝试一次，成功返回true，失败返回false\n\n\n释放锁：\n\n手动释放\n超时释放：获取锁时添加一个超时时间\n\n\n\n\n核心思路：\n我们利用redis 的setnx 方法，当有多个线程进入时，我们就利用该方法，第一个线程进入时，redis 中就有这个key 了，返回了1，如果结果是1，则表示他抢到了锁，那么他去执行业务，然后再删除锁，退出锁逻辑，没有抢到锁的哥们，等待一定时间后重试即可\n \nStringRedisTemplate实现分布式锁\n加锁逻辑\n\n锁的基本接口\n\nSimpleRedisLock\n利用setnx方法进行加锁，同时增加过期时间，防止死锁，此方法可以保证加锁和增加过期时间具有原子性\nprivate static final String KEY_PREFIX=\"lock:\"\n@Override\npublic boolean tryLock(long timeoutSec) &#123;\n    // 获取线程标示\n    String threadId = Thread.currentThread().getId()\n    // 获取锁\n    Boolean success = stringRedisTemplate.opsForValue()\n            .setIfAbsent(KEY_PREFIX + name, threadId + \"\", timeoutSec, TimeUnit.SECONDS);\n    return Boolean.TRUE.equals(success);\n&#125;\n\n\n释放锁逻辑\n\nSimpleRedisLock\n释放锁，防止删除别人的锁\npublic void unlock() &#123;\n    //通过del删除锁\n    stringRedisTemplate.delete(KEY_PREFIX + name);\n&#125;\n\n\n修改业务代码\n\n@Override\n  public Result seckillVoucher(Long voucherId) &#123;\n      // 1.查询优惠券\n      SeckillVoucher voucher = seckillVoucherService.getById(voucherId);\n      // 2.判断秒杀是否开始\n      if (voucher.getBeginTime().isAfter(LocalDateTime.now())) &#123;\n          // 尚未开始\n          return Result.fail(\"秒杀尚未开始！\");\n      &#125;\n      // 3.判断秒杀是否已经结束\n      if (voucher.getEndTime().isBefore(LocalDateTime.now())) &#123;\n          // 已经结束\n          return Result.fail(\"秒杀已经结束！\");\n      &#125;\n      // 4.判断库存是否充足\n      if (voucher.getStock() &lt; 1) &#123;\n          // 库存不足\n          return Result.fail(\"库存不足！\");\n      &#125;\n      Long userId = UserHolder.getUser().getId();\n      //创建锁对象(新增代码)\n      SimpleRedisLock lock = new SimpleRedisLock(\"order:\" + userId, stringRedisTemplate);\n      //获取锁对象\n      boolean isLock = lock.tryLock(1200);\n//加锁失败\n      if (!isLock) &#123;\n          return Result.fail(\"不允许重复下单\");\n      &#125;\n      try &#123;\n          //获取代理对象(事务)\n          IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy();\n          return proxy.createVoucherOrder(voucherId);\n      &#125; finally &#123;\n          //释放锁\n          lock.unlock();\n      &#125;\n  &#125;\n\nRedis分布式锁误删情况说明逻辑说明：\n持有锁的线程在锁的内部出现了阻塞，导致他的锁自动释放，这时其他线程，线程2来尝试获得锁，就拿到了这把锁，然后线程2在持有锁执行过程中，线程1反应过来，继续执行，而线程1执行过程中，走到了删除锁逻辑，此时就会把本应该属于线程2的锁进行删除，这就是误删别人锁的情况说明\n解决方案：解决方案就是在每个线程释放锁的时候，去判断一下当前这把锁是否属于自己，如果属于自己，则不进行锁的删除，假设还是上边的情况，线程1卡顿，锁自动释放，线程2进入到锁的内部执行逻辑，此时线程1反应过来，然后删除锁，但是线程1，一看当前这把锁不是属于自己，于是不进行删除锁逻辑，当线程2走到删除锁逻辑时，如果没有卡过自动释放锁的时间点，则判断当前这把锁是属于自己的，于是删除这把锁。\n\n解决Redis分布式锁误删问题需求：修改之前的分布式锁实现，满足：在获取锁时存入线程标示（可以用UUID表示）在释放锁时先获取锁中的线程标示，判断是否与当前线程标示一致\n\n如果一致则释放锁\n如果不一致则不释放锁\n\n核心逻辑：在存入锁时，放入自己线程的标识，在删除锁时，判断当前这把锁的标识是不是自己存入的，如果是，则进行删除，如果不是，则不进行删除。\n\n具体代码如下：加锁\nprivate static final String ID_PREFIX = UUID.randomUUID().toString(true) + \"-\";\n@Override\npublic boolean tryLock(long timeoutSec) &#123;\n   // 获取线程标示\n   String threadId = ID_PREFIX + Thread.currentThread().getId();\n   // 获取锁\n   Boolean success = stringRedisTemplate.opsForValue()\n                .setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS);\n   return Boolean.TRUE.equals(success);\n&#125;\n\n释放锁\npublic void unlock() &#123;\n    // 获取线程标示\n    String threadId = ID_PREFIX + Thread.currentThread().getId();\n    // 获取锁中的标示\n    String id = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name);\n    // 判断标示是否一致\n    if(threadId.equals(id)) &#123;\n        // 释放锁\n        stringRedisTemplate.delete(KEY_PREFIX + name);\n    &#125;\n&#125;\n\n分布式锁的原子性问题更为极端的误删逻辑说明：\n线程1现在持有锁之后，在执行业务逻辑过程中，他正准备删除锁，而且已经走到了条件判断的过程中，比如他已经拿到了当前这把锁确实是属于他自己的，正准备删除锁，但是此时他的锁到期了，那么此时线程2进来，但是线程1他会接着往后执行，当他卡顿结束后，他直接就会执行删除锁那行代码，相当于条件判断并没有起到作用，这就是删锁时的原子性问题，之所以有这个问题，是因为线程1的拿锁，比较是不是自己的锁，删锁，实际上并不是原子性的，我们要防止刚才的情况发生，\n\nLua脚本解决多条命令原子性问题Redis提供了Lua脚本功能，在一个脚本中编写多条Redis命令，确保多条命令执行时的原子性。Lua是一种编程语言，它的基本语法大家可以参考网站：https://www.runoob.com/lua/lua-tutorial.html，这里重点介绍Redis提供的调用函数，我们可以使用lua去操作redis，又能保证他的原子性，这样就可以实现拿锁，比锁，删锁是一个原子性动作了。\nRedis提供的调用函数，语法如下：\nredis.call('命令名称', 'key', '其它参数', ...)\n\n例如要执行set name jack，则脚本是这样：\n# 执行 set name jack\nredis.call('set', 'name', 'jack')\n\n例如要先执行set name Rose，再执行get name，则脚本如下：\n# 先执行 set name jack\nredis.call('set', 'name', 'Rose')\n# 再执行 get name\nlocal name = redis.call('get', 'name')\n# 返回\nreturn name\n\n写好脚本以后，需要用Redis命令来调用脚本，调用脚本的常见命令如下：\n\n例如，我们要执行 redis.call(‘set’, ‘name’, ‘jack’) 这个脚本，语法如下：\n\n如果脚本中的key、value不想写死，可以作为参数传递。key类型参数会放入KEYS数组，其它参数会放入ARGV数组，在脚本中可以从KEYS和ARGV数组获取这些参数：\n\n释放锁的业务流程是这样的\n​\t1、获取锁中的线程标示\n​\t2、判断是否与指定的标示（当前线程标示）一致\n​\t3、如果一致则释放锁（删除）\n​\t4、如果不一致则什么都不做\n最终我们操作redis的拿锁比锁删锁的lua脚本就会变成这样\n-- 这里的 KEYS[1] 就是锁的key，这里的ARGV[1] 就是当前线程标示\n-- 获取锁中的标示，判断是否与当前线程标示一致\nif (redis.call('GET', KEYS[1]) == ARGV[1]) then\n  -- 一致，则删除锁\n  return redis.call('DEL', KEYS[1])\nend\n-- 不一致，则直接返回\nreturn 0\n\n利用Java代码调用Lua脚本改造分布式锁lua脚本本身并不需要大家花费太多时间去研究，只需要知道如何调用，大致是什么意思即可，所以在笔记中并不会详细的去解释这些lua表达式的含义。\n我们的RedisTemplate中，可以利用execute方法去执行lua脚本，参数对应关系就如下图\n\nJava代码\nprivate static final DefaultRedisScript&lt;Long> UNLOCK_SCRIPT;\n    static &#123;\n        UNLOCK_SCRIPT = new DefaultRedisScript&lt;>();\n        UNLOCK_SCRIPT.setLocation(new ClassPathResource(\"unlock.lua\"));\n        UNLOCK_SCRIPT.setResultType(Long.class);\n    &#125;\n\npublic void unlock() &#123;\n    // 调用lua脚本\n    stringRedisTemplate.execute(\n            UNLOCK_SCRIPT,\n            Collections.singletonList(KEY_PREFIX + name),\n            ID_PREFIX + Thread.currentThread().getId());\n&#125;\n经过以上代码改造后，我们就能够实现 拿锁比锁删锁的原子性动作了~\n\n总结：\n基于Redis的分布式锁实现思路：\n\n利用set nx ex获取锁，并设置过期时间，保存线程标示\n释放锁时先判断线程标示是否与自己一致，一致则删除锁\n特性：\n利用set nx满足互斥性\n利用set ex保证故障时锁依然能释放，避免死锁，提高安全性\n利用Redis集群保证高可用和高并发特性\n\n\n\n\n\n总结：利用添加过期时间，防止死锁问题的发生，但是有了过期时间之后，可能出现误删别人锁的问题，这个问题我们开始是利用删之前 通过拿锁，比锁，删锁这个逻辑来解决的，也就是删之前判断一下当前这把锁是否是属于自己的，但是现在还有原子性问题，也就是我们没法保证拿锁比锁删锁是一个原子性的动作，最后通过lua表达式来解决这个问题\n但是目前还剩下一个问题锁不住，什么是锁不住呢，如果当过期时间到了之后，如果可以续期，是不是后边的问题都不会发生了，那么续期问题怎么解决呢，可以依赖于我们接下来要学习redission啦\n","slug":"Redis实战-分布式锁","date":"2023-06-04T10:10:53.000Z","categories_index":"","tags_index":"Redis实战","author_index":"大宝贝的程序员"},{"id":"a08c023de37b091e96339da5fe4c6318","title":"Redis实战-秒杀","content":"秒杀全局唯一ID当用户抢购时，就会生成订单并保存到订单表中，而订单表如果使用数据库自增ID就存在一些问题：\n\nid的规律性太明显\n受单表数据量的限制\n\n场景分析：如果我们的id具有太明显的规则，用户或者说商业对手很容易猜测出来我们的一些敏感信息，比如商城在一天时间内，卖出了多少单，这明显不合适。\n场景分析：随着我们商城规模越来越大，mysql的单表的容量不宜超过500W，数据量过大之后，我们要进行拆库拆表，但拆分表了之后，他们从逻辑上讲他们是同一张表，所以他们的id是不能一样的， 于是乎我们需要保证id的唯一性。\n全局ID生成器，是一种在分布式系统下用来生成全局唯一ID的工具，一般要满足下列特性：\n\n为了增加ID的安全性，我们可以不直接使用Redis自增的数值，而是拼接一些其它信息：\nID的组成部分：符号位：1bit，永远为0\n时间戳：31bit，以秒为单位，可以使用69年\n序列号：32bit，秒内的计数器，支持每秒产生2^32个不同ID\nRedis实现全局唯一Id@Component\npublic class RedisIdWorker &#123;\n    /**\n     * 开始时间戳\n     */\n    private static final long BEGIN_TIMESTAMP = 1640995200L;\n    /**\n     * 序列号的位数\n     */\n    private static final int COUNT_BITS = 32;\n\n    private StringRedisTemplate stringRedisTemplate;\n\n    public RedisIdWorker(StringRedisTemplate stringRedisTemplate) &#123;\n        this.stringRedisTemplate = stringRedisTemplate;\n    &#125;\n\n    public long nextId(String keyPrefix) &#123;\n        // 1.生成时间戳\n        LocalDateTime now = LocalDateTime.now();\n        long nowSecond = now.toEpochSecond(ZoneOffset.UTC);\n        long timestamp = nowSecond - BEGIN_TIMESTAMP;\n\n        // 2.生成序列号\n        // 2.1.获取当前日期，精确到天\n        String date = now.format(DateTimeFormatter.ofPattern(\"yyyy:MM:dd\"));\n        // 2.2.自增长\n        long count = stringRedisTemplate.opsForValue().increment(\"icr:\" + keyPrefix + \":\" + date);\n        // 3.拼接并返回\n        return timestamp &lt;&lt; COUNT_BITS | count;\n    &#125;\n&#125;\n\n测试\ncountdownlatch名为信号枪：主要的作用是同步协调在多线程的等待于唤醒问题\n我们如果没有CountDownLatch ，那么由于程序是异步的，当异步程序没有执行完时，主线程就已经执行完了，然后我们期望的是子线程全部走完之后，主线程再走，所以我们此时需要使用到CountDownLatch\nCountDownLatch 中有两个最重要的方法\n1、countDown\n2、await\nawait 方法 是阻塞方法，我们担心子线程没有执行完时，main线程就先执行，所以使用await可以让main线程阻塞，那么什么时候main线程不再阻塞呢？当CountDownLatch  内部维护的 变量变为0时，就不再阻塞，直接放行，那么什么时候CountDownLatch   维护的变量变为0 呢，我们只需要调用一次countDown ，内部变量就减少1，我们让子线程和变量绑定， 执行完一个子线程就减少一个变量，当分线程全部走完，CountDownLatch 维护的变量就是0，此时await就不再阻塞，统计出来的时间也就是所有子线程执行完后的时间。\n@Test\nvoid testIdWorker() throws InterruptedException &#123;\n    CountDownLatch latch = new CountDownLatch(300);\n\n    Runnable task = () -> &#123;\n        for (int i = 0; i &lt; 100; i++) &#123;\n            long id = redisIdWorker.nextId(\"order\");\n            System.out.println(\"id = \" + id);\n        &#125;\n        latch.countDown();\n    &#125;;\n    long begin = System.currentTimeMillis();\n    for (int i = 0; i &lt; 300; i++) &#123;\n        es.submit(task);\n    &#125;\n    latch.await();\n    long end = System.currentTimeMillis();\n    System.out.println(\"time = \" + (end - begin));\n&#125;\n\n新增秒杀优惠卷VoucherController\n@PostMapping(\"seckill\")\npublic Result addSeckillVoucher(@RequestBody Voucher voucher) &#123;\n    voucherService.addSeckillVoucher(voucher);\n    return Result.ok(voucher.getId());\n&#125;\n\nVoucherServiceImpl\n@Override\n@Transactional\npublic void addSeckillVoucher(Voucher voucher) &#123;\n    // 保存优惠券\n    save(voucher);\n    // 保存秒杀信息\n    SeckillVoucher seckillVoucher = new SeckillVoucher();\n    seckillVoucher.setVoucherId(voucher.getId());\n    seckillVoucher.setStock(voucher.getStock());\n    seckillVoucher.setBeginTime(voucher.getBeginTime());\n    seckillVoucher.setEndTime(voucher.getEndTime());\n    seckillVoucherService.save(seckillVoucher);\n    // 保存秒杀库存到Redis中\n    stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString());\n&#125;\n\n实现秒杀下单秒杀下单应该思考的内容：\n下单时需要判断两点：\n\n秒杀是否开始或结束，如果尚未开始或已经结束则无法下单\n库存是否充足，不足则无法下单\n\n下单核心逻辑分析：\n当用户开始进行下单，我们应当去查询优惠卷信息，查询到优惠卷信息，判断是否满足秒杀条件\n比如时间是否充足，如果时间充足，则进一步判断库存是否足够，如果两者都满足，则扣减库存，创建订单，然后返回订单id，如果有一个条件不满足则直接结束。\n\nVoucherOrderServiceImpl\n@Override\npublic Result seckillVoucher(Long voucherId) &#123;\n    // 1.查询优惠券\n    SeckillVoucher voucher = seckillVoucherService.getById(voucherId);\n    // 2.判断秒杀是否开始\n    if (voucher.getBeginTime().isAfter(LocalDateTime.now())) &#123;\n        // 尚未开始\n        return Result.fail(\"秒杀尚未开始！\");\n    &#125;\n    // 3.判断秒杀是否已经结束\n    if (voucher.getEndTime().isBefore(LocalDateTime.now())) &#123;\n        // 已经结束\n        return Result.fail(\"秒杀已经结束！\");\n    &#125;\n    // 4.判断库存是否充足\n    if (voucher.getStock() &lt; 1) &#123;\n        // 库存不足\n        return Result.fail(\"库存不足！\");\n    &#125;\n    //5，扣减库存\n    boolean success = seckillVoucherService.update()\n            .setSql(\"stock= stock -1\")\n            .eq(\"voucher_id\", voucherId).update();\n    if (!success) &#123;\n        //扣减库存\n        return Result.fail(\"库存不足！\");\n    &#125;\n    //6.创建订单\n    VoucherOrder voucherOrder = new VoucherOrder();\n    // 6.1.订单id\n    long orderId = redisIdWorker.nextId(\"order\");\n    voucherOrder.setId(orderId);\n    // 6.2.用户id\n    Long userId = UserHolder.getUser().getId();\n    voucherOrder.setUserId(userId);\n    // 6.3.代金券id\n    voucherOrder.setVoucherId(voucherId);\n    save(voucherOrder);\n\n    return Result.ok(orderId);\n\n&#125;\n\n库存超卖问题分析有关超卖问题分析：在我们原有代码中是这么写的\nif (voucher.getStock() &lt; 1) &#123;\n       // 库存不足\n       return Result.fail(\"库存不足！\");\n   &#125;\n   //5，扣减库存\n   boolean success = seckillVoucherService.update()\n           .setSql(\"stock= stock -1\")\n           .eq(\"voucher_id\", voucherId).update();\n   if (!success) &#123;\n       //扣减库存\n       return Result.fail(\"库存不足！\");\n   &#125;\n\n假设线程1过来查询库存，判断出来库存大于1，正准备去扣减库存，但是还没有来得及去扣减，此时线程2过来，线程2也去查询库存，发现这个数量一定也大于1，那么这两个线程都会去扣减库存，最终多个线程相当于一起去扣减库存，此时就会出现库存的超卖问题。\n\n超卖问题是典型的多线程安全问题，针对这一问题的常见解决方案就是加锁：而对于加锁，我们通常有两种解决方案：见下图：\n\n悲观锁：\n 悲观锁可以实现对于数据的串行化执行，比如syn，和lock都是悲观锁的代表，同时，悲观锁中又可以再细分为公平锁，非公平锁，可重入锁，等等\n乐观锁：\n  乐观锁：会有一个版本号，每次操作数据会对版本号+1，再提交回数据时，会去校验是否比之前的版本大1 ，如果大1 ，则进行操作成功，这套机制的核心逻辑在于，如果在操作过程中，版本号只比原来大1 ，那么就意味着操作过程中没有人对他进行过修改，他的操作就是安全的，如果不大1，则数据被修改过，当然乐观锁还有一些变种的处理方式比如cas\n  乐观锁的典型代表：就是cas，利用cas进行无锁化机制加锁，var5 是操作前读取的内存值，while中的var1+var2 是预估值，如果预估值 &#x3D;&#x3D; 内存值，则代表中间没有被人修改过，此时就将新值去替换 内存值\n  其中do while 是为了在操作失败时，再次进行自旋操作，即把之前的逻辑再操作一次。\nint var5;\ndo &#123;\n    var5 = this.getIntVolatile(var1, var2);\n&#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));\n\nreturn var5;\n\n使用了乐观锁思想方式：\n课程中的使用方式是没有像cas一样带自旋的操作，也没有对version的版本号+1 ，他的操作逻辑是在操作时，对版本号进行+1 操作，然后要求version 如果是1 的情况下，才能操作，那么第一个线程在操作后，数据库中的version变成了2，但是他自己满足version&#x3D;1 ，所以没有问题，此时线程2执行，线程2 最后也需要加上条件version &#x3D;1 ，但是现在由于线程1已经操作过了，所以线程2，操作时就不满足version&#x3D;1 的条件了，所以线程2无法执行成功\n\n乐观锁解决超卖问题修改代码方案一\nVoucherOrderServiceImpl 在扣减库存时，改为：\nboolean success = seckillVoucherService.update()\n            .setSql(\"stock= stock -1\") //set stock = stock -1\n            .eq(\"voucher_id\", voucherId)\n    \t\t.eq(\"stock\",voucher.getStock()).update(); //where id = ？ and stock = ?\n\n以上逻辑的核心含义是：只要我扣减库存时的库存和之前我查询到的库存是一样的，就意味着没有人在中间修改过库存，那么此时就是安全的，但是以上这种方式通过测试发现会有很多失败的情况，失败的原因在于：在使用乐观锁过程中假设100个线程同时都拿到了100的库存，然后大家一起去进行扣减，但是100个人中只有1个人能扣减成功，其他的人在处理时，他们在扣减时，库存已经被修改过了，所以此时其他线程都会失败\n修改代码方案二\n之前的方式要修改前后都保持一致，但是这样成功的概率太低，乐观锁需要变一下，改成stock大于0 即可\nboolean success = seckillVoucherService.update()\n            .setSql(\"stock= stock -1\")\n            .eq(\"voucher_id\", voucherId).update().gt(\"stock\",0); //where id = ? and stock > 0\n\n小扩展：\n针对cas中的自旋压力过大，我们可以使用Longaddr这个类去解决\nJava8 提供的一个对AtomicLong改进后的一个类，LongAdder\n大量线程并发更新一个原子性的时候，天然的问题就是自旋，会导致并发性问题，当然这也比我们直接使用syn来的好\n所以利用这么一个类，LongAdder来进行优化\n如果获取某个值，则会对cell和base的值进行递增，最后返回一个完整的值\n\n秒杀里的一人一单需求：修改秒杀业务，要求同一个优惠券，一个用户只能下一单\n具体操作逻辑如下：比如时间是否充足，如果时间充足，则进一步判断库存是否足够，然后再根据优惠卷id和用户id查询是否已经下过这个订单，如果下过这个订单，则不再下单，否则进行下单\n\nVoucherOrderServiceImpl  \n增加一人一单逻辑\n@Override\npublic Result seckillVoucher(Long voucherId) &#123;\n    // 1.查询优惠券\n    SeckillVoucher voucher = seckillVoucherService.getById(voucherId);\n    // 2.判断秒杀是否开始\n    if (voucher.getBeginTime().isAfter(LocalDateTime.now())) &#123;\n        // 尚未开始\n        return Result.fail(\"秒杀尚未开始！\");\n    &#125;\n    // 3.判断秒杀是否已经结束\n    if (voucher.getEndTime().isBefore(LocalDateTime.now())) &#123;\n        // 尚未开始\n        return Result.fail(\"秒杀已经结束！\");\n    &#125;\n    // 4.判断库存是否充足\n    if (voucher.getStock() &lt; 1) &#123;\n        // 库存不足\n        return Result.fail(\"库存不足！\");\n    &#125;\n    // 5.一人一单逻辑\n    // 5.1.用户id\n    Long userId = UserHolder.getUser().getId();\n    int count = query().eq(\"user_id\", userId).eq(\"voucher_id\", voucherId).count();\n    // 5.2.判断是否存在\n    if (count > 0) &#123;\n        // 用户已经购买过了\n        return Result.fail(\"用户已经购买过一次！\");\n    &#125;\n\n    //6，扣减库存\n    boolean success = seckillVoucherService.update()\n            .setSql(\"stock= stock -1\")\n            .eq(\"voucher_id\", voucherId).update();\n    if (!success) &#123;\n        //扣减库存\n        return Result.fail(\"库存不足！\");\n    &#125;\n    //7.创建订单\n    VoucherOrder voucherOrder = new VoucherOrder();\n    // 7.1.订单id\n    long orderId = redisIdWorker.nextId(\"order\");\n    voucherOrder.setId(orderId);\n\n    voucherOrder.setUserId(userId);\n    // 7.3.代金券id\n    voucherOrder.setVoucherId(voucherId);\n    save(voucherOrder);\n\n    return Result.ok(orderId);\n\n&#125;\n\n存在问题：现在的问题还是和之前一样，并发过来，查询数据库，都不存在订单，所以我们还是需要加锁，但是乐观锁比较适合更新数据，而现在是插入数据，所以我们需要使用悲观锁操作\n注意：在这里提到了非常多的问题，我们需要慢慢的来思考，首先初始方案是封装了一个createVoucherOrder方法，同时为了确保他线程安全，在方法上添加了一把synchronized 锁\n@Transactional\npublic synchronized Result createVoucherOrder(Long voucherId) &#123;\n\n\tLong userId = UserHolder.getUser().getId();\n         // 5.1.查询订单\n        int count = query().eq(\"user_id\", userId).eq(\"voucher_id\", voucherId).count();\n        // 5.2.判断是否存在\n        if (count > 0) &#123;\n            // 用户已经购买过了\n            return Result.fail(\"用户已经购买过一次！\");\n        &#125;\n\n        // 6.扣减库存\n        boolean success = seckillVoucherService.update()\n                .setSql(\"stock = stock - 1\") // set stock = stock - 1\n                .eq(\"voucher_id\", voucherId).gt(\"stock\", 0) // where id = ? and stock > 0\n                .update();\n        if (!success) &#123;\n            // 扣减失败\n            return Result.fail(\"库存不足！\");\n        &#125;\n\n        // 7.创建订单\n        VoucherOrder voucherOrder = new VoucherOrder();\n        // 7.1.订单id\n        long orderId = redisIdWorker.nextId(\"order\");\n        voucherOrder.setId(orderId);\n        // 7.2.用户id\n        voucherOrder.setUserId(userId);\n        // 7.3.代金券id\n        voucherOrder.setVoucherId(voucherId);\n        save(voucherOrder);\n\n        // 7.返回订单id\n        return Result.ok(orderId);\n&#125;\n\n但是这样添加锁，锁的粒度太粗了，在使用锁过程中，控制锁粒度 是一个非常重要的事情，如果锁的粒度太大，会导致每个线程进来都会锁住，所以需要去控制锁的粒度。\n以下这段代码需要修改为：intern() 这个方法是从常量池中拿到数据，如果我们直接使用userId.toString() 他拿到的对象实际上是不同的对象，new出来的对象，我们使用锁必须保证锁必须是同一把，所以我们需要使用intern()方法\n@Transactional\npublic  Result createVoucherOrder(Long voucherId) &#123;\n\tLong userId = UserHolder.getUser().getId();\n\tsynchronized(userId.toString().intern())&#123;\n         // 5.1.查询订单\n        int count = query().eq(\"user_id\", userId).eq(\"voucher_id\", voucherId).count();\n        // 5.2.判断是否存在\n        if (count > 0) &#123;\n            // 用户已经购买过了\n            return Result.fail(\"用户已经购买过一次！\");\n        &#125;\n\n        // 6.扣减库存\n        boolean success = seckillVoucherService.update()\n                .setSql(\"stock = stock - 1\") // set stock = stock - 1\n                .eq(\"voucher_id\", voucherId).gt(\"stock\", 0) // where id = ? and stock > 0\n                .update();\n        if (!success) &#123;\n            // 扣减失败\n            return Result.fail(\"库存不足！\");\n        &#125;\n\n        // 7.创建订单\n        VoucherOrder voucherOrder = new VoucherOrder();\n        // 7.1.订单id\n        long orderId = redisIdWorker.nextId(\"order\");\n        voucherOrder.setId(orderId);\n        // 7.2.用户id\n        voucherOrder.setUserId(userId);\n        // 7.3.代金券id\n        voucherOrder.setVoucherId(voucherId);\n        save(voucherOrder);\n\n        // 7.返回订单id\n        return Result.ok(orderId);\n    &#125;\n&#125;\n\n但是以上代码还是存在问题，问题的原因在于当前方法被spring的事务控制，如果你在方法内部加锁，可能会导致当前方法事务还没有提交，但是锁已经释放也会导致问题，所以我们选择将当前方法整体包裹起来，确保事务不会出现问题：如下：\n在seckillVoucher 方法中，添加以下逻辑，这样就能保证事务的特性，同时也控制了锁的粒度\n\n但是以上做法依然有问题，因为你调用的方法，其实是this.的方式调用的，事务想要生效，还得利用代理来生效，所以这个地方，我们需要获得原始的事务对象， 来操作事务\n\n集群环境下的并发问题通过加锁可以解决在单机情况下的一人一单安全问题，但是在集群模式下就不行了。\n有关锁失效原因分析\n由于现在我们部署了多个tomcat，每个tomcat都有一个属于自己的jvm，那么假设在服务器A的tomcat内部，有两个线程，这两个线程由于使用的是同一份代码，那么他们的锁对象是同一个，是可以实现互斥的。但是如果现在是服务器B的tomcat内部，又有两个线程，但是他们的锁对象写的虽然和服务器A一样，但是锁对象却不是同一个，所以线程3和线程4可以实现互斥，但是却无法和线程1和线程2实现互斥，这就是 集群环境下，syn锁失效的原因，在这种情况下，我们就需要使用分布式锁来解决这个问题。\n\n","slug":"Redis实战-秒杀","date":"2023-06-04T08:36:56.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"7d0a0586b751a56e3829bac5576874c8","title":"Redis实战-查询缓存","content":"查询缓存缓存(Cache),就是数据交换的缓冲区,俗称的缓存就是缓冲区内的数据,一般从数据库中获取,存储于本地代码\n例1:static final ConcurrentHashMap&lt;K,V> map = new ConcurrentHashMap&lt;>(); 本地用于高并发\n\n例2:static final Cache&lt;K,V> USER_CACHE = CacheBuilder.newBuilder().build(); 用于redis等缓存\n\n例3:static final Map&lt;K,V> map =  new HashMap(); 本地缓存\n\n由于其被Static修饰,所以随着类的加载而被加载到内存之中,作为本地缓存,由于其又被final修饰,所以其引用和对象之间的关系是固定的,不能改变的,因此不用担心导致缓存失效;\n添加商户缓存在我们查询商户信息时，我们是直接操作从数据库中去进行查询的，大致逻辑是这样，直接查询数据库那肯定慢咯，所以我们需要增加缓存\n@GetMapping(\"/&#123;id&#125;\")\npublic Result queryShopById(@PathVariable(\"id\") Long id) &#123;\n    //这里是直接查询数据库\n    return shopService.queryById(id);\n&#125;\n\n缓存模型和思路标准的操作方式就是查询数据库之前先查询缓存，如果缓存数据存在，则直接从缓存中返回，如果缓存数据不存在，再查询数据库，然后将数据存入redis。\n\n代码如下代码思路：如果缓存有，则直接返回，如果缓存不存在，则查询数据库，然后存入redis。\n\n缓存更新策略缓存更新是redis为了节约内存而设计出来的一个东西，主要是因为内存数据宝贵，当我们向redis插入太多数据，此时就可能会导致缓存中的数据过多，所以redis会对部分数据进行更新(淘汰)\n内存淘汰：redis自动进行，当redis内存达到咱们设定的max-memery的时候，会自动触发淘汰机制，淘汰掉一些不重要的数据(可以自己设置策略方式)\n超时剔除：当我们给redis设置了过期时间ttl之后，redis会将超时的数据进行删除，方便咱们继续使用缓存\n主动更新：我们可以手动调用方法把缓存删掉，通常用于解决缓存和数据库不一致问题\n\n数据库缓存不一致解决方案：由于我们的缓存的数据源来自于数据库,而数据库的数据是会发生变化的,因此,如果当数据库中数据发生变化,而缓存却没有同步,此时就会有一致性问题存在，有如下几种方案\nCache Aside Pattern 人工编码方式：缓存调用者在更新完数据库后再去更新缓存，也称之为双写方案\nRead&#x2F;Write Through Pattern : 由系统本身完成，数据库与缓存的问题交由系统本身去处理\nWrite Behind Caching Pattern ：调用者只操作缓存，其他线程去异步处理数据库，实现最终一致\n\n数据库和缓存不一致采用双写一致方案操作缓存和数据库时有三个问题需要考虑：\n假设我们每次操作数据库后，都操作缓存，但是中间如果没有人查询，那么这个更新动作实际上只有最后一次生效，中间的更新动作意义并不大，我们可以把缓存删除，等待再次查询时，将缓存中的数据加载出来\n\n删除缓存还是更新缓存？\n\n更新缓存：每次更新数据库都更新缓存，无效写操作较多\n删除缓存：更新数据库时让缓存失效，查询时再更新缓存\n\n\n如何保证缓存与数据库的操作的同时成功或失败？\n\n单体系统，将缓存与数据库操作放在一个事务\n分布式系统，利用TCC等分布式事务方案\n\n\n\n应该具体操作缓存还是操作数据库，我们应当是先操作数据库，再删除缓存，原因在于，如果你选择第一种方案，在两个线程并发来访问时，假设线程1先来，他先把缓存删了，此时线程2过来，他查询缓存数据并不存在，此时他写入缓存。之后，线程1再执行更新动作时，实际上写入的就是旧的数据，新的数据被旧数据覆盖了。\n\n先操作缓存还是先操作数据库？\n先删除缓存，再操作数据库\n先操作数据库，再删除缓存\n\n\n\n\n实现商铺和缓存与数据库双写一致核心思路如下：\n修改ShopController中的业务逻辑，满足下面的需求：\n根据id查询店铺时，如果缓存未命中，则查询数据库，将数据库结果写入缓存，并设置超时时间\n根据id修改店铺时，先修改数据库，再删除缓存\n设置redis缓存时添加过期时间\n\n代码分析：通过之前的淘汰，我们确定了采用删除策略，来解决双写问题，当我们修改了数据之后，然后把缓存中的数据进行删除，查询时发现缓存中没有数据，则会从mysql中加载最新的数据，从而避免数据库和缓存不一致的问题\n\n缓存穿透的解决思路缓存穿透 ：缓存穿透是指客户端请求的数据在缓存中和数据库中都不存在，这样缓存永远不会生效，这些请求都会打到数据库。\n常见的解决方案有两种：\n\n缓存空对象\n优点：实现简单，维护方便\n缺点：\n额外的内存消耗\n可能造成短期的不一致\n\n\n\n\n布隆过滤\n优点：内存占用较少，没有多余key\n缺点：\n实现复杂\n存在误判可能\n\n\n\n\n\n缓存空对象思路分析：当我们客户端访问不存在的数据时，先请求redis，但是此时redis中没有数据，此时会访问到数据库，但是数据库中也没有数据，这个数据穿透了缓存，直击数据库，我们都知道数据库能够承载的并发不如redis这么高，如果大量的请求同时过来访问这种不存在的数据，这些请求就都会访问到数据库，简单的解决方案就是哪怕这个数据在数据库中也不存在，我们也把这个数据存入到redis中去，这样，下次用户过来访问这个不存在的数据，那么在redis中也能找到这个数据就不会进入到缓存了\n布隆过滤：布隆过滤器其实采用的是哈希思想来解决这个问题，通过一个庞大的二进制数组，走哈希思想去判断当前这个要查询的这个数据是否存在，如果布隆过滤器判断存在，则放行，这个请求会去访问redis，哪怕此时redis中的数据过期了，但是数据库中一定存在这个数据，在数据库中查询出来这个数据后，再将其放入到redis中，\n假设布隆过滤器判断这个数据不存在，则直接返回\n这种方式优点在于节约内存空间，存在误判，误判原因在于：布隆过滤器走的是哈希思想，只要是哈希思想，就可能存在哈希冲突\n\n解决商品查询的缓存穿透问题：核心思路如下：\n如果这个数据不存在，我们不会返回404 ，还是会把这个数据写入到Redis中，并且将value设置为空，当再次发起查询时，我们如果发现命中之后，判断这个value是否是null，如果是null，则是之前写入的数据，证明是缓存穿透数据，如果不是，则直接返回数据。\n\n缓存穿透产生的原因是什么？\n\n用户请求的数据在缓存中和数据库中都不存在，不断发起这样的请求，给数据库带来巨大压力\n\n缓存穿透的解决方案有哪些？\n\n缓存null值\n布隆过滤\n增强id的复杂度，避免被猜测id规律\n做好数据的基础格式校验\n加强用户权限校验\n做好热点参数的限流\n\n缓存雪崩问题及解决思路缓存雪崩是指在同一时段大量的缓存key同时失效或者Redis服务宕机，导致大量请求到达数据库，带来巨大压力。\n解决方案：\n\n给不同的Key的TTL添加随机值\n利用Redis集群提高服务的可用性\n给缓存业务添加降级限流策略\n给业务添加多级缓存\n\n\n缓存击穿问题及解决思路缓存击穿问题也叫热点Key问题，就是一个被高并发访问并且缓存重建业务较复杂的key突然失效了，无数的请求访问会在瞬间给数据库带来巨大的冲击。\n常见的解决方案有两种：\n\n互斥锁\n逻辑过期\n\n逻辑分析：假设线程1在查询缓存之后，本来应该去查询数据库，然后把这个数据重新加载到缓存的，此时只要线程1走完这个逻辑，其他线程就都能从缓存中加载这些数据了，但是假设在线程1没有走完的时候，后续的线程2，线程3，线程4同时过来访问当前这个方法， 那么这些线程都不能从缓存中查询到数据，那么他们就会同一时刻来访问查询缓存，都没查到，接着同一时间去访问数据库，同时的去执行数据库代码，对数据库访问压力过大\n\n使用锁来解决方案\n因为锁能实现互斥性。假设线程过来，只能一个人一个人的来访问数据库，从而避免对于数据库访问压力过大，但这也会影响查询的性能，因为此时会让查询的性能从并行变成了串行，我们可以采用tryLock方法 + double check来解决这样的问题。\n假设现在线程1过来访问，他查询缓存没有命中，但是此时他获得到了锁的资源，那么线程1就会一个人去执行逻辑，假设现在线程2过来，线程2在执行过程中，并没有获得到锁，那么线程2就可以进行到休眠，直到线程1把锁释放后，线程2获得到锁，然后再来执行逻辑，此时就能够从缓存中拿到数据了。\n\n逻辑过期方案\n方案分析：我们之所以会出现这个缓存击穿问题，主要原因是在于我们对key设置了过期时间，假设我们不设置过期时间，其实就不会有缓存击穿的问题，但是不设置过期时间，这样数据不就一直占用我们内存了吗，我们可以采用逻辑过期方案。\n我们把过期时间设置在 redis的value中，注意：这个过期时间并不会直接作用于redis，而是我们后续通过逻辑去处理。假设线程1去查询缓存，然后从value中判断出来当前的数据已经过期了，此时线程1去获得互斥锁，那么其他线程会进行阻塞，获得了锁的线程他会开启一个 线程去进行 以前的重构数据的逻辑，直到新开的线程完成这个逻辑后，才释放锁， 而线程1直接进行返回，假设现在线程3过来访问，由于线程2持有着锁，所以线程3无法获得锁，线程3也直接返回数据，只有等到新开的线程2把重建数据构建完后，其他线程才能走返回正确的数据。\n这种方案巧妙在于，异步的构建缓存，缺点在于在构建完缓存之前，返回的都是脏数据。\n\n进行对比\n互斥锁方案：由于保证了互斥性，所以数据一致，且实现简单，因为仅仅只需要加一把锁而已，也没其他的事情需要操心，所以没有额外的内存消耗，缺点在于有锁就有死锁问题的发生，且只能串行执行性能肯定受到影响\n逻辑过期方案： 线程读取过程中不需要等待，性能好，有一个额外的线程持有锁去进行重构数据，但是在重构数据完成前，其他的线程只能返回之前的数据，且实现起来麻烦\n\n利用互斥锁解决缓存击穿问题核心思路：相较于原来从缓存中查询不到数据后直接查询数据库而言，现在的方案是 进行查询之后，如果从缓存没有查询到数据，则进行互斥锁的获取，获取互斥锁后，判断是否获得到了锁，如果没有获得到，则休眠，过一会再进行尝试，直到获取到锁为止，才能进行查询\n如果获取到了锁的线程，再去进行查询，查询后将数据写入redis，再释放锁，返回数据，利用互斥锁就能保证只有一个线程去执行操作数据库的逻辑，防止缓存击穿\n\n操作锁的代码：\n核心思路就是利用redis的setnx方法来表示获取锁，该方法含义是redis中如果没有这个key，则插入成功，返回1，在stringRedisTemplate中返回true，  如果有这个key则插入失败，则返回0，在stringRedisTemplate返回false，我们可以通过true，或者是false，来表示是否有线程成功插入key，成功插入的key的线程我们认为他就是获得到锁的线程。\nprivate boolean tryLock(String key) &#123;\n    Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \"1\", 10, TimeUnit.SECONDS);\n    return BooleanUtil.isTrue(flag);\n&#125;\n\nprivate void unlock(String key) &#123;\n    stringRedisTemplate.delete(key);\n&#125;\n\n操作代码：\npublic Shop queryWithMutex(Long id)  &#123;\n       String key = CACHE_SHOP_KEY + id;\n       // 1、从redis中查询商铺缓存\n       String shopJson = stringRedisTemplate.opsForValue().get(\"key\");\n       // 2、判断是否存在\n       if (StrUtil.isNotBlank(shopJson)) &#123;\n           // 存在,直接返回\n           return JSONUtil.toBean(shopJson, Shop.class);\n       &#125;\n       //判断命中的值是否是空值\n       if (shopJson != null) &#123;\n           //返回一个错误信息\n           return null;\n       &#125;\n       // 4.实现缓存重构\n       //4.1 获取互斥锁\n       String lockKey = \"lock:shop:\" + id;\n       Shop shop = null;\n       try &#123;\n           boolean isLock = tryLock(lockKey);\n           // 4.2 判断否获取成功\n           if(!isLock)&#123;\n               //4.3 失败，则休眠重试\n               Thread.sleep(50);\n               return queryWithMutex(id);\n           &#125;\n           //4.4 成功，根据id查询数据库\n            shop = getById(id);\n           // 5.不存在，返回错误\n           if(shop == null)&#123;\n                //将空值写入redis\n               stringRedisTemplate.opsForValue().set(key,\"\",CACHE_NULL_TTL,TimeUnit.MINUTES);\n               //返回错误信息\n               return null;\n           &#125;\n           //6.写入redis\n           stringRedisTemplate.opsForValue().set(key,JSONUtil.toJsonStr(shop),CACHE_NULL_TTL,TimeUnit.MINUTES);\n\n       &#125;catch (Exception e)&#123;\n           throw new RuntimeException(e);\n       &#125;\n       finally &#123;\n           //7.释放互斥锁\n           unlock(lockKey);\n       &#125;\n       return shop;\n   &#125;\n\n利用逻辑过期解决缓存击穿问题需求：修改根据id查询商铺的业务，基于逻辑过期方式来解决缓存击穿问题\n思路分析：当用户开始查询redis时，判断是否命中，如果没有命中则直接返回空数据，不查询数据库，而一旦命中后，将value取出，判断value中的过期时间是否满足，如果没有过期，则直接返回redis中的数据，如果过期，则在开启独立线程后直接返回之前的数据，独立线程去重构数据，重构完成后释放互斥锁。\n\n新建一个实体类，对原来代码没有侵入性。\n@Data\npublic class RedisData &#123;\n    private LocalDateTime expireTime;\n    private Object data;\n&#125;\n\nShopServiceImpl\nprivate static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10);\npublic Shop queryWithLogicalExpire( Long id ) &#123;\n    String key = CACHE_SHOP_KEY + id;\n    // 1.从redis查询商铺缓存\n    String json = stringRedisTemplate.opsForValue().get(key);\n    // 2.判断是否存在\n    if (StrUtil.isBlank(json)) &#123;\n        // 3.存在，直接返回\n        return null;\n    &#125;\n    // 4.命中，需要先把json反序列化为对象\n    RedisData redisData = JSONUtil.toBean(json, RedisData.class);\n    Shop shop = JSONUtil.toBean((JSONObject) redisData.getData(), Shop.class);\n    LocalDateTime expireTime = redisData.getExpireTime();\n    // 5.判断是否过期\n    if(expireTime.isAfter(LocalDateTime.now())) &#123;\n        // 5.1.未过期，直接返回店铺信息\n        return shop;\n    &#125;\n    // 5.2.已过期，需要缓存重建\n    // 6.缓存重建\n    // 6.1.获取互斥锁\n    String lockKey = LOCK_SHOP_KEY + id;\n    boolean isLock = tryLock(lockKey);\n    // 6.2.判断是否获取锁成功\n    if (isLock)&#123;\n        //异步更新\n        CACHE_REBUILD_EXECUTOR.submit( ()->&#123;\n            try&#123;\n                //重建缓存\n                this.saveShop2Redis(id,20L);\n            &#125;catch (Exception e)&#123;\n                throw new RuntimeException(e);\n            &#125;finally &#123;\n                unlock(lockKey);\n            &#125;\n        &#125;);\n    &#125;\n    // 6.4.返回过期的商铺信息\n    return shop;\n&#125;\n\n封装Redis工具类基于StringRedisTemplate封装一个缓存工具类，满足下列需求：\n\n方法1：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置TTL过期时间\n方法2：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置逻辑过期时间，用于处理缓\n\n存击穿问题\n\n方法3：根据指定的key查询缓存，并反序列化为指定类型，利用缓存空值的方式解决缓存穿透问题\n方法4：根据指定的key查询缓存，并反序列化为指定类型，需要利用逻辑过期解决缓存击穿问题\n\n将逻辑进行封装\n@Slf4j\n@Component\npublic class CacheClient &#123;\n\n    private final StringRedisTemplate stringRedisTemplate;\n\n    private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10);\n\n    public CacheClient(StringRedisTemplate stringRedisTemplate) &#123;\n        this.stringRedisTemplate = stringRedisTemplate;\n    &#125;\n\n    public void set(String key, Object value, Long time, TimeUnit unit) &#123;\n        stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, unit);\n    &#125;\n\n    public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit unit) &#123;\n        // 设置逻辑过期\n        RedisData redisData = new RedisData();\n        redisData.setData(value);\n        redisData.setExpireTime(LocalDateTime.now().plusSeconds(unit.toSeconds(time)));\n        // 写入Redis\n        stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData));\n    &#125;\n\n    public &lt;R,ID> R queryWithPassThrough(\n            String keyPrefix, ID id, Class&lt;R> type, Function&lt;ID, R> dbFallback, Long time, TimeUnit unit)&#123;\n        String key = keyPrefix + id;\n        // 1.从redis查询商铺缓存\n        String json = stringRedisTemplate.opsForValue().get(key);\n        // 2.判断是否存在\n        if (StrUtil.isNotBlank(json)) &#123;\n            // 3.存在，直接返回\n            return JSONUtil.toBean(json, type);\n        &#125;\n        // 判断命中的是否是空值\n        if (json != null) &#123;\n            // 返回一个错误信息\n            return null;\n        &#125;\n\n        // 4.不存在，根据id查询数据库\n        R r = dbFallback.apply(id);\n        // 5.不存在，返回错误\n        if (r == null) &#123;\n            // 将空值写入redis\n            stringRedisTemplate.opsForValue().set(key, \"\", CACHE_NULL_TTL, TimeUnit.MINUTES);\n            // 返回错误信息\n            return null;\n        &#125;\n        // 6.存在，写入redis\n        this.set(key, r, time, unit);\n        return r;\n    &#125;\n\n    public &lt;R, ID> R queryWithLogicalExpire(\n            String keyPrefix, ID id, Class&lt;R> type, Function&lt;ID, R> dbFallback, Long time, TimeUnit unit) &#123;\n        String key = keyPrefix + id;\n        // 1.从redis查询商铺缓存\n        String json = stringRedisTemplate.opsForValue().get(key);\n        // 2.判断是否存在\n        if (StrUtil.isBlank(json)) &#123;\n            // 3.存在，直接返回\n            return null;\n        &#125;\n        // 4.命中，需要先把json反序列化为对象\n        RedisData redisData = JSONUtil.toBean(json, RedisData.class);\n        R r = JSONUtil.toBean((JSONObject) redisData.getData(), type);\n        LocalDateTime expireTime = redisData.getExpireTime();\n        // 5.判断是否过期\n        if(expireTime.isAfter(LocalDateTime.now())) &#123;\n            // 5.1.未过期，直接返回店铺信息\n            return r;\n        &#125;\n        // 5.2.已过期，需要缓存重建\n        // 6.缓存重建\n        // 6.1.获取互斥锁\n        String lockKey = LOCK_SHOP_KEY + id;\n        boolean isLock = tryLock(lockKey);\n        // 6.2.判断是否获取锁成功\n        if (isLock)&#123;\n            // 6.3.成功，开启独立线程，实现缓存重建\n            CACHE_REBUILD_EXECUTOR.submit(() -> &#123;\n                try &#123;\n                    // 查询数据库\n                    R newR = dbFallback.apply(id);\n                    // 重建缓存\n                    this.setWithLogicalExpire(key, newR, time, unit);\n                &#125; catch (Exception e) &#123;\n                    throw new RuntimeException(e);\n                &#125;finally &#123;\n                    // 释放锁\n                    unlock(lockKey);\n                &#125;\n            &#125;);\n        &#125;\n        // 6.4.返回过期的商铺信息\n        return r;\n    &#125;\n\n    public &lt;R, ID> R queryWithMutex(\n            String keyPrefix, ID id, Class&lt;R> type, Function&lt;ID, R> dbFallback, Long time, TimeUnit unit) &#123;\n        String key = keyPrefix + id;\n        // 1.从redis查询商铺缓存\n        String shopJson = stringRedisTemplate.opsForValue().get(key);\n        // 2.判断是否存在\n        if (StrUtil.isNotBlank(shopJson)) &#123;\n            // 3.存在，直接返回\n            return JSONUtil.toBean(shopJson, type);\n        &#125;\n        // 判断命中的是否是空值\n        if (shopJson != null) &#123;\n            // 返回一个错误信息\n            return null;\n        &#125;\n\n        // 4.实现缓存重建\n        // 4.1.获取互斥锁\n        String lockKey = LOCK_SHOP_KEY + id;\n        R r = null;\n        try &#123;\n            boolean isLock = tryLock(lockKey);\n            // 4.2.判断是否获取成功\n            if (!isLock) &#123;\n                // 4.3.获取锁失败，休眠并重试\n                Thread.sleep(50);\n                return queryWithMutex(keyPrefix, id, type, dbFallback, time, unit);\n            &#125;\n            // 4.4.获取锁成功，根据id查询数据库\n            r = dbFallback.apply(id);\n            // 5.不存在，返回错误\n            if (r == null) &#123;\n                // 将空值写入redis\n                stringRedisTemplate.opsForValue().set(key, \"\", CACHE_NULL_TTL, TimeUnit.MINUTES);\n                // 返回错误信息\n                return null;\n            &#125;\n            // 6.存在，写入redis\n            this.set(key, r, time, unit);\n        &#125; catch (InterruptedException e) &#123;\n            throw new RuntimeException(e);\n        &#125;finally &#123;\n            // 7.释放锁\n            unlock(lockKey);\n        &#125;\n        // 8.返回\n        return r;\n    &#125;\n\n    private boolean tryLock(String key) &#123;\n        Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \"1\", 10, TimeUnit.SECONDS);\n        return BooleanUtil.isTrue(flag);\n    &#125;\n\n    private void unlock(String key) &#123;\n        stringRedisTemplate.delete(key);\n    &#125;\n&#125;\n\n在ShopServiceImpl 中\n@Resource\nprivate CacheClient cacheClient;\n\n @Override\n    public Result queryById(Long id) &#123;\n        // 解决缓存穿透\n        Shop shop = cacheClient\n                .queryWithPassThrough(CACHE_SHOP_KEY, id, Shop.class, this::getById, CACHE_SHOP_TTL, TimeUnit.MINUTES);\n\n        // 互斥锁解决缓存击穿\n        // Shop shop = cacheClient\n        //         .queryWithMutex(CACHE_SHOP_KEY, id, Shop.class, this::getById, CACHE_SHOP_TTL, TimeUnit.MINUTES);\n\n        // 逻辑过期解决缓存击穿\n        // Shop shop = cacheClient\n        //         .queryWithLogicalExpire(CACHE_SHOP_KEY, id, Shop.class, this::getById, 20L, TimeUnit.SECONDS);\n\n        if (shop == null) &#123;\n            return Result.fail(\"店铺不存在！\");\n        &#125;\n        // 7.返回\n        return Result.ok(shop);\n    &#125;\n\n","slug":"Redis实战-查询缓存","date":"2023-06-04T05:51:43.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"794b81b06cdfd907e7a76dd281a21694","title":"Redis实战_共享Session","content":"Redis共享Session短信登录案例基于Session实现登录流程发送验证码：\n用户在提交手机号后，会校验手机号是否合法，如果不合法，则要求用户重新输入手机号\n如果手机号合法，后台此时生成对应的验证码，同时将验证码进行保存，然后再通过短信的方式将验证码发送给用户\n短信验证码登录、注册：\n用户将验证码和手机号进行输入，后台从session中拿到当前验证码，然后和用户输入的验证码进行校验，如果不一致，则无法通过校验，如果一致，则后台根据手机号查询用户，如果用户不存在，则为用户创建账号信息，保存到数据库，无论是否存在，都会将用户信息保存到session中，方便后续获得当前登录信息\n校验登录状态:\n用户在请求时候，会从cookie中携带者JsessionId到后台，后台通过JsessionId从session中拿到用户信息，如果没有session信息，则进行拦截，如果有session信息，则将用户信息保存到threadLocal中，并且放行\n\n关于threadlocal\n在threadLocal中，无论是他的put方法和他的get方法， 都是先从获得当前用户的线程，然后从线程中取出线程的成员变量map，只要线程不一样，map就不一样，所以可以通过这种方式来做到线程隔离\n实现登录拦截功能拦截器代码\nUserHolder是自己实现的，内部逻辑是对ThreadLocal做了封装\npublic class LoginInterceptor implements HandlerInterceptor &#123;\n\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;\n       &#x2F;&#x2F;1.获取session\n        HttpSession session &#x3D; request.getSession();\n        &#x2F;&#x2F;2.获取session中的用户\n        Object user &#x3D; session.getAttribute(&quot;user&quot;);\n        &#x2F;&#x2F;3.判断用户是否存在\n        if(user &#x3D;&#x3D; null)&#123;\n              &#x2F;&#x2F;4.不存在，拦截，返回401状态码\n              response.setStatus(401);\n              return false;\n        &#125;\n        &#x2F;&#x2F;5.存在，保存用户信息到Threadlocal\n        UserHolder.saveUser((User)user);\n        &#x2F;&#x2F;6.放行\n        return true;\n    &#125;\n&#125;\n\n让拦截器生效\n@Configuration\npublic class MvcConfig implements WebMvcConfigurer &#123;\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) &#123;\n        // 登录拦截器\n        registry.addInterceptor(new LoginInterceptor())\n                .excludePathPatterns(\n                        \"/shop/**\",\n                        \"/voucher/**\",\n                        \"/shop-type/**\",\n                        \"/upload/**\",\n                        \"/blog/hot\",\n                        \"/user/code\",\n                        \"/user/login\"\n                ).order(1);\n&#125;\n\n隐藏用户敏感信息通过浏览器观察到此时用户的全部信息都在，这样极为不靠谱，所以应当在返回用户信息之前，将用户的敏感信息进行隐藏，采用的核心思路就是书写一个UserDto对象，这个UserDto对象就没有敏感信息了，我们在返回前，将有用户敏感信息的User对象转化成没有敏感信息的UserDto对象\n在登录方法处修改\nsession.setAttribute(\"user\", BeanUtils.copyProperties(user,UserDTO.class));\n\n在拦截器处：\nUserHolder.saveUser((UserDTO) user);\n\n在UserHolder处：将user对象换成UserDTO\npublic class UserHolder &#123;\n    private static final ThreadLocal&lt;UserDTO> tl = new ThreadLocal&lt;>();\n\n    public static void saveUser(UserDTO user)&#123;\n        tl.set(user);\n    &#125;\n\n    public static UserDTO getUser()&#123;\n        return tl.get();\n    &#125;\n\n    public static void removeUser()&#123;\n        tl.remove();\n    &#125;\n&#125;\n\nsession共享问题每个tomcat中都有一份属于自己的session,假设用户第一次访问第一台tomcat，并且把自己的信息存放到第一台服务器的session中，但是第二次这个用户访问到了第二台tomcat，那么在第二台服务器上，肯定没有第一台服务器存放的session，所以此时 整个登录拦截功能就会出现问题。\n早期的方案是session拷贝，就是说虽然每个tomcat上都有不同的session，但是每当任意一台服务器的session修改时，都会同步给其他的Tomcat服务器的session，这样的话，就可以实现session的共享了\n但是这种方案具有两个大问题\n1、每台服务器中都有完整的一份session数据，服务器压力过大。\n2、session拷贝数据时，可能会出现延迟\n所以咱们后来采用的方案都是基于redis来完成，我们把session换成redis，redis数据本身就是共享的，就可以避免session共享的问题了\n\nRedis代替session的业务流程设计key的结构由于存入的数据比较简单，可以考虑使用String，或者是使用哈希，如下图，如果使用String，value用多占用一点空间，如果使用哈希，value中只会存储他数据本身，如果不是特别在意内存，其实使用String就可以啦。\n\n设计key的具体细节在设计这个key的时候，需要满足两点\n1、key要具有唯一性\n2、key要方便携带\n如果我们采用phone：手机号这个的数据来存储当然是可以的，但是如果把这样的敏感数据存储到redis中并且从页面中带过来毕竟不太合适，所以我们在后台生成一个随机串token，然后让前端带来这个token就能完成我们的整体逻辑了\n整体访问流程当注册完成后，用户去登录会去校验用户提交的手机号和验证码，是否一致，如果一致，则根据手机号查询用户信息，不存在则新建，最后将用户数据保存到redis，并且生成token作为redis的key，当我们校验用户是否登录时，会去携带着token进行访问，从redis中取出token对应的value，判断是否存在这个数据，如果没有则拦截，如果存在则将其保存到threadLocal中，并且放行。\n\n基于Redis实现短信登录UserServiceImpl代码\n@Override\npublic Result login(LoginFormDTO loginForm, HttpSession session) &#123;\n    // 1.校验手机号\n    String phone = loginForm.getPhone();\n    if (RegexUtils.isPhoneInvalid(phone)) &#123;\n        // 2.如果不符合，返回错误信息\n        return Result.fail(\"手机号格式错误！\");\n    &#125;\n    // 3.从redis获取验证码并校验\n    String cacheCode = stringRedisTemplate.opsForValue().get(LOGIN_CODE_KEY + phone);\n    String code = loginForm.getCode();\n    if (cacheCode == null || !cacheCode.equals(code)) &#123;\n        // 不一致，报错\n        return Result.fail(\"验证码错误\");\n    &#125;\n\n    // 4.一致，根据手机号查询用户 select * from tb_user where phone = ?\n    User user = query().eq(\"phone\", phone).one();\n\n    // 5.判断用户是否存在\n    if (user == null) &#123;\n        // 6.不存在，创建新用户并保存\n        user = createUserWithPhone(phone);\n    &#125;\n\n    // 7.保存用户信息到 redis中\n    // 7.1.随机生成token，作为登录令牌\n    String token = UUID.randomUUID().toString(true);\n    // 7.2.将User对象转为HashMap存储\n    UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class);\n    Map&lt;String, Object> userMap = BeanUtil.beanToMap(userDTO, new HashMap&lt;>(),\n            CopyOptions.create()\n                    .setIgnoreNullValue(true)\n                    .setFieldValueEditor((fieldName, fieldValue) -> fieldValue.toString()));\n    // 7.3.存储\n    String tokenKey = LOGIN_USER_KEY + token;\n    stringRedisTemplate.opsForHash().putAll(tokenKey, userMap);\n    // 7.4.设置token有效期\n    stringRedisTemplate.expire(tokenKey, LOGIN_USER_TTL, TimeUnit.MINUTES);\n\n    // 8.返回token\n    return Result.ok(token);\n&#125;\n\n解决状态登录刷新问题初始方案思路总结：在这个方案中，他确实可以使用对应路径的拦截，同时刷新登录token令牌的存活时间，但是现在这个拦截器他只是拦截需要被拦截的路径，假设当前用户访问了一些不需要拦截的路径，那么这个拦截器就不会生效，所以此时令牌刷新的动作实际上就不会执行，所以这个方案他是存在问题的\n优化方案既然之前的拦截器无法对不需要拦截的路径生效，那么我们可以添加一个拦截器，在第一个拦截器中拦截所有的路径，把第二个拦截器做的事情放入到第一个拦截器中，同时刷新令牌，因为第一个拦截器有了threadLocal的数据，所以此时第二个拦截器只需要判断拦截器中的user对象是否存在即可，完成整体刷新功能。\n\nRefreshTokenInterceptor\npublic class RefreshTokenInterceptor implements HandlerInterceptor &#123;\n\n    private StringRedisTemplate stringRedisTemplate;\n\n    public RefreshTokenInterceptor(StringRedisTemplate stringRedisTemplate) &#123;\n        this.stringRedisTemplate = stringRedisTemplate;\n    &#125;\n\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;\n        // 1.获取请求头中的token\n        String token = request.getHeader(\"authorization\");\n        if (StrUtil.isBlank(token)) &#123;\n            return true;\n        &#125;\n        // 2.基于TOKEN获取redis中的用户\n        String key  = LOGIN_USER_KEY + token;\n        Map&lt;Object, Object> userMap = stringRedisTemplate.opsForHash().entries(key);\n        // 3.判断用户是否存在\n        if (userMap.isEmpty()) &#123;\n            return true;\n        &#125;\n        // 5.将查询到的hash数据转为UserDTO\n        UserDTO userDTO = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false);\n        // 6.存在，保存用户信息到 ThreadLocal\n        UserHolder.saveUser(userDTO);\n        // 7.刷新token有效期\n        stringRedisTemplate.expire(key, LOGIN_USER_TTL, TimeUnit.MINUTES);\n        // 8.放行\n        return true;\n    &#125;\n\n    @Override\n    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123;\n        // 移除用户\n        UserHolder.removeUser();\n    &#125;\n&#125;\n\t\n\nLoginInterceptor\npublic class LoginInterceptor implements HandlerInterceptor &#123;\n\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;\n        // 1.判断是否需要拦截（ThreadLocal中是否有用户）\n        if (UserHolder.getUser() == null) &#123;\n            // 没有，需要拦截，设置状态码\n            response.setStatus(401);\n            // 拦截\n            return false;\n        &#125;\n        // 有用户，则放行\n        return true;\n    &#125;\n&#125;\n\n让拦截器生效\n@Configuration\npublic class MvcConfig implements WebMvcConfigurer &#123;\n\n    @Resource\n    private StringRedisTemplate stringRedisTemplate;\n\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) &#123;\n        // 登录拦截器\n        registry.addInterceptor(new LoginInterceptor())\n                .excludePathPatterns(\n                        \"/shop/**\",\n                        \"/voucher/**\",\n                        \"/shop-type/**\",\n                        \"/upload/**\",\n                        \"/blog/hot\",\n                        \"/user/code\",\n                        \"/user/login\"\n                ).order(1);\n        // token刷新的拦截器\n        registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)).addPathPatterns(\"/**\").order(0);\n    &#125;\n&#125;\n\n","slug":"Redis实战-共享Session","date":"2023-06-04T04:17:12.000Z","categories_index":"","tags_index":"Redis实战","author_index":"大宝贝的程序员"},{"id":"e6ecbb75a271638e27405d472d7b664c","title":"CompletableFuture原理与实战","content":"","slug":"CompletableFuture原理与实战","date":"2023-06-04T03:03:29.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"3790eb53c6264ff24e57195f4b6f77f1","title":"Java设计原则中，组合优先于继承","content":"Java 设计原则中，为什么反复强调组合要优先于继承？在《阿里巴巴Java开发手册》中有一条规定：谨慎使用继承的方式进行扩展，优先使用组合的方式实现。\n为什么不推荐使用继承\n是面向对象的四大特性之一，用来表示类之间的is-a关系，可以解决代码复用的问题。虽然继承有诸多作用，但继承层次过深、过复杂，也会影响到代码的可维护性。\n假设我们要设计一个关于鸟的类。我们将“鸟”这样一个抽象的事物概念，定义为一个抽象类AbstractBird。所有更细分的鸟，比如麻雀、鸽子、乌鸦等，都继承这个抽象类。我们知道，大部分鸟都会飞，那我们可不可以在 AbstractBird抽象类中，定义一个fly()方法呢？\n答案是否定的。尽管大部分鸟都会飞，但也有特例，比如鸵鸟就不会飞。鸵鸟继承具有fly()方法的父类，那鸵鸟就具有“飞”这样的行为，虽然它可以独自抛出异常，但不优雅。\npublic class AbstractBird &#123;\n  //...省略其他属性和方法...\n  public void fly() &#123; //... &#125;\n&#125;\n\n\npublic class Ostrich extends AbstractBird &#123; //鸵鸟\n  //...省略其他属性和方法...\n  public void fly() &#123;\n    throw new UnSupportedMethodException(\"I can't fly.'\");\n  &#125;\n&#125;\n\n因为除了鸵鸟之外，不会飞的鸟还有很多，比如企鹅。对于这些不会飞的鸟来说，全部都去重写fly()方法，抛出异常，完全属于代码重复。理论上这些不会飞的鸟根本就不应该拥有fly()方法，让不会飞的鸟暴露fly()接口给外部，增加了被误用的概率。\n要解决上面的问题，就得让AbstractBird类派生出两个更加细分的抽象类：会飞的鸟类AbstractFlyableBird和不会飞的鸟类AbstractUnFlyableBird，让麻雀、乌鸦这些会飞的鸟都继承 AbstractFlyableBird，让鸵鸟、企鹅这些不会飞的鸟，都继承 AbstractUnFlyableBird 类。\n具体的继承关系如下图所示：\n\n这样一来，继承关系变成了三层。但是如果我们不只关注“鸟会不会飞”，还要继续关注“鸟会不会叫”，将鸟划分得更加细致时呢？两个关注行为自由搭配起来会产生四种情况：会飞会叫、不会飞会叫、会飞不会叫、不会飞不会叫。如果继续沿用刚才的设计思路，继承层次会再次加深。\n类的继承层次会越来越深、继承关系会越来越复杂。而这种层次很深、很复杂的继承关系，一方面，会导致代码的可读性变差。我们要搞清楚某个类具有哪些方法、属性，必须阅读父类的代码、父类的父类的代码……一直追溯到最顶层父类的代码。另一方面，这也破坏了类的封装特性，将父类的实现细节暴露给了子类。子类的实现依赖父类的实现，两者高度耦合，一旦父类代码修改，就会影响所有子类的逻辑。\n组合相比继承有哪些优势\n复用性是面向对象技术带来的很棒的潜在好处，代码复用是Java引人注意的功能之一。\nJava代码的复用有继承、组合以及委托三种具体的实现形式。\n对于上面提到的继承带来的问题，可以利用组合（composition）、接口、委托（delegation）三个技术手段一块儿来解决。\n接口表示具有某种行为特性。针对“会飞”这样一个行为特性，我们可以定义一个Flyable接口，只让会飞的鸟去实现这个接口。对于会叫、会下蛋这些行为特性，我们可以类似地定义Tweetable接口、EggLayable接口。\npublic interface Flyable &#123;\n  void fly();\n&#125;\npublic interface Tweetable &#123;\n  void tweet();\n&#125;\npublic interface EggLayable &#123;\n  void layEgg();\n&#125;\npublic class Ostrich implements Tweetable, EggLayable &#123;//鸵鸟\n  //... 省略其他属性和方法...\n  @Override\n  public void tweet() &#123; //... &#125;\n  @Override\n  public void layEgg() &#123; //... &#125;\n&#125;\npublic class Sparrow implements Flayable, Tweetable, EggLayable &#123;//麻雀\n  //... 省略其他属性和方法...\n  @Override\n  public void fly() &#123; //... &#125;\n  @Override\n  public void tweet() &#123; //... &#125;\n  @Override\n  public void layEgg() &#123; //... &#125;\n&#125;\n\n不过，接口只声明方法，不定义实现。也就是说，每个会下蛋的鸟都要实现一遍layEgg()方法，并且实现逻辑几乎是一样的（可能极少场景下会不一样），这就会导致代码重复的问题。那这个问题又该如何解决呢？有以下两种方法。\n使用委托\n针对三个接口再定义三个实现类，它们分别是：实现了fly()方法的 FlyAbility类、实现了tweet()方法的TweetAbility类、实现了layEgg()方法的 EggLayAbility类。然后，通过组合和委托技术来消除代码重复。\npublic interface Flyable &#123;\n  void fly()；\n&#125;\npublic class FlyAbility implements Flyable &#123;\n  @Override\n  public void fly() &#123; ... &#125;\n&#125;\n//省略Tweetable/TweetAbility/EggLayable/EggLayAbility\npublic class Ostrich implements Tweetable, EggLayable &#123;//鸵鸟\n  private TweetAbility tweetAbility = new TweetAbility(); //组合\n  private EggLayAbility eggLayAbility = new EggLayAbility(); //组合\n  //... 省略其他属性和方法...\n  @Override\n  public void tweet() &#123;\n    tweetAbility.tweet(); // 委托\n  &#125;\n  @Override\n  public void layEgg() &#123;\n    eggLayAbility.layEgg(); // 委托\n  &#125;\n&#125;\n\n使用Java8的接口默认方法\n在Java8中，我们可以在接口中写默认实现方法。使用关键字default定义默认接口实现，当然这个默认的方法也可以重写。\npublic interface Flyable &#123;\n  default void fly() &#123;\n    //默认实现...\n  &#125;\n&#125;\n\npublic interface Flyable &#123;\n  default void fly() &#123;\n    //默认实现...\n  &#125;\n&#125;\n\npublic interface Tweetable &#123;\n  default void tweet() &#123;\n    //默认实现...\n  &#125;\n&#125;\n\npublic interface EggLayable &#123;\n  default void layEgg() &#123;\n    //默认实现...\n  &#125;\n&#125;\n\npublic class Ostrich implements Tweetable, EggLayable &#123;//鸵鸟\n  //... 省略其他属性和方法...\n&#125;\npublic class Sparrow implements Flayable, Tweetable, EggLayable &#123;//麻雀\n  //... 省略其他属性和方法...\n&#125;\n\n继承主要有三个作用：表示is-a关系、支持多态特性、代码复用。\n而这三个作用都可以通过其他技术手段来达成，比如：\n\nis-a关系，我们可以通过组合和接口的has-a关系来替代\n多态特性我们也可以利用接口来实现\n代码复用我们可以通过组合和委托来实现。\n\n从理论上讲，通过组合、接口、委托三个技术手段，我们完全可以替换掉继承，在项目中不用或者少用继承关系，特别是一些复杂的继承关系。\n如何判断该用组合还是继承\n尽管鼓励多用组合少用继承，但组合也并不是完美的，继承也并非一无是处。从上面的例子来看，继承改写成组合意味着要做更细粒度的类的拆分。这也就意味着，我们要定义更多的类和接口。类和接口的增多也就或多或少地增加代码的复杂程度和维护成本。\n如果类之间的继承结构稳定（不会轻易改变），继承层次比较浅（比如，最多有两层继承关系），继承关系不复杂，我们就可以大胆地使用继承。反之，系统越不稳定，继承层次很深，继承关系复杂，我们就尽量使用组合来替代继承。\n文章来源：「后端」Java 设计原则中，为什么反复强调组合要优先于继承？-今日头条 (toutiao.com)\n","slug":"Java设计原则中，组合优先于继承","date":"2023-06-03T13:35:14.000Z","categories_index":"","tags_index":"设计模式","author_index":"大宝贝的程序员"},{"id":"b6845422ba4775a7cab0c414606e3ecd","title":"Redis基础","content":"认识NoSQLNoSql可以翻译做Not Only Sql（不仅仅是SQL），或者是No Sql（非Sql的）数据库。是相对于传统关系型数据库而言，有很大差异的一种特殊的数据库，因此也称之为非关系型数据库。\n结构化与非结构化​\t\t传统关系型数据库是结构化数据，每一张表都有严格的约束信息：字段名.字段数据类型.字段约束等等信息，插入的数据必须遵守这些约束\n​\t\tNoSql则对数据库格式没有严格约束，往往形式松散，自由。\n关联和非关联​\t\t传统数据库的表与表之间往往存在关联，例如外键\n​\t\t非关系型数据库不存在关联关系，要维护关系要么靠代码中的业务逻辑，要么靠数据之间的耦合：\n查询方式​\t\t传统关系型数据库会基于Sql语句做查询，语法有统一标准；\n​\t\t而不同的非关系数据库查询语法差异极大，五花八门各种各样。\n事务​\t\t传统关系型数据库能满足事务ACID的原则。\n​\t\t非关系型数据库往往不支持事务，或者不能严格保证ACID的特性，只能实现基本的一致性。\n在存储方式.扩展性.查询性能上关系型与非关系型也都有着显著差异\n存储方式\n关系型数据库基于磁盘进行存储，会有大量的磁盘IO，对性能有一定影响\n非关系型数据库，他们的操作更多的是依赖于内存来操作，内存的读写速度会非常快，性能自然会好一些\n\n\n\n\n扩展性\n关系型数据库集群模式一般是主从，主从数据一致，起到数据备份的作用，称为垂直扩展。\n非关系型数据库可以将数据拆分，存储在不同机器上，可以保存海量数据，解决内存大小有限的问题。称为水平扩展。\n关系型数据库因为表之间存在关联关系，如果做水平扩展会给数据查询带来很多麻烦\n\n\n\n认识Redis特征：\n\n键值（key-value）型，value支持多种不同数据结构，功能丰富\n单线程，每个命令具备原子性\n低延迟，速度快（基于内存.IO多路复用.良好的编码）。\n支持数据持久化\n支持主从集群.分片集群\n支持多语言客户端\n\n安装RedisLinux版本为CentOS 7.\nRedis是基于C语言编写的，因此首先需要安装Redis所需要的gcc依赖：yum install -y gcc tcl\n上传安装包到&#x2F;usr&#x2F;local&#x2F;src 目录并解压：tar -xzf redis-6.2.6.tar.gz\n解压后进入redis目录：cd redis-6.2.6\n运行编译命令：make &amp;&amp; make install\n默认的安装路径是在 /usr/local/bin目录下\n该目录已经默认配置到环境变量，因此可以在任意目录下运行这些命令。其中：\n\nredis-cli：是redis提供的命令行客户端\nredis-server：是redis的服务端启动脚本\nredis-sentinel：是redis的哨兵启动脚本\n\n启动默认启动：redis-server 这种启动属于前台启动，会阻塞整个会话窗口，窗口关闭或者按下CTRL + C则Redis停止。不推荐使用。\n指定配置启动\n让Redis以后台方式启动，则必须修改Redis配置文件\n先将这个配置文件备份一份：cp redis.conf redis.conf.bck\n修改redis.conf文件中的一些配置：\n# 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0\nbind 0.0.0.0\n# 守护进程，修改为yes后即可后台运行\ndaemonize yes \n# 密码，设置后访问Redis必须输入密码\nrequirepass 123321\n\nRedis的其它常见配置：\n# 监听的端口\nport 6379\n# 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志.持久化等文件会保存在这个目录\ndir .\n# 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15\ndatabases 1\n# 设置redis能够使用的最大内存\nmaxmemory 512mb\n# 日志文件，默认为空，不记录日志，可以指定日志文件名\nlogfile \"redis.log\"\n\n启动Redis：\n# 进入redis安装目录 \ncd /usr/local/src/redis-6.2.6\n# 启动\nredis-server redis.conf\n\n停止服务：\n# 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务，\n# 因为之前配置了密码，因此需要通过 -u 来指定密码\nredis-cli -u 123321 shutdown\n\n开机自启我们也可以通过配置来实现开机自启。\n首先，新建一个系统服务文件：\nvi /etc/systemd/system/redis.service\n\n内容如下：\n[Unit]\nDescription&#x3D;redis-server\nAfter&#x3D;network.target\n\n[Service]\nType&#x3D;forking\nExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;redis-server &#x2F;usr&#x2F;local&#x2F;src&#x2F;redis-6.2.6&#x2F;redis.conf\nPrivateTmp&#x3D;true\n\n[Install]\nWantedBy&#x3D;multi-user.target\n\n然后重载系统服务：\nsystemctl daemon-reload\n\n现在可以用下面这组命令来操作redis了：\n# 启动\nsystemctl start redis\n# 停止\nsystemctl stop redis\n# 重启\nsystemctl restart redis\n# 查看状态\nsystemctl status redis\n\n执行下面的命令，可以让redis开机自启：\nsystemctl enable redis\n\nRedis常见命令Redis数据结构介绍Redis是一个key-value的数据库，key一般是String类型，不过value的类型多种多样：\n\n可以通过Help命令来帮助我们去查看命令\n4.2 Redis 通用命令通用指令是部分数据类型的，都可以使用的指令，常见的有：\n\nKEYS：查看符合模板的所有key\nDEL：删除一个指定的key\nEXISTS：判断key是否存在\nEXPIRE：给一个key设置有效期，有效期到期时该key会被自动删除\nTTL：查看一个KEY的剩余有效期\n\n通过help [command] 查看一个命令的具体用法\n\n不推荐使用keys 命令，因为这个命令在key过多的情况下，效率不高\n\nDEL、KEYS\n\n127.0.0.1:6379> help del\n\n  DEL key [key ...]\n  summary: Delete a key\n  since: 1.0.0\n  group: generic\n\n127.0.0.1:6379> del name #删除单个\n(integer) 1  #成功删除1个\n\n127.0.0.1:6379> MSET k1 v1 k2 v2 k3 v3 #批量添加数据\nOK\n\n127.0.0.1:6379> keys *\n1) \"k3\"\n2) \"k2\"\n3) \"k1\"\n4) \"age\"\n\n127.0.0.1:6379> del k1 k2 k3 k4\n(integer) 3   #此处返回的是成功删除的key，由于redis中只有k1,k2,k3 所以只成功删除3个，最终返回\n127.0.0.1:6379>\n\n\nEXISTS\n\n127.0.0.1:6379> help EXISTS\n\n  EXISTS key [key ...]\n  summary: Determine if a key exists\n  since: 1.0.0\n  group: generic\n\n127.0.0.1:6379> exists age\n(integer) 1\n\n127.0.0.1:6379> exists name\n(integer) 0\n\n\nEXPIRE\nTTL\n\n内存非常宝贵，对于一些数据，我们应当给他一些过期时间，当过期时间到了之后，他就会自动被删除~\n127.0.0.1:6379> expire age 10\n(integer) 1\n\n127.0.0.1:6379> ttl age\n(integer) 8\n\n127.0.0.1:6379> ttl age\n(integer) 6\n\n127.0.0.1:6379> ttl age\n(integer) -2\n\n127.0.0.1:6379> ttl age\n(integer) -2  #当这个key过期了，那么此时查询出来就是-2 \n\n127.0.0.1:6379> keys *\n(empty list or set)\n\n127.0.0.1:6379> set age 10 #如果没有设置过期时间\nOK\n\n127.0.0.1:6379> ttl age\n(integer) -1  # ttl的返回值就是-1\n\nString命令String类型，也就是字符串类型，是Redis中最简单的存储类型。\n其value是字符串，不过根据字符串的格式不同，又可以分为3类：\n\nstring：普通字符串\nint：整数类型，可以做自增.自减操作\nfloat：浮点类型，可以做自增.自减操作\n\nString的常见命令有：\n\nSET：添加或者修改已经存在的一个String类型的键值对\n\nGET：根据key获取String类型的value\n\nMSET：批量添加多个String类型的键值对\n\nMGET：根据多个key获取多个String类型的value\n\nINCR：让一个整型的key自增1\n\nINCRBY:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2\n\nINCRBYFLOAT：让一个浮点类型的数字自增并指定步长\n\nSETNX：添加一个String类型的键值对，前提是这个key不存在，否则不执行\n\nSETEX：添加一个String类型的键值对，并且指定有效期\n\nSETNX  常用作分布式锁\n\n\n127.0.0.1:6379> help setnx\n\n  SETNX key value\n  summary: Set the value of a key, only if the key does not exist\n  since: 1.0.0\n  group: string\n\n127.0.0.1:6379> set name Jack  //设置名称\nOK\n127.0.0.1:6379> setnx name lisi //如果key不存在，则添加成功\n(integer) 0\n127.0.0.1:6379> get name //由于name已经存在，所以lisi的操作失败\n\"Jack\"\n127.0.0.1:6379> setnx name2 lisi //name2 不存在，所以操作成功\n(integer) 1\n127.0.0.1:6379> get name2 \n\"lisi\"\n\n\nSETEX\n\n127.0.0.1:6379> setex name 10 jack\nOK\n127.0.0.1:6379> ttl name\n(integer) 8\n\nRedis命令-Key的层级结构我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范：\nRedis的key允许有多个单词形成层级结构，多个单词之间用’ : ‘ 隔开，格式如下：\n这个格式并非固定，也可以根据自己的需求来删除或添加词条。\n如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储：\n\n\n\nKEY\nVALUE\n\n\n\nheima:user:1\n{“id”:1, “name”: “Jack”, “age”: 21}\n\n\nheima:product:1\n{“id”:1, “name”: “小米11”, “price”: 4999}\n\n\nHash命令Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构。\nString结构是将对象序列化为JSON字符串后存储，当需要修改对象某个字段时很不方便：\n\nHash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD：\n\nHash类型的常见命令\n\nHSET key field value：添加或者修改hash类型key的field的值\n\nHGET key field：获取一个hash类型key的field的值\n\nHMSET：批量添加多个hash类型key的field的值\n\nHMGET：批量获取多个hash类型key的field的值\n\nHGETALL：获取一个hash类型的key中的所有的field和value\n\nHKEYS：获取一个hash类型的key中的所有的field\n\nHINCRBY:让一个hash类型key的字段值自增并指定步长\n\nHSETNX：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行\n\n\n贴心小提示：哈希结构也是我们以后实际开发中常用的命令哟\n\nHSET和HGET\n\n127.0.0.1:6379> HSET heima:user:3 name Lucy//大key是 heima:user:3 小key是name，小value是Lucy\n(integer) 1\n127.0.0.1:6379> HSET heima:user:3 age 21// 如果操作不存在的数据，则是新增\n(integer) 1\n127.0.0.1:6379> HSET heima:user:3 age 17 //如果操作存在的数据，则是修改\n(integer) 0\n127.0.0.1:6379> HGET heima:user:3 name \n\"Lucy\"\n127.0.0.1:6379> HGET heima:user:3 age\n\"17\"\n\n\nHMSET和HMGET\n\n127.0.0.1:6379> HMSET heima:user:4 name HanMeiMei\nOK\n127.0.0.1:6379> HMSET heima:user:4 name LiLei age 20 sex man\nOK\n127.0.0.1:6379> HMGET heima:user:4 name age sex\n1) \"LiLei\"\n2) \"20\"\n3) \"man\"\n\n\nHGETALL\n\n127.0.0.1:6379> HGETALL heima:user:4\n1) \"name\"\n2) \"LiLei\"\n3) \"age\"\n4) \"20\"\n5) \"sex\"\n6) \"man\"\n\n\nHKEYS和HVALS\n\n127.0.0.1:6379> HKEYS heima:user:4\n1) \"name\"\n2) \"age\"\n3) \"sex\"\n127.0.0.1:6379> HVALS heima:user:4\n1) \"LiLei\"\n2) \"20\"\n3) \"man\"\n\n\nHINCRBY\n\n127.0.0.1:6379> HINCRBY  heima:user:4 age 2\n(integer) 22\n127.0.0.1:6379> HVALS heima:user:4\n1) \"LiLei\"\n2) \"22\"\n3) \"man\"\n127.0.0.1:6379> HINCRBY  heima:user:4 age -2\n(integer) 20\n\n\nHSETNX\n\n127.0.0.1:6379> HGETALL heima:user:3\n1) \"name\"\n2) \"Lucy\"\n3) \"age\"\n4) \"17\"\n127.0.0.1:6379> HSETNX heima:user:3 sex woman\n(integer) 1\n127.0.0.1:6379> HGETALL heima:user:3\n1) \"name\"\n2) \"Lucy\"\n3) \"age\"\n4) \"17\"\n5) \"sex\"\n6) \"woman\"\n\nList命令Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。\n特征也与LinkedList类似：\n\n有序\n元素可以重复\n插入和删除快\n查询速度一般\n\n常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。\nList的常见命令有：\n\nLPUSH key element … ：向列表左侧插入一个或多个元素\nLPOP key：移除并返回列表左侧的第一个元素，没有则返回nil\nRPUSH key element … ：向列表右侧插入一个或多个元素\nRPOP key：移除并返回列表右侧的第一个元素\nLRANGE key star end：返回一段角标范围内的所有元素\nBLPOP和BRPOP：与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil\n\n\n\nLPUSH和RPUSH\n\n127.0.0.1:6379> LPUSH users 1 2 3\n(integer) 3\n127.0.0.1:6379> RPUSH users 4 5 6\n(integer) 6\n\n\nLPOP和RPOP\n\n127.0.0.1:6379> LPOP users\n\"3\"\n127.0.0.1:6379> RPOP users\n\"6\"\n\n\nLRANGE\n\n127.0.0.1:6379> LRANGE users 1 2\n1) \"1\"\n2) \"4\"\n\nSet命令Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征：\n\n无序\n元素不可重复\n查找快\n支持交集.并集.差集等功能\n\nSet类型的常见命令\n\nSADD key member … ：向set中添加一个或多个元素\nSREM key member … : 移除set中的指定元素\nSCARD key： 返回set中元素的个数\nSISMEMBER key member：判断一个元素是否存在于set中\nSMEMBERS：获取set中的所有元素\nSINTER key1 key2 … ：求key1与key2的交集\nSDIFF key1 key2 … ：求key1与key2的差集\nSUNION key1 key2 ..：求key1和key2的并集\n\n例如两个集合：s1和s2:\n\n求交集：SINTER s1 s2\n求s1与s2的不同：SDIFF s1 s2\n\n具体命令\n127.0.0.1:6379> sadd s1 a b c\n(integer) 3\n127.0.0.1:6379> smembers s1\n1) \"c\"\n2) \"b\"\n3) \"a\"\n127.0.0.1:6379> srem s1 a\n(integer) 1\n    \n127.0.0.1:6379> SISMEMBER s1 a\n(integer) 0\n    \n127.0.0.1:6379> SISMEMBER s1 b\n(integer) 1\n    \n127.0.0.1:6379> SCARD s1\n(integer) 2\n\n案例\n将下列数据用Redis的Set集合来存储：\n\n张三的好友有：李四.王五.赵六\n李四的好友有：王五.麻子.二狗\n\n利用Set的命令实现下列功能：\n\n计算张三的好友有几人\n计算张三和李四有哪些共同好友\n查询哪些人是张三的好友却不是李四的好友\n查询张三和李四的好友总共有哪些人\n判断李四是否是张三的好友\n判断张三是否是李四的好友\n将李四从张三的好友列表中移除\n\n127.0.0.1:6379> SADD zs lisi wangwu zhaoliu\n(integer) 3\n    \n127.0.0.1:6379> SADD ls wangwu mazi ergou\n(integer) 3\n    \n127.0.0.1:6379> SCARD zs\n(integer) 3\n    \n127.0.0.1:6379> SINTER zs ls\n1) \"wangwu\"\n    \n127.0.0.1:6379> SDIFF zs ls\n1) \"zhaoliu\"\n2) \"lisi\"\n    \n127.0.0.1:6379> SUNION zs ls\n1) \"wangwu\"\n2) \"zhaoliu\"\n3) \"lisi\"\n4) \"mazi\"\n5) \"ergou\"\n    \n127.0.0.1:6379> SISMEMBER zs lisi\n(integer) 1\n    \n127.0.0.1:6379> SISMEMBER ls zhangsan\n(integer) 0\n    \n127.0.0.1:6379> SREM zs lisi\n(integer) 1\n    \n127.0.0.1:6379> SMEMBERS zs\n1) \"zhaoliu\"\n2) \"wangwu\"\n\nSortedSet类型Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。\nSortedSet具备下列特性：\n\n可排序\n元素不重复\n查询速度快\n\n因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。\nSortedSet的常见命令有：\n\nZADD key score member：添加一个或多个元素到sorted set ，如果已经存在则更新其score值\nZREM key member：删除sorted set中的一个指定元素\nZSCORE key member : 获取sorted set中的指定元素的score值\nZRANK key member：获取sorted set 中的指定元素的排名\nZCARD key：获取sorted set中的元素个数\nZCOUNT key min max：统计score值在给定范围内的所有元素的个数\nZINCRBY key increment member：让sorted set中的指定元素自增，步长为指定的increment值\nZRANGE key min max：按照score排序后，获取指定排名范围内的元素\nZRANGEBYSCORE key min max：按照score排序后，获取指定score范围内的元素\nZDIFF.ZINTER.ZUNION：求差集.交集.并集\n\n注意：所有的排名默认都是升序，如果要降序则在命令的Z后面添加REV即可，例如：\n\n升序获取sorted set 中的指定元素的排名：ZRANK key member\n降序获取sorted set 中的指定元素的排名：ZREVRANK key memeber\n\nRedis的Java客户端-Jedis其中Java客户端也包含很多：\n推荐使用的java客户端，包括：\n\nJedis和Lettuce：这两个主要是提供了Redis命令对应的API，方便我们操作Redis，而SpringDataRedis又对这两种做了抽象和封装，因此直接以SpringDataRedis\nRedisson：是在Redis基础上实现了分布式的可伸缩的java数据结构，例如Map.Queue等，而且支持跨进程的同步机制：Lock.Semaphore等待，比较适合用来实现特殊的功能需求。\n\nJedis快速入门入门案例详细步骤\n案例分析：\n1）引入依赖：\n&lt;!--jedis-->\n&lt;dependency>\n    &lt;groupId>redis.clients&lt;/groupId>\n    &lt;artifactId>jedis&lt;/artifactId>\n    &lt;version>3.7.0&lt;/version>\n&lt;/dependency>\n&lt;!--单元测试-->\n&lt;dependency>\n    &lt;groupId>org.junit.jupiter&lt;/groupId>\n    &lt;artifactId>junit-jupiter&lt;/artifactId>\n    &lt;version>5.7.0&lt;/version>\n    &lt;scope>test&lt;/scope>\n&lt;/dependency>\n\n2）建立连接\n新建一个单元测试类，内容如下：\nprivate Jedis jedis;\n\n@BeforeEach\nvoid setUp() &#123;\n    // 1.建立连接\n    jedis = new Jedis(\"192.168.150.101\", 6379);\n    //1.1 连接池的方式建立连接\n    //jedis = JedisConnectionFactory.getJedis();\n    // 2.设置密码\n    jedis.auth(\"123321\");\n    // 3.选择库\n    jedis.select(0);\n&#125;\n\n3）测试：\n@Test\nvoid testString() &#123;\n    // 存入数据\n    String result = jedis.set(\"name\", \"虎哥\");\n    System.out.println(\"result = \" + result);\n    // 获取数据\n    String name = jedis.get(\"name\");\n    System.out.println(\"name = \" + name);\n&#125;\n\n@Test\nvoid testHash() &#123;\n    // 插入hash数据\n    jedis.hset(\"user:1\", \"name\", \"Jack\");\n    jedis.hset(\"user:1\", \"age\", \"21\");\n\n    // 获取\n    Map&lt;String, String> map = jedis.hgetAll(\"user:1\");\n    System.out.println(map);\n&#125;\n\n4）释放资源\n@AfterEach\nvoid tearDown() &#123;\n    if (jedis != null) &#123;\n        jedis.close();\n    &#125;\n&#125;\n\nJedis连接池Jedis本身是线程不安全的，并且频繁的创建和销毁连接会有性能损耗，因此我们推荐使用Jedis连接池代替Jedis的直连方式\n有关池化思想，并不仅仅是这里会使用，很多地方都有，比如说我们的数据库连接池，比如我们tomcat中的线程池，这些都是池化思想的体现。\n创建Jedis的连接池public class JedisConnectionFacotry &#123;\n\n     private static final JedisPool jedisPool;\n\n     static &#123;\n         //配置连接池\n         JedisPoolConfig poolConfig = new JedisPoolConfig();\n         poolConfig.setMaxTotal(8);\n         poolConfig.setMaxIdle(8);\n         poolConfig.setMinIdle(0);\n         poolConfig.setMaxWaitMillis(1000);\n         //创建连接池对象\n         jedisPool = new JedisPool(poolConfig,\n                 \"192.168.150.101\",6379,1000,\"123321\");\n     &#125;\n\n     public static Jedis getJedis()&#123;\n          return jedisPool.getResource();\n     &#125;\n&#125;\n\n代码说明：\n\n1） JedisConnectionFacotry：工厂设计模式是实际开发中非常常用的一种设计模式，我们可以使用工厂，去降低代的耦合，比如Spring中的Bean的创建，就用到了工厂设计模式\n\n2）静态代码块：随着类的加载而加载，确保只能执行一次，我们在加载当前工厂类的时候，就可以执行static的操作完成对 连接池的初始化\n\n3）最后提供返回连接池中连接的方法.\n\n\n改造原始代码代码说明:\n\n在我们完成了使用工厂设计模式来完成代码的编写之后，我们在获得连接时，就可以通过工厂来获得，而不用直接去new对象，降低耦合，并且使用的还是连接池对象。\n\n当我们使用了连接池后，当我们关闭连接其实并不是关闭，而是将Jedis还回连接池的。\n\n\n @BeforeEach\n void setUp()&#123;\n     //建立连接\n     /*jedis = new Jedis(\"127.0.0.1\",6379);*/\n     jedis = JedisConnectionFacotry.getJedis();\n      //选择库\n     jedis.select(0);\n &#125;\n\n@AfterEach\n void tearDown() &#123;\n     if (jedis != null) &#123;\n         jedis.close();\n     &#125;\n &#125;\n\nSpringDataRedisSpringData是Spring中数据操作的模块，包含对各种数据库的集成，其中对Redis的集成模块就叫做SpringDataRedis，官网地址：https://spring.io/projects/spring-data-redis\n\n提供了对不同Redis客户端的整合（Lettuce和Jedis）\n提供了RedisTemplate统一API来操作Redis\n支持Redis的发布订阅模型\n支持Redis哨兵和Redis集群\n支持基于Lettuce的响应式编程\n支持基于JDK.JSON.字符串.Spring对象的数据序列化及反序列化\n支持基于Redis的JDKCollection实现\n\nSpringDataRedis中提供了RedisTemplate工具类，其中封装了各种对Redis的操作。并且将不同数据类型的操作API封装到了不同的类型中：\n\n快速入门SpringBoot已经提供了对SpringDataRedis的支持，使用非常简单：\n\n引入spring-boot-starter-data-redis依赖\n在application.yml配置Redis信息\n注入RedisTemplate\n\n导入pom坐标&lt;dependencies>\n    &lt;!--redis依赖-->\n    &lt;dependency>\n        &lt;groupId>org.springframework.boot&lt;/groupId>\n        &lt;artifactId>spring-boot-starter-data-redis&lt;/artifactId>\n    &lt;/dependency>\n    &lt;!--common-pool-->\n    &lt;dependency>\n        &lt;groupId>org.apache.commons&lt;/groupId>\n        &lt;artifactId>commons-pool2&lt;/artifactId>\n    &lt;/dependency>\n    &lt;!--Jackson依赖-->\n    &lt;dependency>\n        &lt;groupId>com.fasterxml.jackson.core&lt;/groupId>\n        &lt;artifactId>jackson-databind&lt;/artifactId>\n    &lt;/dependency>\n&lt;/dependencies>\n\n配置文件spring:\n  redis:\n    host: 192.168.150.101\n    port: 6379\n    password: 123321\n    lettuce:\n      pool:\n        max-active: 8  #最大连接\n        max-idle: 8   #最大空闲连接\n        min-idle: 0   #最小空闲连接\n        max-wait: 100ms #连接等待时间\n\n测试代码@SpringBootTest\nclass RedisDemoApplicationTests &#123;\n\n    @Autowired\n    private RedisTemplate&lt;String, Object> redisTemplate;\n\n    @Test\n    void testString() &#123;\n        // 写入一条String数据\n        redisTemplate.opsForValue().set(\"name\", \"虎哥\");\n        // 获取string数据\n        Object name = redisTemplate.opsForValue().get(\"name\");\n        System.out.println(\"name = \" + name);\n    &#125;\n&#125;\n\n数据序列化器RedisTemplate可以接收任意Object作为值写入Redis\n只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化\nJDK序列化缺点：\n\n可读性差\n内存占用较大\n\n我们可以自定义RedisTemplate的序列化方式，代码如下：\n@Configuration\npublic class RedisConfig &#123;\n\n    @Bean\n    public RedisTemplate&lt;String, Object> redisTemplate(RedisConnectionFactory connectionFactory)&#123;\n        // 创建RedisTemplate对象\n        RedisTemplate&lt;String, Object> template = new RedisTemplate&lt;>();\n        // 设置连接工厂\n        template.setConnectionFactory(connectionFactory);\n        // 创建JSON序列化工具\n        GenericJackson2JsonRedisSerializer jsonRedisSerializer = \n            \t\t\t\t\t\t\tnew GenericJackson2JsonRedisSerializer();\n        // 设置Key的序列化\n        template.setKeySerializer(RedisSerializer.string());\n        template.setHashKeySerializer(RedisSerializer.string());\n        // 设置Value的序列化\n        template.setValueSerializer(jsonRedisSerializer);\n        template.setHashValueSerializer(jsonRedisSerializer);\n        // 返回\n        return template;\n    &#125;\n&#125;\n\n尽管JSON的序列化方式可以满足我们的需求，但依然存在一些问题，如图：\n\n这里采用了JSON序列化来代替默认的JDK序列化方式。整体可读性有了很大提升，并且能将Java对象自动的序列化为JSON字符串，并且查询时能自动把JSON反序列化为Java对象。不过，为了在反序列化时知道对象的类型，JSON序列化器会将类的class类型写入json结果中，存入Redis，会带来额外的内存开销。\n为了减少内存的消耗，我们可以采用手动序列化的方式，换句话说，就是不借助默认的序列化器，而是我们自己来控制序列化的动作，同时，我们只采用String的序列化器，这样，在存储value时，我们就不需要在内存中就不用多存储数据，从而节约我们的内存空间\n\n这种用法比较普遍，因此SpringDataRedis就提供了RedisTemplate的子类：StringRedisTemplate，它的key和value的序列化方式默认就是String方式。\n省去了我们自定义RedisTemplate的序列化方式的步骤，而是直接使用：\n@SpringBootTest\nclass RedisStringTests &#123;\n\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n    @Test\n    void testString() &#123;\n        // 写入一条String数据\n        stringRedisTemplate.opsForValue().set(\"verify:phone:13600527634\", \"124143\");\n        // 获取string数据\n        Object name = stringRedisTemplate.opsForValue().get(\"name\");\n        System.out.println(\"name = \" + name);\n    &#125;\n\n    private static final ObjectMapper mapper = new ObjectMapper();\n\n    @Test\n    void testSaveUser() throws JsonProcessingException &#123;\n        // 创建对象\n        User user = new User(\"虎哥\", 21);\n        // 手动序列化\n        String json = mapper.writeValueAsString(user);\n        // 写入数据\n        stringRedisTemplate.opsForValue().set(\"user:200\", json);\n\n        // 获取数据\n        String jsonUser = stringRedisTemplate.opsForValue().get(\"user:200\");\n        // 手动反序列化\n        User user1 = mapper.readValue(jsonUser, User.class);\n        System.out.println(\"user1 = \" + user1);\n    &#125;\n\n&#125;\n\n那个class数据已经不在了，节约了空间\n\n小结：\nRedisTemplate的两种序列化实践方案：\n\n方案一：\n\n自定义RedisTemplate\n修改RedisTemplate的序列化器为GenericJackson2JsonRedisSerializer\n\n\n方案二：\n\n使用StringRedisTemplate\n写入Redis时，手动把对象序列化为JSON\n读取Redis时，手动把读取到的JSON反序列化为对象\n\n\n\nHash结构操作@SpringBootTest\nclass RedisStringTests &#123;\n\n    @Autowired\n    private StringRedisTemplate stringRedisTemplate;\n\n\n    @Test\n    void testHash() &#123;\n        stringRedisTemplate.opsForHash().put(\"user:400\", \"name\", \"虎哥\");\n        stringRedisTemplate.opsForHash().put(\"user:400\", \"age\", \"21\");\n\n        Map&lt;Object, Object> entries = stringRedisTemplate.opsForHash().entries(\"user:400\");\n        System.out.println(\"entries = \" + entries);\n    &#125;\n&#125;\n\n","slug":"Redis基础","date":"2023-06-02T14:33:03.000Z","categories_index":"","tags_index":"redis","author_index":"大宝贝的程序员"},{"id":"4f10da7078fbc31ecbcb9fd541bf0d6b","title":"MDC实现链路追踪","content":"MDC实现链路追踪先明确Filter、HandlerInterceptor 和 AOP 的执行流程在 Spring Boot 应用程序中，Filter、HandlerInterceptor 和 AOP 切面都是拦截器组件，在请求处理过程中扮演不同的角色，各自负责不同的任务。在一个请求处理中，这三者的执行顺序如下：\n\n首先，请求到达 Web 服务器后，将会首先执行 Filter 组件。如果存在多个 Filter，这些 Filter 将会依次进行拦截，执行顺序按照在 web.xml 或者 WebApplicationInitializer 配置的顺序进行。注意，Filter 的执行跟 URL 的 mappings 有关系，只有请求路径匹配了 mapping 的URL，才会执行过滤器中的逻辑。\n\n紧接着，请求会经过 Spring MVC 中的 DispatcherServlet，DispatcherServlet 会根据请求信息找到对应的 HandlerMapping，确定将要执行哪个 Controller 中的处理方法。\n\n如果存在 HandlerInterceptor，那么它们就会在请求到达 Controller 方法之前，先进行一些处理。如果存在多个 HandlerInterceptor，它们的执行顺序按照配置顺序执行，即 @Order 注解的顺序为order的值越小越优先执行\n   Interceptor1: preHandle()\n   Interceptor2: preHandle()\n   Interceptor3: preHandle()\n   \n   Interceptor3: postHandle()\n   Interceptor2: postHandle()\n   Interceptor1: postHandle()\n   \n   Interceptor3: afterCompletion()\n   Interceptor2: afterCompletion()\n   Interceptor1: afterCompletion()\n\n4. Controller 方法处理请求，并返回结果。这个过程中，如果使用了 AOP 切面来增强方法，AOP 切面会在 Controller 方法执行之前或之后执行，对方法的调用进行增强和切入。\n\n5. 如果存在 HandlerInterceptor，那么它们会在 Controller 方法返回结果之后，发生 \"postHandle\"。此时，返回值还没有真正的传递到前端展示，HandlerInterceptor 可以对 ModelAndView 进行再次操作。\n\n6. 最后，HandlerInterceptor 会完成请求的所有操作，并执行 afterCompletion 方法。\n\n总体来说，Filter 通常用于处理跨域、安全、日志记录等，HandlerInterceptor 主要用于处理权限控制、日志记录、请求转发等，而 AOP 切面则主要用于对方法调用进行增强、切入和拦截。在一个请求处理中，Filter 通常是最先执行的，然后是 HandlerInterceptor，最后是 AOP 切面。因此，这三者的执行顺序具体根据系统的配置的顺序、URL 路径、Filter 配置等因素而定，需要根据具体情况进行分析和调整。\n\n#### 整合logback日志实现链路追踪\n\n**xml配置**\n\nspringboot自带logback的依赖，不需要导入任何包。我们只需要在项目路径下创建文件`logback.xml`\n\n````xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?>\n&lt;configuration>\n  &lt;!--自定义日志相关的变量 -->\n  &lt;property name=\"LOG_PATH\" value=\"./data/logs\"/>\n  &lt;property name=\"LOG_FILE\" value=\"Request_Log\"/>\n  &lt;!-- 格式化\n\t%level：日志级别，例如DEBUG、INFO、WARN、ERROR等。\n\t%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;：时间，格式为年-月-日 时:分:秒.毫秒。\n\t%thread：线程名。\n\t%X&#123;&#125;：日志格式中的一个占位符\n\t\t%X&#123;tid&#125;：线程ID。\n\t\t%X&#123;uid&#125;：用户ID。\n\t%msg：日志消息。\n\t%n：换行符。\n\t-->\n  &lt;property name=\"CONSOLE_LOG_PATTERN\"\n    value=\"|%level|%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;|%thread|%X&#123;tid&#125;|uid=%X&#123;uid&#125;|%msg|%n\"/>\n\n  &lt;include resource=\"org/springframework/boot/logging/logback/defaults.xml\"/>\n  &lt;include resource=\"org/springframework/boot/logging/logback/console-appender.xml\"/>\n\n  &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n    &lt;encoder >\n      &lt;pattern>$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern>\n    &lt;/encoder>\n  &lt;/appender>\n  &lt;!-- 全部日志的配置\n\t日志文件会输出到$&#123;LOG_PATH&#125;/$&#123;LOG_FILE&#125;.log，也就是日志文件路径加上文件名。日志格式使用了 $&#123;CONSOLE_LOG_PATTERN&#125; 常量，即控制台日志的格式。日志文件滚动按照天数进行，每天生成一个日志文件，保留最近 30 天的日志文件并限制单个文件大小在 5GB 以内。生成的日志文件名按照 $&#123;LOG_FILE&#125;.%d&#123;dd-MM-yyyy&#125;.log 的格式进行命名，并在archived文件夹下进行归档 -->\n  &lt;appender name=\"fileAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n    &lt;file>$&#123;LOG_PATH&#125;/$&#123;LOG_FILE&#125;.log&lt;/file>\n    &lt;append>true&lt;/append>\n    &lt;encoder >\n      &lt;pattern>$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern>\n    &lt;/encoder>\n    &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n      &lt;!-- 按天生成日志文件 -->\n      &lt;fileNamePattern>\n        $&#123;LOG_PATH&#125;/archived/$&#123;LOG_FILE&#125;.%d&#123;dd-MM-yyyy&#125;.log\n      &lt;/fileNamePattern>\n      &lt;!--保留天数-->\n      &lt;maxHistory>30&lt;/maxHistory>\n      &lt;!--单个文件的大小-->\n      &lt;totalSizeCap>5GB&lt;/totalSizeCap>\n    &lt;/rollingPolicy>\n\n  &lt;/appender>\n  &lt;!-- error日志的配置-->\n  &lt;appender name=\"fileError\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n      &lt;!-- 仅输出指定日志级别的日志。在这里是ERROR级别 -->\n    &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\">\n      &lt;level>ERROR&lt;/level>\n      &lt;onMatch>ACCEPT&lt;/onMatch>\n      &lt;onMismatch>DENY&lt;/onMismatch>\n    &lt;/filter>\n      &lt;!--日志文件的生成路径和文件名 -->\n    &lt;file>$&#123;LOG_PATH&#125;/$&#123;LOG_FILE&#125;.error.log&lt;/file>\n    &lt;append>true&lt;/append>\n    &lt;encoder >\n      &lt;pattern>$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern>\n    &lt;/encoder>\n    &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n      &lt;fileNamePattern>\n        $&#123;LOG_PATH&#125;/archived/$&#123;LOG_FILE&#125;.%d&#123;dd-MM-yyyy&#125;.error.log\n      &lt;/fileNamePattern>\n      &lt;maxHistory>30&lt;/maxHistory>\n      &lt;totalSizeCap>2GB&lt;/totalSizeCap>\n    &lt;/rollingPolicy>\n  &lt;/appender>\n\n  &lt;root level=\"info\">\n    &lt;!--文件输出-->\n    &lt;appender-ref ref=\"fileAppender\"/>\n    &lt;appender-ref ref=\"fileError\"/>\n    &lt;!--制台输出-->\n    &lt;appender-ref ref=\"STDOUT\"/>\n  &lt;/root>\n\n&lt;/configuration>\n\n其中有两个自定义参数 tid和uid，需要将他们放入logback的MDC上下文中，这样日志在打印的时候就可以打印出来。\n日志排查\n控制台找到报错的请求的uid 执行下面的语句\ngrep \"7eb0c96e-9296-4eec-8a9b-1d9ffc09949b\" Request_Log.log \n\n传入&#x2F;移除自定义参数\n根据执行顺序：在filter 设置上每一个请求的标识MDC.put( 唯一标识 )，在handlerInterceptor的最外层记得把信息移除RequestHolder.remove();\n","slug":"MDC实现简单的链路追踪","date":"2023-06-02T12:36:15.000Z","categories_index":"","tags_index":"链路追踪,日志","author_index":"大宝贝的程序员"},{"id":"b491cfca14c8d6dfd4f8108806508bb6","title":"线程池的管理和使用","content":"线程池的管理和使用​\t\t频繁的创建、销毁线程和线程池，会给系统带来额外的开销。未经池化及统一管理的线程，则会导致系统内线程数上限不可控。这种情况下，随着访问数增加，系统内线程数持续增长，CPU负载逐步提高。极端情况下，甚至可能会导致CPU资源被吃满，整个服务不可用。\n解决方案替换掉自建线程和线程池\n自建线程池在ThreadPoolConfig中，创建我们项目统一的线程池，并交给spring管理。\n@Configuration\n@EnableAsync\npublic class ThreadPoolConfig implements AsyncConfigurer &#123;\n    /**\n     * 项目共用线程池\n     */\n    public static final String MY_EXECUTOR = \"Executor\";\n\n    @Override\n    public Executor getAsyncExecutor() &#123;\n        return createExecutor();\n    &#125;\n\n    @Bean(MY_EXECUTOR)\n    @Primary\n    public ThreadPoolTaskExecutor createExecutor() &#123;\n        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n        executor.setCorePoolSize(10);\n        executor.setMaxPoolSize(10);\n        executor.setQueueCapacity(200);\n        executor.setThreadNamePrefix(\"executor-\");\n        executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());//满了调用线程执行，认为重要任务\n        executor.initialize();\n        return executor;\n    &#125;\n&#125;\n\n类 ThreadPoolConfig 实现了 AsyncConfigurer 接口并添加了 @EnableAsync 注解后，我们就可以在 Spring 中使用 @Async 注解来声明异步执行的方法了。由于使用了自定义的线程池，异步任务的执行不会影响应用的主线程，从而提高了系统的可用性和性能。\n线程池的创建没有用Excutors的快速创建。是因为Excutors创建的线程池用的无界队列，有内存溢出（oom）的风险。\n优雅停机当项目关闭的时候，需要通过jvm的shutdownHook回调线程池，等队列里任务执行完再停机。保证任务不丢失。\nspring管理的线程池，连优雅停机的事，都可以直接交给spring自己来管理。\n@Override\npublic void destroy() &#123;\n    shutdown();\n&#125;\n\n/**\n * Perform a shutdown on the underlying ExecutorService.\n * @see java.util.concurrent.ExecutorService#shutdown()\n * @see java.util.concurrent.ExecutorService#shutdownNow()\n */\npublic void shutdown() &#123;\n    if (logger.isDebugEnabled()) &#123;\n        logger.debug(\"Shutting down ExecutorService\" + (this.beanName != null ? \" '\" + this.beanName + \"'\" : \"\"));\n    &#125;\n    if (this.executor != null) &#123;\n        if (this.waitForTasksToCompleteOnShutdown) &#123;\n            this.executor.shutdown();\n        &#125;\n        else &#123;\n            for (Runnable remainingTask : this.executor.shutdownNow()) &#123;\n                cancelRemainingTask(remainingTask);\n            &#125;\n        &#125;\n        awaitTerminationIfNecessary(this.executor);\n    &#125;\n&#125;\n\n线程池使用我们放进容器的线程池设置了beanName。\n\n业务需要用，也可以根据beanName取出想用的线程池。\n\n或者是直接在方法上加上异步注解@async\n\n五、异常捕获线程池，千万别忘了一点，就是线程运行抛异常了要怎么处理。\npublic static void main(String[] args) &#123;\n    Thread thread =new Thread(()->&#123;\n        log.info(\"111\");\n        throw new RuntimeException(\"运行时异常了\");\n    &#125;);\n    thread.start();\n&#125;\n\n结果是这样的，异常并不会打印日志，只会在控制台输出。如果出了问题，却不打印error日志。\n传统模式下，我们一般会通过try catch的方法去捕获线程的异常，并且打印到日志中。\nThread thread =new Thread(()->&#123;\n    try&#123;\n        log.info(\"111\");\n        throw new RuntimeException(\"运行时异常了\");\n    &#125;catch (Exception e)&#123;\n        log.error(\"异常发生\",e);\n    &#125;\n&#125;);\nthread.start();\n\n当我们捕获了异常，就没有控制台的告警了，全都是日志打印\n其实，如果一个异常未被捕获，从线程中抛了出来。JVM会回调一个方法dispatchUncaughtException\n/**\n * Dispatch an uncaught exception to the handler. This method is\n * intended to be called only by the JVM.\n */\nprivate void dispatchUncaughtException(Throwable e) &#123;\n    getUncaughtExceptionHandler().uncaughtException(this, e);\n&#125;\n\n这个方法在Thread类中，会进行默认的异常处理，其实就是获取一个默认的异常处理器。默认的异常处理器是\nThreadGroup实现的异常捕获方法。前面看到的控制台ERR打印，就出自这里。\n我们要做的很简单，就是给线程添加一个异常捕获处理器，以后抛了异常，就给它转成error日志。这样才能及时发现问题。\nThread有两个属性，一个类静态变量，一个实例对象。都可以设置异常捕获。区别在于一个生效的范围是单个thread对象，一个生效的范围是全局的thread。\n// null unless explicitly set\nprivate volatile UncaughtExceptionHandler uncaughtExceptionHandler;\n\n// null unless explicitly set\nprivate static volatile UncaughtExceptionHandler defaultUncaughtExceptionHandler;\n\n我们一般选择给每个thread实例都加一个异常捕获。毕竟别人的thread咱们别管，只管自己创建的thread。\nThread thread = new Thread(() -> &#123;\n    log.info(\"111\");\n    throw new RuntimeException(\"运行时异常了\");\n&#125;);\nThread.UncaughtExceptionHandler uncaughtExceptionHandler =(t,e)->&#123;\n    log.error(\"Exception in thread \",e);\n&#125;;\nthread.setUncaughtExceptionHandler(uncaughtExceptionHandler);\nthread.start();\n\n线程池的异常捕获我们工一般不直接创建对象，都用的线程池\n用线程池的ThreadFactory，创建线程的工厂，创建线程的时候给线程添加异常捕获。\nprivate static ExecutorService executor = new ThreadPoolExecutor(1, 1,\n    0L, TimeUnit.MILLISECONDS,\n    new LinkedBlockingQueue&lt;Runnable>(500), \n    new NamedThreadFactory(\"refresh-ipDetail\",null, false,\n                           new MyUncaughtExceptionHandler()));\n\n自建的线程池，直接在工厂里添加一个异常捕获处理器就好了。它在创建thread的时候，会把这个异常捕获赋值给thread。\n\nSpring的线程池ThreadPoolTaskExecutor由于Spring的封装，想要给线程工厂设置一个捕获器，是很困难的。\n @Bean\npublic ThreadPoolTaskExecutor websocketExecutor() &#123;\n    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n    executor.setCorePoolSize(16);\n    executor.setMaxPoolSize(16);\n    executor.setQueueCapacity(1000);\n    executor.setThreadNamePrefix(\"websocket-executor-\");\n    executor.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardPolicy());//满了直接丢弃，默认为不重要消息推送\n    executor.initialize();\n    return executor;\n&#125;\n\n可以看到创建Spring提供的线程池并没有把异常处理器传进去的方法。\n观察ThreadPoolTaskExecutor的类继承关系图\n\n可以看到它自己实现了ThreadFactory。在CustomizableThreadFactory类的位置\n\n\n跟进去可以看见它内部封装好的创建线程的方法，压根就没有机会去设置一个线程异常捕获器。\n但是，它的抽象类ExecutorConfigurationSupport将自己赋值给线程工厂，提供了一个解耦的机会。\n\n如果我们把这个线程工厂换了，那么它的线程创建方法就会失效。线程名，优先级啥的全都得我们一并做了。而我们只是想扩展一个线程捕获。\n这时候就要使用到装饰器模式 ，装饰器模式不会改变原有的功能，而是在功能前后做一个扩展点。 \n首先先写一个自己的线程工厂，把spring的线程工厂传进来。调用它的线程创建后，再扩展设置我们的异常捕获\npublic class MyThreadFactory implements ThreadFactory &#123;\n\n    private ThreadFactory original;\n\n    @Override\n    public Thread newThread(Runnable r) &#123;\n        Thread thread = original.newThread(r);\n        thread.setUncaughtExceptionHandler(new MyUncaughtExceptionHandler());//异常捕获\n        return thread;\n    &#125;\n&#125;\n\n替换spring线程池的线程工厂就可以了\n\n","slug":"线程池的管理和使用","date":"2023-06-01T06:36:07.000Z","categories_index":"","tags_index":"线程池","author_index":"大宝贝的程序员"},{"id":"34f154820ac35e3d8f3e9afce6bf7431","title":"SpringTask实现定时任务","content":"SpringTask\n\n\n\n\n\n\n\n\nSpringTask是Spring自主研发的轻量级定时任务工具，相比于Quartz更加简单方便，且不需要引入其他依赖即可使用。\nCron表达式\n\n\n\n\n\n\n\n\nCron表达式是一个字符串，包括6~7个时间元素，在SpringTask中可以用于指定任务的执行时间。\nCron的语法格式每个时间元素的说明\n\n\n时间元素\n可出现的字符\n有效数值范围\n\n\n\nSeconds\n, - * &#x2F;\n0-59\n\n\nMinutes\n, - * &#x2F;\n0-59\n\n\nHours\n, - * &#x2F;\n0-23\n\n\nDayofMonth\n, - * &#x2F; ? L W\n0-31\n\n\nMonth\n, - * &#x2F;\n1-12\n\n\nDayofWeek\n, - * &#x2F; ? L #\n1-7或SUN-SAT\n\n\n格式中特殊字符说明\n\n\n字符\n作用\n举例\n\n\n\n,\n列出枚举值\n在Minutes域使用5,10，表示在5分和10分各触发一次\n\n\n-\n表示触发范围\n在Minutes域使用5-10，表示从5分到10分钟每分钟触发一次\n\n\n*\n匹配任意值\n在Minutes域使用*, 表示每分钟都会触发一次\n\n\n&#x2F;\n起始时间开始触发，每隔固定时间触发一次\n在Minutes域使用5&#x2F;10,表示5分时触发一次，每10分钟再触发一次\n\n\n?\n在DayofMonth和DayofWeek中，用于匹配任意值\n在DayofMonth域使用?,表示每天都触发一次\n\n\n#\n在DayofMonth中，确定第几个星期几\n1#3表示第三个星期日\n\n\nL\n表示最后\n在DayofWeek中使用5L,表示在最后一个星期四触发\n\n\nW\n表示有效工作日(周一到周五)\n在DayofMonth使用5W，如果5日是星期六，则将在最近的工作日4日触发一次\n\n\n演示案例说明\n用户对某商品进行下单操作；\n系统需要根据用户购买的商品信息生成订单并锁定商品的库存；\n系统设置了60分钟用户不付款就会取消订单；\n开启一个定时任务，每隔10分钟检查下，如果有超时还未付款的订单，就取消订单并取消锁定的商品库存。\n\n添加SpringTask的配置\n\n\n\n\n\n\n\n\n只需要在配置类中添加一个@EnableScheduling注解即可开启SpringTask的定时任务能力。\n/**\n * 定时任务配置\n */\n@Configuration\n@EnableScheduling\npublic class SpringTaskConfig &#123;\n&#125;\n\n添加OrderTimeOutCancelTask来执行定时任务/**\n * 订单超时取消并解锁库存的定时器\n */\n@Component\npublic class OrderTimeOutCancelTask &#123;\n    private Logger LOGGER = LoggerFactory.getLogger(OrderTimeOutCancelTask.class);\n\n    /**\n     * cron表达式：Seconds Minutes Hours DayofMonth Month DayofWeek [Year]\n     * 每10分钟扫描一次，扫描设定超时时间之前下的订单，如果没支付则取消该订单\n     */\n    @Scheduled(cron = \"0 0/10 * ? * ?\")\n    private void cancelTimeOutOrder() &#123;\n        LOGGER.info(\"取消订单\");\n    &#125;\n&#125;\n","slug":"SpringTask实现定时任务","date":"2023-05-31T02:51:56.000Z","categories_index":"","tags_index":"Spring,SpringTask,定时任务","author_index":"大宝贝的程序员"},{"id":"c03c4f80044b1216d3d7a05319a58324","title":"线程池异常日志处理","content":"线程池异常日志处理线程池是作为池化技术的一种常见应用\n先定义一个线程池实例\n@Bean\n    public ThreadPoolExecutor uncaughtExceptionExecutor() &#123;\n\n        ThreadFactory threadFactory = new ThreadFactoryBuilder().\n                setNameFormat(\"uncaughtException-worker-%d\").build();\n        int processors = Runtime.getRuntime().availableProcessors();\n        log.info(\"processors:&#123;&#125;\", processors);\n        ThreadPoolExecutor executor = new ThreadPoolExecutor(processors,\n                processors * 2,\n                0L,\n                TimeUnit.MILLISECONDS,\n                new LinkedBlockingDeque&lt;>(1000),\n                threadFactory,\n                new ThreadPoolExecutor.AbortPolicy());\n        return executor;\n    &#125;\n\n测试案例\n@Override\n   public void executorLogTest() throws Exception &#123;\n       int count = 10;\n       CountDownLatch countDownLatch = new CountDownLatch(count);\n       for (int i = 0; i &lt; count; i++) &#123;\n           uncaughtExceptionExecutor.execute(() -> &#123;\n               String str = null;\n               log.info(\"execute str:&#123;&#125;\", str);\n\t\t//空指针异常\n               int length = str.length();\n\n               countDownLatch.countDown();\n\n           &#125;);\n       &#125;\n\n       countDownLatch.await();\n\n       log.info(\"concurrent execute finished!\");\n   &#125;\n\n会报空指针，希望会在日志文件中将堆栈信息的日志打印出来，但真实的情况会是什么样的呢？\n2022-04-30 11:54:26.593 [uncaughtException-worker-5] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.593 [uncaughtException-worker-3] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.593 [uncaughtException-worker-2] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.593 [uncaughtException-worker-0] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.593 [uncaughtException-worker-4] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.593 [uncaughtException-worker-1] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.599 [uncaughtException-worker-10] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.598 [uncaughtException-worker-8] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.599 [uncaughtException-worker-6] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n2022-04-30 11:54:26.599 [uncaughtException-worker-9] INFO  c.t.m.s.i.UserServiceImpl - execute str:null\n\nlog文件中没有异常的堆栈日志，通过分析源码ThreadPoolExecutor 会发现这个问题的根源！\n线程池为了不影响其他线程的执行，会将当前线程的异常全部捕获，但是这样会带来一个问题，若开发人员忘记了处理异常，就会莫名缺少一些关键的堆栈日志，处理这个问题主要有三种方案。\n解决方案\n\n自行处理异常\n\n最简单直接的方法就是自己动手丰衣足食，将线程池中的业务逻辑catch起来然后自己处理，这个方案会带来代码上的冗余，尤其是业务代码没有checkException，另外有时开发人员也会忘记添加catch，显然，此方案不是最优方案；\n@Override\npublic void executorLogTest() throws Exception &#123;\n    int count = 10;\n    CountDownLatch countDownLatch = new CountDownLatch(count);\n    for (int i = 0; i &lt; count; i++) &#123;\n        uncaughtExceptionExecutor.execute(() -> &#123;\n            try &#123;\n                String str = null;\n                log.info(\"execute str:&#123;&#125;\", str);\n\n                int length = str.length();\n\n            &#125; catch (Exception e) &#123;\n                log.error(\"executorLogTest occur error\", e);\n\n            &#125; finally &#123;\n                countDownLatch.countDown();\n            &#125;\n        &#125;);\n    &#125;\n\n    countDownLatch.await();\n\n    log.info(\"concurrent execute finished!\");\n&#125;\n\n按照这种方式处理后，log文件中会显示堆栈信息\n\n继承ThreadPoolExecutor并重写afterExecute方法\n\n@Slf4j\npublic class CustomThreadPoolExecutor extends ThreadPoolExecutor &#123;\n\n    public CustomThreadPoolExecutor(int corePoolSize,\n                                    int maximumPoolSize,\n                                    long keepAliveTime,\n                                    TimeUnit unit,\n                                    BlockingQueue&lt;Runnable> workQueue,\n                                    ThreadFactory threadFactory,\n                                    RejectedExecutionHandler handler) &#123;\n        super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, handler);\n    &#125;\n\n    @Override\n    protected void afterExecute(Runnable r, Throwable t) &#123;\n        log.error(\"CustomThreadPoolExecutor occur error\", t);\n    &#125;\n&#125;\n\n\n\n@Override\n    public void customExecutor() throws InterruptedException &#123;\n        int count = 10;\n        CountDownLatch countDownLatch = new CountDownLatch(count);\n        for (int i = 0; i &lt; count; i++) &#123;\n            customExecutor.execute(() -> &#123;\n                String str = null;\n                log.info(\"customExecutor str:&#123;&#125;\", str);\n\n                int length = str.length();\n\n                countDownLatch.countDown();\n\n            &#125;);\n\n        &#125;\n\n        countDownLatch.await();\n\n        log.info(\"concurrent execute finished!\");\n    &#125;\n\nlog文件中会显示堆栈信息\n\n实现Thread.UncaughtExceptionHandler\n\nThread.UncaughtExceptionHandler exceptionHandler = (Thread t, Throwable e) -> &#123;\n        log.info(\"current thread occurs error！\", e);\n    &#125;;\n\n    @Bean\n    public ThreadPoolExecutor executor() &#123;\n\n        ThreadFactory threadFactory = new ThreadFactoryBuilder().\n                setUncaughtExceptionHandler(exceptionHandler).\n                setNameFormat(\"mouse-worker-%d\").build();\n        int processors = Runtime.getRuntime().availableProcessors();\n        log.info(\"processors:&#123;&#125;\", processors);\n        ThreadPoolExecutor executor = new ThreadPoolExecutor(processors,\n                processors * 2,\n                0L,\n                TimeUnit.MILLISECONDS,\n                new LinkedBlockingDeque&lt;>(1000),\n                threadFactory,\n                new ThreadPoolExecutor.AbortPolicy());\n        return executor;\n\n这段代码创建了一个线程池，并为线程池中的线程设置了异常处理器（UncaughtExceptionHandler）。异常处理器的作用是在异常未被捕获时，将异常信息打印到日志中，以便后续排查问题。\n@Override\n   public void executorLogTest() throws Exception &#123;\n       int count = 10;\n       CountDownLatch countDownLatch = new CountDownLatch(count);\n       for (int i = 0; i &lt; count; i++) &#123;\n           executor.execute(() -> &#123;\n               String str = null;\n               log.info(\"execute str:&#123;&#125;\", str);\n\n               int length = str.length();\n           &#125;);\n       &#125;\n\n       countDownLatch.await();\n\n       log.info(\"concurrent execute finished!\");\n   &#125;\n\nlog文件中会显示堆栈信息\n","slug":"线程池异常日志处理","date":"2023-05-30T12:43:04.000Z","categories_index":"","tags_index":"线程池","author_index":"大宝贝的程序员"},{"id":"2e24c73ebcb94047e46d06be7faec991","title":"SpringSecurity和JWT实现认证授权","content":"SpringSecurity和JWT实现认证授权在pom.xml中添加项目依赖&lt;!--SpringSecurity依赖配置-->\n&lt;dependency>\n    &lt;groupId>org.springframework.boot&lt;/groupId>\n    &lt;artifactId>spring-boot-starter-security&lt;/artifactId>\n&lt;/dependency>\n&lt;!--Hutool Java工具包-->\n&lt;dependency>\n    &lt;groupId>cn.hutool&lt;/groupId>\n    &lt;artifactId>hutool-all&lt;/artifactId>\n    &lt;version>4.5.7&lt;/version>\n&lt;/dependency>\n&lt;!--JWT(Json Web Token)登录支持-->\n&lt;dependency>\n    &lt;groupId>io.jsonwebtoken&lt;/groupId>\n    &lt;artifactId>jjwt&lt;/artifactId>\n    &lt;version>0.9.0&lt;/version>\n&lt;/dependency>\n\n添加JWT token的工具类\n\n\n\n\n\n\n\n\n用于生成和解析JWT token的工具类\n相关方法说明：\n\ngenerateToken(UserDetails userDetails) :用于根据登录用户信息生成token\ngetUserNameFromToken(String token)：从token中获取登录用户的信息\nvalidateToken(String token, UserDetails userDetails)：判断token是否还有效\n\n/**\n * JwtToken生成的工具类\n */\n@Component\npublic class JwtTokenUtil &#123;\n    private static final Logger LOGGER = LoggerFactory.getLogger(JwtTokenUtil.class);\n    private static final String CLAIM_KEY_USERNAME = \"sub\";\n    private static final String CLAIM_KEY_CREATED = \"created\";\n    @Value(\"$&#123;jwt.secret&#125;\")\n    private String secret;\n    @Value(\"$&#123;jwt.expiration&#125;\")\n    private Long expiration;\n\n    /**\n     * 根据负责生成JWT的token\n     */\n    private String generateToken(Map&lt;String, Object> claims) &#123;\n        return Jwts.builder()\n                .setClaims(claims)\n                .setExpiration(generateExpirationDate())\n                .signWith(SignatureAlgorithm.HS512, secret)\n                .compact();\n    &#125;\n\n    /**\n     * 从token中获取JWT中的负载\n     */\n    private Claims getClaimsFromToken(String token) &#123;\n        Claims claims = null;\n        try &#123;\n            claims = Jwts.parser()\n                    .setSigningKey(secret)\n                    .parseClaimsJws(token)\n                    .getBody();\n        &#125; catch (Exception e) &#123;\n            LOGGER.info(\"JWT格式验证失败:&#123;&#125;\",token);\n        &#125;\n        return claims;\n    &#125;\n\n    /**\n     * 生成token的过期时间\n     */\n    private Date generateExpirationDate() &#123;\n        return new Date(System.currentTimeMillis() + expiration * 1000);\n    &#125;\n\n    /**\n     * 从token中获取登录用户名\n     */\n    public String getUserNameFromToken(String token) &#123;\n        String username;\n        try &#123;\n            Claims claims = getClaimsFromToken(token);\n            username =  claims.getSubject();\n        &#125; catch (Exception e) &#123;\n            username = null;\n        &#125;\n        return username;\n    &#125;\n\n    /**\n     * 验证token是否还有效\n     *\n     * @param token       客户端传入的token\n     * @param userDetails 从数据库中查询出来的用户信息\n     */\n    public boolean validateToken(String token, UserDetails userDetails) &#123;\n        String username = getUserNameFromToken(token);\n        return username.equals(userDetails.getUsername()) &amp;&amp; !isTokenExpired(token);\n    &#125;\n\n    /**\n     * 判断token是否已经失效\n     */\n    private boolean isTokenExpired(String token) &#123;\n        Date expiredDate = getExpiredDateFromToken(token);\n        return expiredDate.before(new Date());\n    &#125;\n\n    /**\n     * 从token中获取过期时间\n     */\n    private Date getExpiredDateFromToken(String token) &#123;\n        Claims claims = getClaimsFromToken(token);\n        return claims.getExpiration();\n    &#125;\n\n    /**\n     * 根据用户信息生成token\n     */\n    public String generateToken(UserDetails userDetails) &#123;\n        Map&lt;String, Object> claims = new HashMap&lt;>();\n        claims.put(CLAIM_KEY_USERNAME, userDetails.getUsername());\n        claims.put(CLAIM_KEY_CREATED, new Date());\n        return generateToken(claims);\n    &#125;\n\n    /**\n     * 判断token是否可以被刷新\n     */\n    public boolean canRefresh(String token) &#123;\n        return !isTokenExpired(token);\n    &#125;\n\n    /**\n     * 刷新token\n     */\n    public String refreshToken(String token) &#123;\n        Claims claims = getClaimsFromToken(token);\n        claims.put(CLAIM_KEY_CREATED, new Date());\n        return generateToken(claims);\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n\n\n定义了一个名为 generateToken 的方法，这个方法的作用是根据传入的参数Map&lt;String, Object&gt; claims生成一个JWT令牌，并将其作为字符串返回。\n使用 JJWT 库，JJWT 是一款 Java 语言的 JWT 实现库，用于生成和验证 JSON Web Tokens。在这里我们使用了该库提供的 Jwts.builder() 方法来构建 JWT。\n在该方法中，通过调用 setClaims(claims) 方法将传入的声明（claims）设置到 JWT 中，这些声明包括了一些标准声明，例如过期时间（exp）、发行者（iss）、接收者（aud）等。可以根据具体需求自定义添加更多的声明。\n然后通过调用 setExpiration(generateExpirationDate()) 方法来设置 JWT 的过期时间。generateExpirationDate() 方法返回一个过期时间戳，该时间戳用于表示 JWT 的有效期限。\n接下来调用 signWith(SignatureAlgorithm.HS512, secret) 方法来设置 JWT 的签名算法和签名密钥。在这个例子中，我们使用 HS512 算法来进行签名，并且将秘密字符串 secret 作为签名密钥来进行签名。\n最后，调用 compact() 方法将 JWT 转换成字符串，并将其作为返回值返回到调用者。\n\n\n\n\n\n\n\n\n\n调用 getClaimsFromToken(token) 方法获取令牌中的声明（claims）。然后，我们通过 claims.getSubject() 方法获取用户名信息，并将其赋值给 username 变量。\n如果在获取用户名的过程中发生了任何异常，则捕获异常并将 username 变量设置为 null。claims.getSubject() 方法是 JJWT 库提供的一个方法，用于从 JWT 令牌的声明中获取 sub 字段所对应的值。\n在 JWT 中，sub 字段通常表示令牌的主题，通常被用来识别令牌所代表的用户或实体。例如，在用户经过身份验证之后，我们可以将用户的唯一标识存储在 sub 字段中，这样就可以在后续的请求中使用令牌中的 sub 字段来识别用户。\n当我们调用 claims.getSubject() 方法时，JJWT 库会从令牌的声明中获取 sub 字段的值，并将其作为字符串类型的返回值返回给调用者。\n\n\n\n\n\n\n\n\n\n在这个方法中，我们先通过 JJWT 库提供的 Jwts.parser() 方法获取一个 JWT 解析器实例。然后，通过 setSigningKey(secret) 方法将传入的秘密字符串作为签名密钥来进行签名验证。\n接下来，我们通过调用 parseClaimsJws(token) 方法将 JWT 令牌字符串解析并进行签名验证。如果签名验证通过，则调用 getBody() 方法将 JWT 令牌的声明（claims）提取出来保存到一个 Claims 对象中，并将其返回。\n如果在解析和验证的过程中出现了异常，则捕获并记录日志，并返回 null。\n这个方法的作用是从 JWT 令牌中提取出声明（claims），从而得到 JWT 中存储的信息，例如用户ID、过期时间、权限等。通常情况下，我们会在服务器端使用此方法来验证 JWT 是否有效，并提取出其中存储的信息。\nSpringSecurity的配置类相关依赖及方法说明\nconfigure(HttpSecurity httpSecurity)：用于配置需要拦截的url路径、jwt过滤器及出异常后的处理器；\nconfigure(AuthenticationManagerBuilder auth)：用于配置UserDetailsService及PasswordEncoder；\nRestfulAccessDeniedHandler：当用户没有访问权限时的处理器，用于返回JSON格式的处理结果；\nRestAuthenticationEntryPoint：当未登录或token失效时，返回JSON格式的结果；\nUserDetailsService:SpringSecurity定义的核心接口，用于根据用户名获取用户信息，需要自行实现；\nUserDetails：SpringSecurity定义用于封装用户信息的类（主要是用户信息和权限），需要自行实现；\nPasswordEncoder：SpringSecurity定义的用于对密码进行编码及比对的接口，目前使用的是BCryptPasswordEncoder；\nJwtAuthenticationTokenFilter：在用户名和密码校验前添加的过滤器，如果有jwt的token，会自行根据token信息进行登录。\n\n/**\n * SpringSecurity的配置\n * Created by macro on 2018/4/26.\n */\n@Configuration\n@EnableWebSecurity\n@EnableGlobalMethodSecurity(prePostEnabled=true)\npublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123;\n    @Autowired\n    private UmsAdminService adminService;\n    @Autowired\n    private RestfulAccessDeniedHandler restfulAccessDeniedHandler;\n    @Autowired\n    private RestAuthenticationEntryPoint restAuthenticationEntryPoint;\n\n    @Override\n    protected void configure(HttpSecurity httpSecurity) throws Exception &#123;\n        httpSecurity.csrf()// 由于使用的是JWT，我们这里不需要csrf\n                .disable()\n                .sessionManagement()// 基于token，所以不需要session\n                .sessionCreationPolicy(SessionCreationPolicy.STATELESS)\n                .and()\n                .authorizeRequests()\n                .antMatchers(HttpMethod.GET, // 允许对于网站静态资源的无授权访问\n                        \"/\",\n                        \"/*.html\",\n                        \"/favicon.ico\",\n                        \"/**/*.html\",\n                        \"/**/*.css\",\n                        \"/**/*.js\",\n                        \"/swagger-resources/**\",\n                        \"/v2/api-docs/**\"\n                )\n                .permitAll()\n                .antMatchers(\"/admin/login\", \"/admin/register\")// 对登录注册要允许匿名访问\n                .permitAll()\n                .antMatchers(HttpMethod.OPTIONS)//跨域请求会先进行一次options请求\n                .permitAll()\n//                .antMatchers(\"/**\")//测试时全部运行访问\n//                .permitAll()\n                .anyRequest()// 除上面外的所有请求全部需要鉴权认证\n                .authenticated();\n        // 禁用缓存\n        httpSecurity.headers().cacheControl();\n        /**\n 使用`addFilterBefore`方法将自定义的JWT认证过滤器`jwtAuthenticationTokenFilter`添加到Spring Security的认证过滤器链中。这样，所有请求必须经过JWT认证过滤器才能继续处理。其次，使用`exceptionHandling()`方法添加自定义的未授权和未登录结果返回处理器，分别为`restfulAccessDeniedHandler`和`restAuthenticationEntryPoint`。这表示当用户没有登录或没有访问权限时，会返回自定义的结果，而不是Spring Security默认的结果。\n        */\n        // 添加JWT filter\n        httpSecurity.addFilterBefore(jwtAuthenticationTokenFilter(), UsernamePasswordAuthenticationFilter.class);\n        //添加自定义未授权和未登录结果返回\n        httpSecurity.exceptionHandling()\n                .accessDeniedHandler(restfulAccessDeniedHandler)\n                .authenticationEntryPoint(restAuthenticationEntryPoint);\n    &#125;\n\n    @Override\n    protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123;\n        auth.userDetailsService(userDetailsService())\n                .passwordEncoder(passwordEncoder());\n    &#125;\n\n    @Bean\n    public PasswordEncoder passwordEncoder() &#123;\n        return new BCryptPasswordEncoder();\n    &#125;\n\n    @Bean\n    public UserDetailsService userDetailsService() &#123;\n        //获取登录用户信息\n        return username -> &#123;\n            UmsAdmin admin = adminService.getAdminByUsername(username);\n            if (admin != null) &#123;\n                List&lt;UmsPermission> permissionList = adminService.getPermissionList(admin.getId());\n                return new AdminUserDetails(admin,permissionList);\n            &#125;\n            throw new UsernameNotFoundException(\"用户名或密码错误\");\n        &#125;;\n    &#125;\n\n    @Bean\n    public JwtAuthenticationTokenFilter jwtAuthenticationTokenFilter()&#123;\n        return new JwtAuthenticationTokenFilter();\n    &#125;\n\n    @Bean\n    @Override\n    public AuthenticationManager authenticationManagerBean() throws Exception &#123;\n        return super.authenticationManagerBean();\n    &#125;\n\n&#125;\n\nRestfulAccessDeniedHandler/**\n * 当访问接口没有权限时，自定义的返回结果\n * Created by macro on 2018/4/26.\n */\n@Component\npublic class RestfulAccessDeniedHandler implements AccessDeniedHandler&#123;\n    @Override\n    public void handle(HttpServletRequest request,\n                       HttpServletResponse response,\n                       AccessDeniedException e) throws IOException, ServletException &#123;\n        response.setCharacterEncoding(\"UTF-8\");\n        response.setContentType(\"application/json\");\n        response.getWriter().println(JSONUtil.parse(CommonResult.forbidden(e.getMessage())));\n        response.getWriter().flush();\n    &#125;\n&#125;\n\nRestAuthenticationEntryPoint/**\n * 当未登录或者token失效访问接口时，自定义的返回结果\n * Created by macro on 2018/5/14.\n */\n@Component\npublic class RestAuthenticationEntryPoint implements AuthenticationEntryPoint &#123;\n    @Override\n    public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException authException) throws IOException, ServletException &#123;\n        response.setCharacterEncoding(\"UTF-8\");\n        response.setContentType(\"application/json\");\n        response.getWriter().println(JSONUtil.parse(CommonResult.unauthorized(authException.getMessage())));\n        response.getWriter().flush();\n    &#125;\n&#125;\n\n\nAdminUserDetails/**\n * SpringSecurity需要的用户详情\n \n */\npublic class AdminUserDetails implements UserDetails &#123;\n    private UmsAdmin umsAdmin;\n    private List&lt;UmsPermission> permissionList;\n    public AdminUserDetails(UmsAdmin umsAdmin, List&lt;UmsPermission> permissionList) &#123;\n        this.umsAdmin = umsAdmin;\n        this.permissionList = permissionList;\n    &#125;\n\n    @Override\n    public Collection&lt;? extends GrantedAuthority> getAuthorities() &#123;\n        //返回当前用户的权限\n        return permissionList.stream()\n                .filter(permission -> permission.getValue()!=null)\n                .map(permission ->new SimpleGrantedAuthority(permission.getValue()))\n                .collect(Collectors.toList());\n    &#125;\n\n    @Override\n    public String getPassword() &#123;\n        return umsAdmin.getPassword();\n    &#125;\n\n    @Override\n    public String getUsername() &#123;\n        return umsAdmin.getUsername();\n    &#125;\n\n    @Override\n    public boolean isAccountNonExpired() &#123;\n        return true;\n    &#125;\n\n    @Override\n    public boolean isAccountNonLocked() &#123;\n        return true;\n    &#125;\n\n    @Override\n    public boolean isCredentialsNonExpired() &#123;\n        return true;\n    &#125;\n\n    @Override\n    public boolean isEnabled() &#123;\n        return umsAdmin.getStatus().equals(1);\n    &#125;\n&#125;\n\nAdminUserDetails\n\n\n\n\n\n\n\n\nUserDetails 是 Spring Security 中的核心接口之一，它代表了认证用户的相关信息。该接口中包含了用户的用户名、密码、角色等信息。\n一般来说，我们需要在使用 Spring Security 进行身份认证时，提供一个实现了 UserDetails 接口的类，这个类通常被称为 UserDetailsService。它负责根据给定的用户名从数据库或其他数据源中查找用户信息，并返回一个 UserDetails 对象，Spring Security 将通过该对象来进行身份认证。\n要实现 UserDetails 接口，通常可以新建一个实体类用于封装用户信息，并实现 UserDetailsService 接口中的方法。在实现 getAuthorities() 方法时，需要将用户的角色信息转换为一个 GrantedAuthority 对象的集合，通常可以使用 Spring Security 提供的 SimpleGrantedAuthority 类来作为 GrantedAuthority 的实现类。\n使用 UserDetails 接口，可以将用户信息和角色相关的信息封装在一起，并且方便了 Spring Security 进行身份验证和权限控制。同时，通过实现 UserDetailsService 接口，可以将用户信息的获取方式与具体的实现解耦，从而提高了程序的灵活性和可维护性。\n/**\n * SpringSecurity需要的用户详情\n */\npublic class AdminUserDetails implements UserDetails &#123;\n    private UmsAdmin umsAdmin;\n    private List&lt;UmsPermission> permissionList;\n    public AdminUserDetails(UmsAdmin umsAdmin, List&lt;UmsPermission> permissionList) &#123;\n        this.umsAdmin = umsAdmin;\n        this.permissionList = permissionList;\n    &#125;\n\n    @Override\n    public Collection&lt;? extends GrantedAuthority> getAuthorities() &#123;\n        //返回当前用户的权限\n        return permissionList.stream()\n                .filter(permission -> permission.getValue()!=null)\n                .map(permission ->new SimpleGrantedAuthority(permission.getValue()))\n                .collect(Collectors.toList());\n    &#125;\n\n    @Override\t//获取用户的密码\n    public String getPassword() &#123;\n        return umsAdmin.getPassword();\n    &#125;\n\n    @Override\t//获取用户的用户名\n    public String getUsername() &#123;\n        return umsAdmin.getUsername();\n    &#125;\n\n    @Override\t//判断用户账号是否已过期\n    public boolean isAccountNonExpired() &#123;\n        return true;\n    &#125;\n\n    @Override\t//判断用户账号是否已锁定\n    public boolean isAccountNonLocked() &#123;\n        return true;\n    &#125;\n\n    @Override\t//判断用户的证书是否已过期\n    public boolean isCredentialsNonExpired() &#123;\n        return true;\n    &#125;\n\n    @Override\t//判断用户是否已启用\n    public boolean isEnabled() &#123;\n        return umsAdmin.getStatus().equals(1);\n    &#125;\n&#125;\n\nJwtAuthenticationTokenFilter\n\n\n\n\n\n\n\n\n在用户名和密码校验前添加的过滤器，如果请求中有jwt的token且有效，会取出token中的用户名，然后调用SpringSecurity的API进行登录操作。\n/**\n * JWT登录授权过滤器\n OncePerRequestFilter 是 Spring Security 提供的一个过滤器，它用于确保同一个请求中的过滤器只会被执行一次，即使多次调用 filterChain.doFilter() 方法。\n */\npublic class JwtAuthenticationTokenFilter extends OncePerRequestFilter &#123;\n    private static final Logger LOGGER = LoggerFactory.getLogger(JwtAuthenticationTokenFilter.class);\n    @Autowired\n    private UserDetailsService userDetailsService;\n    @Autowired\n    private JwtTokenUtil jwtTokenUtil;\n    @Value(\"$&#123;jwt.tokenHeader&#125;\")\n    private String tokenHeader;\n    @Value(\"$&#123;jwt.tokenHead&#125;\")\n    private String tokenHead;\n\n    @Override\n    protected void doFilterInternal(HttpServletRequest request,\n                                    HttpServletResponse response,\n                                    FilterChain chain) throws ServletException, IOException &#123;\n        String authHeader = request.getHeader(this.tokenHeader);\n        if (authHeader != null &amp;&amp; authHeader.startsWith(this.tokenHead)) &#123;\n            String authToken = authHeader.substring(this.tokenHead.length());// The part after \"Bearer \"\n            String username = jwtTokenUtil.getUserNameFromToken(authToken);\n            LOGGER.info(\"checking username:&#123;&#125;\", username);\n            if (username != null &amp;&amp; SecurityContextHolder.getContext().getAuthentication() == null) &#123;\n                UserDetails userDetails = this.userDetailsService.loadUserByUsername(username);\n                if (jwtTokenUtil.validateToken(authToken, userDetails)) &#123;\n                    UsernamePasswordAuthenticationToken authentication = new UsernamePasswordAuthenticationToken(userDetails, null, userDetails.getAuthorities());\n                    authentication.setDetails(new WebAuthenticationDetailsSource().buildDetails(request));\n                    LOGGER.info(\"authenticated user:&#123;&#125;\", username);\n                    SecurityContextHolder.getContext().setAuthentication(authentication);\n                &#125;\n            &#125;\n        &#125;\n        chain.doFilter(request, response);\n    &#125;\n&#125;\n\n\n登录注册功能实现UmsAdminController类\n\n\n\n\n\n\n\n\n实现了后台用户登录、注册及获取权限的接口\n/**\n * 后台用户管理\n * Created by macro on 2018/4/26.\n */\n@Controller\n@Api(tags = \"UmsAdminController\", description = \"后台用户管理\")\n@RequestMapping(\"/admin\")\npublic class UmsAdminController &#123;\n    @Autowired\n    private UmsAdminService adminService;\n    @Value(\"$&#123;jwt.tokenHeader&#125;\")\n    private String tokenHeader;\n    @Value(\"$&#123;jwt.tokenHead&#125;\")\n    private String tokenHead;\n\n    @ApiOperation(value = \"用户注册\")\n    @RequestMapping(value = \"/register\", method = RequestMethod.POST)\n    @ResponseBody\n    public CommonResult&lt;UmsAdmin> register(@RequestBody UmsAdmin umsAdminParam, BindingResult result) &#123;\n        UmsAdmin umsAdmin = adminService.register(umsAdminParam);\n        if (umsAdmin == null) &#123;\n            CommonResult.failed();\n        &#125;\n        return CommonResult.success(umsAdmin);\n    &#125;\n\n    @ApiOperation(value = \"登录以后返回token\")\n    @RequestMapping(value = \"/login\", method = RequestMethod.POST)\n    @ResponseBody\n    public CommonResult login(@RequestBody UmsAdminLoginParam umsAdminLoginParam, BindingResult result) &#123;\n        String token = adminService.login(umsAdminLoginParam.getUsername(), umsAdminLoginParam.getPassword());\n        if (token == null) &#123;\n            return CommonResult.validateFailed(\"用户名或密码错误\");\n        &#125;\n        Map&lt;String, String> tokenMap = new HashMap&lt;>();\n        tokenMap.put(\"token\", token);\n        tokenMap.put(\"tokenHead\", tokenHead);\n        return CommonResult.success(tokenMap);\n    &#125;\n\n    @ApiOperation(\"获取用户所有权限（包括+-权限）\")\n    @RequestMapping(value = \"/permission/&#123;adminId&#125;\", method = RequestMethod.GET)\n    @ResponseBody\n    public CommonResult&lt;List&lt;UmsPermission>> getPermissionList(@PathVariable Long adminId) &#123;\n        List&lt;UmsPermission> permissionList = adminService.getPermissionList(adminId);\n        return CommonResult.success(permissionList);\n    &#125;\n&#125;\n\nUmsAdminService接口/**\n * 后台管理员Service\n * Created by macro on 2018/4/26.\n */\npublic interface UmsAdminService &#123;\n    /**\n     * 根据用户名获取后台管理员\n     */\n    UmsAdmin getAdminByUsername(String username);\n\n    /**\n     * 注册功能\n     */\n    UmsAdmin register(UmsAdmin umsAdminParam);\n\n    /**\n     * 登录功能\n     * @param username 用户名\n     * @param password 密码\n     * @return 生成的JWT的token\n     */\n    String login(String username, String password);\n\n    /**\n     * 获取用户所有权限（包括角色权限和+-权限）\n     */\n    List&lt;UmsPermission> getPermissionList(Long adminId);\n&#125;\n\nUmsAdminServiceImpl类/**\n * UmsAdminService实现类\n * Created by macro on 2018/4/26.\n */\n@Service\npublic class UmsAdminServiceImpl implements UmsAdminService &#123;\n    private static final Logger LOGGER = LoggerFactory.getLogger(UmsAdminServiceImpl.class);\n    @Autowired\n    private UserDetailsService userDetailsService;\n    @Autowired\n    private JwtTokenUtil jwtTokenUtil;\n    @Autowired\n    private PasswordEncoder passwordEncoder;\n    @Value(\"$&#123;jwt.tokenHead&#125;\")\n    private String tokenHead;\n    @Autowired\n    private UmsAdminMapper adminMapper;\n    @Autowired\n    private UmsAdminRoleRelationDao adminRoleRelationDao;\n\n    @Override\n    public UmsAdmin getAdminByUsername(String username) &#123;\n        UmsAdminExample example = new UmsAdminExample();\n        example.createCriteria().andUsernameEqualTo(username);\n        List&lt;UmsAdmin> adminList = adminMapper.selectByExample(example);\n        if (adminList != null &amp;&amp; adminList.size() > 0) &#123;\n            return adminList.get(0);\n        &#125;\n        return null;\n    &#125;\n\n    @Override\n    public UmsAdmin register(UmsAdmin umsAdminParam) &#123;\n        UmsAdmin umsAdmin = new UmsAdmin();\n        BeanUtils.copyProperties(umsAdminParam, umsAdmin);\n        umsAdmin.setCreateTime(new Date());\n        umsAdmin.setStatus(1);\n        //查询是否有相同用户名的用户\n        UmsAdminExample example = new UmsAdminExample();\n        example.createCriteria().andUsernameEqualTo(umsAdmin.getUsername());\n        List&lt;UmsAdmin> umsAdminList = adminMapper.selectByExample(example);\n        if (umsAdminList.size() > 0) &#123;\n            return null;\n        &#125;\n        //将密码进行加密操作\n        String encodePassword = passwordEncoder.encode(umsAdmin.getPassword());\n        umsAdmin.setPassword(encodePassword);\n        adminMapper.insert(umsAdmin);\n        return umsAdmin;\n    &#125;\n\n    @Override\n    public String login(String username, String password) &#123;\n        String token = null;\n        try &#123;\n            UserDetails userDetails = userDetailsService.loadUserByUsername(username);\n            if (!passwordEncoder.matches(password, userDetails.getPassword())) &#123;\n                throw new BadCredentialsException(\"密码不正确\");\n            &#125;\n            UsernamePasswordAuthenticationToken authentication = new UsernamePasswordAuthenticationToken(userDetails, null, userDetails.getAuthorities());\n            SecurityContextHolder.getContext().setAuthentication(authentication);\n            token = jwtTokenUtil.generateToken(userDetails);\n        &#125; catch (AuthenticationException e) &#123;\n            LOGGER.warn(\"登录异常:&#123;&#125;\", e.getMessage());\n        &#125;\n        return token;\n    &#125;\n\n\n    @Override\n    public List&lt;UmsPermission> getPermissionList(Long adminId) &#123;\n        return adminRoleRelationDao.getPermissionList(adminId);\n    &#125;\n&#125;\n\n修改Swagger的配置\n\n\n\n\n\n\n\n\n通过修改配置实现调用接口自带Authorization头，这样就可以访问需要登录的接口了。\n/**\n * Swagger2API文档的配置\n */\n@Configuration\n@EnableSwagger2\npublic class Swagger2Config &#123;\n    @Bean\n    public Docket createRestApi()&#123;\n        return new Docket(DocumentationType.SWAGGER_2)\n                .apiInfo(apiInfo())\n                .select()\n                //为当前包下controller生成API文档\n                .apis(RequestHandlerSelectors.basePackage(\"com.macro.mall.tiny.controller\"))\n                .paths(PathSelectors.any())\n                .build()\n                //添加登录认证\n                .securitySchemes(securitySchemes())\n                .securityContexts(securityContexts());\n    &#125;\n\n    private ApiInfo apiInfo() &#123;\n        return new ApiInfoBuilder()\n                .title(\"SwaggerUI演示\")\n                .description(\"mall-tiny\")\n                .contact(\"macro\")\n                .version(\"1.0\")\n                .build();\n    &#125;\n\n    private List&lt;ApiKey> securitySchemes() &#123;\n        //设置请求头信息\n        List&lt;ApiKey> result = new ArrayList&lt;>();\n        ApiKey apiKey = new ApiKey(\"Authorization\", \"Authorization\", \"header\");\n        result.add(apiKey);\n        return result;\n    &#125;\n\n    private List&lt;SecurityContext> securityContexts() &#123;\n        //设置需要登录认证的路径\n        List&lt;SecurityContext> result = new ArrayList&lt;>();\n        result.add(getContextByPath(\"/brand/.*\"));\n        return result;\n    &#125;\n\n    private SecurityContext getContextByPath(String pathRegex)&#123;\n        return SecurityContext.builder()\n                .securityReferences(defaultAuth())\n                .forPaths(PathSelectors.regex(pathRegex))\n                .build();\n    &#125;\n\n    private List&lt;SecurityReference> defaultAuth() &#123;\n        List&lt;SecurityReference> result = new ArrayList&lt;>();\n        AuthorizationScope authorizationScope = new AuthorizationScope(\"global\", \"accessEverything\");\n        AuthorizationScope[] authorizationScopes = new AuthorizationScope[1];\n        authorizationScopes[0] = authorizationScope;\n        result.add(new SecurityReference(\"Authorization\", authorizationScopes));\n        return result;\n    &#125;\n&#125;\n","slug":"SpringSecurity和JWT实现认证授权","date":"2023-05-30T02:49:20.000Z","categories_index":"","tags_index":"认证授权,SpringSecurity,JWT","author_index":"大宝贝的程序员"},{"id":"d706fd469eec284e90755d9abf2cccf2","title":"SpringBoot中自定义注解","content":"SpringBoot中自定义注解","slug":"SpringBoot中自定义注解","date":"2023-05-29T14:05:15.000Z","categories_index":"","tags_index":"注解,SpringBoot","author_index":"大宝贝的程序员"},{"id":"6ee3eede71e00c52eb0546cb31634cea","title":"Netty进阶","content":"粘包与半包服务端代码\npublic class HelloWorldServer &#123;\n    static final Logger log = LoggerFactory.getLogger(HelloWorldServer.class);\n    void start() &#123;\n        NioEventLoopGroup boss = new NioEventLoopGroup(1);\n        NioEventLoopGroup worker = new NioEventLoopGroup();\n        try &#123;\n            ServerBootstrap serverBootstrap = new ServerBootstrap();\n            serverBootstrap.channel(NioServerSocketChannel.class);\n            serverBootstrap.group(boss, worker);\n            serverBootstrap.childHandler(new ChannelInitializer&lt;SocketChannel>() &#123;\n                @Override\n                protected void initChannel(SocketChannel ch) throws Exception &#123;\n                    ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n                    ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                        @Override\n                        public void channelActive(ChannelHandlerContext ctx) throws Exception &#123;\n                            log.debug(\"connected &#123;&#125;\", ctx.channel());\n                            super.channelActive(ctx);\n                        &#125;\n\n                        @Override\n                        public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123;\n                            log.debug(\"disconnect &#123;&#125;\", ctx.channel());\n                            super.channelInactive(ctx);\n                        &#125;\n                    &#125;);\n                &#125;\n            &#125;);\n            ChannelFuture channelFuture = serverBootstrap.bind(8080);\n            log.debug(\"&#123;&#125; binding...\", channelFuture.channel());\n            channelFuture.sync();\n            log.debug(\"&#123;&#125; bound...\", channelFuture.channel());\n            channelFuture.channel().closeFuture().sync();\n        &#125; catch (InterruptedException e) &#123;\n            log.error(\"server error\", e);\n        &#125; finally &#123;\n            boss.shutdownGracefully();\n            worker.shutdownGracefully();\n            log.debug(\"stoped\");\n        &#125;\n    &#125;\n\n    public static void main(String[] args) &#123;\n        new HelloWorldServer().start();\n    &#125;\n&#125;\n\n客户端代码希望发送 10 个消息，每个消息是 16 字节\npublic class HelloWorldClient &#123;\n    static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class);\n    public static void main(String[] args) &#123;\n        NioEventLoopGroup worker = new NioEventLoopGroup();\n        try &#123;\n            Bootstrap bootstrap = new Bootstrap();\n            bootstrap.channel(NioSocketChannel.class);\n            bootstrap.group(worker);\n            bootstrap.handler(new ChannelInitializer&lt;SocketChannel>() &#123;\n                @Override\n                protected void initChannel(SocketChannel ch) throws Exception &#123;\n                    log.debug(\"connetted...\");\n                    ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                        @Override\n                        public void channelActive(ChannelHandlerContext ctx) throws Exception &#123;\n                            log.debug(\"sending...\");\n                            Random r = new Random();\n                            char c = 'a';\n                            for (int i = 0; i &lt; 10; i++) &#123;\n                                ByteBuf buffer = ctx.alloc().buffer();\n                                buffer.writeBytes(new byte[]&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15&#125;);\n                                ctx.writeAndFlush(buffer);\n                            &#125;\n                        &#125;\n                    &#125;);\n                &#125;\n            &#125;);\n            ChannelFuture channelFuture = bootstrap.connect(\"127.0.0.1\", 8080).sync();\n            channelFuture.channel().closeFuture().sync();\n\n        &#125; catch (InterruptedException e) &#123;\n            log.error(\"client error\", e);\n        &#125; finally &#123;\n            worker.shutdownGracefully();\n        &#125;\n    &#125;\n&#125;\n\n服务器端的某次输出，可以看到一次就接收了 160 个字节，而非分 10 次接收\n08:24:46 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0x81e0fda5] binding...\n08:24:46 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0x81e0fda5, L:&#x2F;0:0:0:0:0:0:0:0:8080] bound...\n08:24:55 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x94132411, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:58177] REGISTERED\n08:24:55 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x94132411, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:58177] ACTIVE\n08:24:55 [DEBUG] [nioEventLoopGroup-3-1] c.i.n.HelloWorldServer - connected [id: 0x94132411, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:58177]\n08:24:55 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x94132411, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:58177] READ: 160B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000010| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000020| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000030| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000040| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000050| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000060| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000070| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000080| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000090| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n+--------+-------------------------------------------------+----------------+\n08:24:55 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x94132411, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:58177] READ COMPLETE\n\n粘包\n\n现象，发送 abc def，接收 abcdef\n原因\n应用层：接收方 ByteBuf 设置太大（Netty 默认 1024）\n滑动窗口：假设发送方 256 bytes 表示一个完整报文，但由于接收方处理不及时且窗口大小足够大，这 256 bytes 字节就会缓冲在接收方的滑动窗口中，当滑动窗口中缓冲了多个报文就会粘包\nNagle 算法：会造成粘包\n\n\n\n半包\n半包现象客户端代码希望发送 1 个消息，这个消息是 160 字节，代码改为\nByteBuf buffer = ctx.alloc().buffer();\nfor (int i = 0; i &lt; 10; i++) &#123;\n    buffer.writeBytes(new byte[]&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15&#125;);\n&#125;\nctx.writeAndFlush(buffer);\n\n为现象明显，服务端修改一下接收缓冲区，其它代码不变\nserverBootstrap.option(ChannelOption.SO_RCVBUF, 10);\n\n服务器端的某次输出，可以看到接收的消息被分为两节，第一次 20 字节，第二次 140 字节\n08:43:49 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0x4d6c6a84] binding...\n08:43:49 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0x4d6c6a84, L:&#x2F;0:0:0:0:0:0:0:0:8080] bound...\n08:44:23 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:59221] REGISTERED\n08:44:23 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:59221] ACTIVE\n08:44:23 [DEBUG] [nioEventLoopGroup-3-1] c.i.n.HelloWorldServer - connected [id: 0x1719abf7, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:59221]\n08:44:24 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:59221] READ: 20B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 00 01 02 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f |................|\n|00000010| 00 01 02 03                                     |....            |\n+--------+-------------------------------------------------+----------------+\n08:44:24 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:59221] READ COMPLETE\n08:44:24 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:59221] READ: 140B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................|\n|00000010| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................|\n|00000020| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................|\n|00000030| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................|\n|00000040| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................|\n|00000050| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................|\n|00000060| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................|\n|00000070| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 00 01 02 03 |................|\n|00000080| 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f             |............    |\n+--------+-------------------------------------------------+----------------+\n08:44:24 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x1719abf7, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:59221] READ COMPLETE\n\n\n\n\n\n\n\n\n\n\nserverBootstrap.option(ChannelOption.SO_RCVBUF, 10) 影响的底层接收缓冲区（即滑动窗口）大小，仅决定了 netty 读取的最小单位，netty 实际每次读取的一般是它的整数倍\n半包\n\n现象，发送 abcdef，接收 abc def\n原因\n应用层：接收方 ByteBuf 小于实际发送数据量\n滑动窗口：假设接收方的窗口只剩了 128 bytes，发送方的报文大小是 256 bytes，这时放不下了，只能先发送前 128 bytes，等待 ack 后才能发送剩余部分，这就造成了半包\nMSS 限制：当发送的数据超过 MSS 限制后，会将数据切分发送，就会造成半包\n\n\n\n本质是因为 TCP 是流式协议，消息无边界\n\n\n\n\n\n\n\n\n\n滑动窗口\n\nTCP 以一个段（segment）为单位，每发送一个段就需要进行一次确认应答（ack）处理，但如果这么做，缺点是包的往返时间越长性能就越差\n\n\n为了解决此问题，引入了窗口概念，窗口大小即决定了无需等待应答而可以继续发送的数据最大值\n\n\n窗口实际就起到一个缓冲区的作用，同时也能起到流量控制的作用\n\n图中深色的部分即要发送的数据，高亮的部分即窗口\n窗口内的数据才允许被发送，当应答未到达前，窗口必须停止滑动\n如果 1001~2000 这个段的数据 ack 回来了，窗口就可以向前滑动\n接收方也会维护一个窗口，只有落在窗口内的数据才能允许接收\n\n\n\n\n\n\n\n\n\n\n\n\n MSS 限制\n\n链路层对一次能够发送的最大数据有限制，这个限制称之为 MTU（maximum transmission unit），不同的链路设备的 MTU 值也有所不同，例如\n\n以太网的 MTU 是 1500\n\nFDDI（光纤分布式数据接口）的 MTU 是 4352\n\n本地回环地址的 MTU 是 65535 - 本地测试不走网卡\n\nMSS 是最大段长度（maximum segment size），它是 MTU 刨去 tcp 头和 ip 头后剩余能够作为数据传输的字节数\n\nipv4 tcp 头占用 20 bytes，ip 头占用 20 bytes，因此以太网 MSS 的值为 1500 - 40 &#x3D; 1460\n\nTCP 在传递大量数据时，会按照 MSS 大小将数据进行分割发送\n\nMSS 的值在三次握手时通知对方自己 MSS 的值，然后在两者之间选择一个小值作为 MSS\n\n\n  \n\n\n\n\n\n\n\n\n\nNagle 算法\n\n即使发送一个字节，也需要加入 tcp 头和 ip 头，也就是总字节数会使用 41 bytes，非常不经济。因此为了提高网络利用率，tcp 希望尽可能发送足够大的数据，这就是 Nagle 算法产生的缘由\n该算法是指发送端即使还有应该发送的数据，但如果这部分数据很少的话，则进行延迟发送\n如果 SO_SNDBUF 的数据达到 MSS，则需要发送\n如果 SO_SNDBUF 中含有 FIN（表示需要连接关闭）这时将剩余数据发送，再关闭\n如果 TCP_NODELAY &#x3D; true，则需要发送\n已发送的数据都收到 ack 时，则需要发送\n上述条件不满足，但发生超时（一般为 200ms）则需要发送\n除上述情况，延迟发送\n\n\n\n解决方案\n短链接，发一个包建立一次连接，这样连接建立到连接断开之间就是消息的边界，缺点效率太低\n每一条消息采用固定长度，缺点浪费空间\n每一条消息采用分隔符，例如 \\n，缺点需要转义\n每一条消息分为 head 和 body，head 中包含 body 的长度\n\n短链接以解决粘包为例\npublic class HelloWorldClient &#123;\n    static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class);\n\n    public static void main(String[] args) &#123;\n        // 分 10 次发送\n        for (int i = 0; i &lt; 10; i++) &#123;\n            send();\n        &#125;\n    &#125;\n\n    private static void send() &#123;\n        NioEventLoopGroup worker = new NioEventLoopGroup();\n        try &#123;\n            Bootstrap bootstrap = new Bootstrap();\n            bootstrap.channel(NioSocketChannel.class);\n            bootstrap.group(worker);\n            bootstrap.handler(new ChannelInitializer&lt;SocketChannel>() &#123;\n                @Override\n                protected void initChannel(SocketChannel ch) throws Exception &#123;\n                    log.debug(\"conneted...\");\n                    ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n                    ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                        @Override\n                        public void channelActive(ChannelHandlerContext ctx) throws Exception &#123;\n                            log.debug(\"sending...\");\n                            ByteBuf buffer = ctx.alloc().buffer();\n                            buffer.writeBytes(new byte[]&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15&#125;);\n                            ctx.writeAndFlush(buffer);\n                            // 发完即关\n                            ctx.close();\n                        &#125;\n                    &#125;);\n                &#125;\n            &#125;);\n            ChannelFuture channelFuture = bootstrap.connect(\"localhost\", 8080).sync();\n            channelFuture.channel().closeFuture().sync();\n\n        &#125; catch (InterruptedException e) &#123;\n            log.error(\"client error\", e);\n        &#125; finally &#123;\n            worker.shutdownGracefully();\n        &#125;\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n\n\n半包用这种办法还是不好解决，因为接收方的缓冲区大小是有限的\n固定长度让所有数据包长度固定（假设长度为 8 字节），服务器端加入\nch.pipeline().addLast(new FixedLengthFrameDecoder(8));\n\n客户端测试代码，注意, 采用这种方法后，客户端什么时候 flush 都可以\npublic class HelloWorldClient &#123;\n    static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class);\n\n    public static void main(String[] args) &#123;\n        NioEventLoopGroup worker = new NioEventLoopGroup();\n        try &#123;\n            Bootstrap bootstrap = new Bootstrap();\n            bootstrap.channel(NioSocketChannel.class);\n            bootstrap.group(worker);\n            bootstrap.handler(new ChannelInitializer&lt;SocketChannel>() &#123;\n                @Override\n                protected void initChannel(SocketChannel ch) throws Exception &#123;\n                    log.debug(\"connetted...\");\n                    ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n                    ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                        @Override\n                        public void channelActive(ChannelHandlerContext ctx) throws Exception &#123;\n                            log.debug(\"sending...\");\n                            // 发送内容随机的数据包\n                            Random r = new Random();\n                            char c = 'a';\n                            ByteBuf buffer = ctx.alloc().buffer();\n                            for (int i = 0; i &lt; 10; i++) &#123;\n                                byte[] bytes = new byte[8];\n                                for (int j = 0; j &lt; r.nextInt(8); j++) &#123;\n                                    bytes[j] = (byte) c;\n                                &#125;\n                                c++;\n                                buffer.writeBytes(bytes);\n                            &#125;\n                            ctx.writeAndFlush(buffer);\n                        &#125;\n                    &#125;);\n                &#125;\n            &#125;);\n            ChannelFuture channelFuture = bootstrap.connect(\"127.0.0.1\", 9090).sync();\n            channelFuture.channel().closeFuture().sync();\n\n        &#125; catch (InterruptedException e) &#123;\n            log.error(\"client error\", e);\n        &#125; finally &#123;\n            worker.shutdownGracefully();\n        &#125;\n    &#125;\n&#125;\n\n客户端输出\n12:07:00 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - connetted...\n12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2] REGISTERED\n12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2] CONNECT: &#x2F;192.168.0.103:9090\n12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2, L:&#x2F;192.168.0.103:53155 - R:&#x2F;192.168.0.103:9090] ACTIVE\n12:07:00 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - sending...\n12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2, L:&#x2F;192.168.0.103:53155 - R:&#x2F;192.168.0.103:9090] WRITE: 80B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 61 61 61 61 00 00 00 00 62 00 00 00 00 00 00 00 |aaaa....b.......|\n|00000010| 63 63 00 00 00 00 00 00 64 00 00 00 00 00 00 00 |cc......d.......|\n|00000020| 00 00 00 00 00 00 00 00 66 66 66 66 00 00 00 00 |........ffff....|\n|00000030| 67 67 67 00 00 00 00 00 68 00 00 00 00 00 00 00 |ggg.....h.......|\n|00000040| 69 69 69 69 69 00 00 00 6a 6a 6a 6a 00 00 00 00 |iiiii...jjjj....|\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x3c2ef3c2, L:&#x2F;192.168.0.103:53155 - R:&#x2F;192.168.0.103:9090] FLUSH\n\n服务端输出\n12:06:51 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0xe3d9713f] binding...\n12:06:51 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0xe3d9713f, L:&#x2F;192.168.0.103:9090] bound...\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] REGISTERED\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] ACTIVE\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] c.i.n.HelloWorldServer - connected [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155]\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 61 61 61 61 00 00 00 00                         |aaaa....        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 62 00 00 00 00 00 00 00                         |b.......        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 63 63 00 00 00 00 00 00                         |cc......        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 64 00 00 00 00 00 00 00                         |d.......        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 00 00 00 00 00 00 00 00                         |........        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 66 66 66 66 00 00 00 00                         |ffff....        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 67 67 67 00 00 00 00 00                         |ggg.....        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 68 00 00 00 00 00 00 00                         |h.......        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 69 69 69 69 69 00 00 00                         |iiiii...        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 6a 6a 6a 6a 00 00 00 00                         |jjjj....        |\n+--------+-------------------------------------------------+----------------+\n12:07:00 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0xd739f137, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:53155] READ COMPLETE\n\n缺点是，数据包的大小不好把握\n\n长度定的太大，浪费\n长度定的太小，对某些数据包又显得不够\n\n固定分隔符服务端加入，默认以 \\n 或 \\r\\n 作为分隔符，如果超出指定长度仍未出现分隔符，则抛出异常\nch.pipeline().addLast(new LineBasedFrameDecoder(1024));\n\n客户端在每条消息之后，加入 \\n 分隔符\npublic class HelloWorldClient &#123;\n    static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class);\n\n    public static void main(String[] args) &#123;\n        NioEventLoopGroup worker = new NioEventLoopGroup();\n        try &#123;\n            Bootstrap bootstrap = new Bootstrap();\n            bootstrap.channel(NioSocketChannel.class);\n            bootstrap.group(worker);\n            bootstrap.handler(new ChannelInitializer&lt;SocketChannel>() &#123;\n                @Override\n                protected void initChannel(SocketChannel ch) throws Exception &#123;\n                    log.debug(\"connetted...\");\n                    ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n                    ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                        @Override\n                        public void channelActive(ChannelHandlerContext ctx) throws Exception &#123;\n                            log.debug(\"sending...\");\n                            Random r = new Random();\n                            char c = 'a';\n                            ByteBuf buffer = ctx.alloc().buffer();\n                            for (int i = 0; i &lt; 10; i++) &#123;\n                                for (int j = 1; j &lt;= r.nextInt(16)+1; j++) &#123;\n                                    buffer.writeByte((byte) c);\n                                &#125;\n                                buffer.writeByte(10);\n                                c++;\n                            &#125;\n                            ctx.writeAndFlush(buffer);\n                        &#125;\n                    &#125;);\n                &#125;\n            &#125;);\n            ChannelFuture channelFuture = bootstrap.connect(\"127.0.0.1\", 9090).sync();\n            channelFuture.channel().closeFuture().sync();\n\n        &#125; catch (InterruptedException e) &#123;\n            log.error(\"client error\", e);\n        &#125; finally &#123;\n            worker.shutdownGracefully();\n        &#125;\n    &#125;\n&#125;\n\n客户端输出\n14:08:18 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - connetted...\n14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755] REGISTERED\n14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755] CONNECT: &#x2F;192.168.0.103:9090\n14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755, L:&#x2F;192.168.0.103:63641 - R:&#x2F;192.168.0.103:9090] ACTIVE\n14:08:18 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - sending...\n14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755, L:&#x2F;192.168.0.103:63641 - R:&#x2F;192.168.0.103:9090] WRITE: 60B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 61 0a 62 62 62 0a 63 63 63 0a 64 64 0a 65 65 65 |a.bbb.ccc.dd.eee|\n|00000010| 65 65 65 65 65 65 65 0a 66 66 0a 67 67 67 67 67 |eeeeeee.ff.ggggg|\n|00000020| 67 67 0a 68 68 68 68 0a 69 69 69 69 69 69 69 0a |gg.hhhh.iiiiiii.|\n|00000030| 6a 6a 6a 6a 6a 6a 6a 6a 6a 6a 6a 0a             |jjjjjjjjjjj.    |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0x1282d755, L:&#x2F;192.168.0.103:63641 - R:&#x2F;192.168.0.103:9090] FLUSH\n\n\n\n服务端输出\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] c.i.n.HelloWorldServer - connected [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641]\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 1B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 61                                              |a               |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 3B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 62 62 62                                        |bbb             |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 3B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 63 63 63                                        |ccc             |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 2B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 64 64                                           |dd              |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 10B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 65 65 65 65 65 65 65 65 65 65                   |eeeeeeeeee      |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 2B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 66 66                                           |ff              |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 7B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 67 67 67 67 67 67 67                            |ggggggg         |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 4B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 68 68 68 68                                     |hhhh            |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 7B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 69 69 69 69 69 69 69                            |iiiiiii         |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ: 11B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 6a 6a 6a 6a 6a 6a 6a 6a 6a 6a 6a                |jjjjjjjjjjj     |\n+--------+-------------------------------------------------+----------------+\n14:08:18 [DEBUG] [nioEventLoopGroup-3-5] i.n.h.l.LoggingHandler - [id: 0xa4b3be43, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:63641] READ COMPLETE\n\n缺点，处理字符数据比较合适，但如果内容本身包含了分隔符（字节数据常常会有此情况），那么就会解析错误\n预设长度在发送消息前，先约定用定长字节表示接下来数据的长度\n\t\t\t\t\t\t\t\t\t\t// 最大长度，长度偏移，长度占用字节，长度调整，剥离字节数\nch.pipeline().addLast(new LengthFieldBasedFrameDecoder(1024, 0, 1, 0, 1));\n\n客户端代码\npublic class HelloWorldClient &#123;\n    static final Logger log = LoggerFactory.getLogger(HelloWorldClient.class);\n\n    public static void main(String[] args) &#123;\n        NioEventLoopGroup worker = new NioEventLoopGroup();\n        try &#123;\n            Bootstrap bootstrap = new Bootstrap();\n            bootstrap.channel(NioSocketChannel.class);\n            bootstrap.group(worker);\n            bootstrap.handler(new ChannelInitializer&lt;SocketChannel>() &#123;\n                @Override\n                protected void initChannel(SocketChannel ch) throws Exception &#123;\n                    log.debug(\"connetted...\");\n                    ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n                    ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                        @Override\n                        public void channelActive(ChannelHandlerContext ctx) throws Exception &#123;\n                            log.debug(\"sending...\");\n                            Random r = new Random();\n                            char c = 'a';\n                            ByteBuf buffer = ctx.alloc().buffer();\n                            for (int i = 0; i &lt; 10; i++) &#123;\n                                byte length = (byte) (r.nextInt(16) + 1);\n                                // 先写入长度\n                                buffer.writeByte(length);\n                                // 再\n                                for (int j = 1; j &lt;= length; j++) &#123;\n                                    buffer.writeByte((byte) c);\n                                &#125;\n                                c++;\n                            &#125;\n                            ctx.writeAndFlush(buffer);\n                        &#125;\n                    &#125;);\n                &#125;\n            &#125;);\n            ChannelFuture channelFuture = bootstrap.connect(\"127.0.0.1\", 9090).sync();\n            channelFuture.channel().closeFuture().sync();\n\n        &#125; catch (InterruptedException e) &#123;\n            log.error(\"client error\", e);\n        &#125; finally &#123;\n            worker.shutdownGracefully();\n        &#125;\n    &#125;\n&#125;\n\n\n\n客户端输出\n14:37:10 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - connetted...\n14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8] REGISTERED\n14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8] CONNECT: &#x2F;192.168.0.103:9090\n14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8, L:&#x2F;192.168.0.103:49979 - R:&#x2F;192.168.0.103:9090] ACTIVE\n14:37:10 [DEBUG] [nioEventLoopGroup-2-1] c.i.n.HelloWorldClient - sending...\n14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8, L:&#x2F;192.168.0.103:49979 - R:&#x2F;192.168.0.103:9090] WRITE: 97B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 09 61 61 61 61 61 61 61 61 61 09 62 62 62 62 62 |.aaaaaaaaa.bbbbb|\n|00000010| 62 62 62 62 06 63 63 63 63 63 63 08 64 64 64 64 |bbbb.cccccc.dddd|\n|00000020| 64 64 64 64 0f 65 65 65 65 65 65 65 65 65 65 65 |dddd.eeeeeeeeeee|\n|00000030| 65 65 65 65 0d 66 66 66 66 66 66 66 66 66 66 66 |eeee.fffffffffff|\n|00000040| 66 66 02 67 67 02 68 68 0e 69 69 69 69 69 69 69 |ff.gg.hh.iiiiiii|\n|00000050| 69 69 69 69 69 69 69 09 6a 6a 6a 6a 6a 6a 6a 6a |iiiiiii.jjjjjjjj|\n|00000060| 6a                                              |j               |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-2-1] i.n.h.l.LoggingHandler - [id: 0xf0f347b8, L:&#x2F;192.168.0.103:49979 - R:&#x2F;192.168.0.103:9090] FLUSH\n\n\n\n服务端输出\n14:36:50 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0xdff439d3] binding...\n14:36:51 [DEBUG] [main] c.i.n.HelloWorldServer - [id: 0xdff439d3, L:&#x2F;192.168.0.103:9090] bound...\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] REGISTERED\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] ACTIVE\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] c.i.n.HelloWorldServer - connected [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979]\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 9B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 61 61 61 61 61 61 61 61 61                      |aaaaaaaaa       |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 9B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 62 62 62 62 62 62 62 62 62                      |bbbbbbbbb       |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 6B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 63 63 63 63 63 63                               |cccccc          |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 64 64 64 64 64 64 64 64                         |dddddddd        |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 15B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65    |eeeeeeeeeeeeeee |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 13B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 66 66 66 66 66 66 66 66 66 66 66 66 66          |fffffffffffff   |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 2B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 67 67                                           |gg              |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 2B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 68 68                                           |hh              |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 14B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 69 69 69 69 69 69 69 69 69 69 69 69 69 69       |iiiiiiiiiiiiii  |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ: 9B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 6a 6a 6a 6a 6a 6a 6a 6a 6a                      |jjjjjjjjj       |\n+--------+-------------------------------------------------+----------------+\n14:37:10 [DEBUG] [nioEventLoopGroup-3-1] i.n.h.l.LoggingHandler - [id: 0x744f2b47, L:&#x2F;192.168.0.103:9090 - R:&#x2F;192.168.0.103:49979] READ COMPLETE\n\n\n协议设计与解析TCP&#x2F;IP 中消息传输基于流的方式，没有边界。\n协议的目的就是划定消息的边界，制定通信双方要共同遵守的通信规则\nhttp 协议举例NioEventLoopGroup boss = new NioEventLoopGroup();\nNioEventLoopGroup worker = new NioEventLoopGroup();\ntry &#123;\n    ServerBootstrap serverBootstrap = new ServerBootstrap();\n    serverBootstrap.channel(NioServerSocketChannel.class);\n    serverBootstrap.group(boss, worker);\n    serverBootstrap.childHandler(new ChannelInitializer&lt;SocketChannel>() &#123;\n        @Override\n        protected void initChannel(SocketChannel ch) throws Exception &#123;\n            ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n            ch.pipeline().addLast(new HttpServerCodec());\n            ch.pipeline().addLast(new SimpleChannelInboundHandler&lt;HttpRequest>() &#123;\n                @Override\n                protected void channelRead0(ChannelHandlerContext ctx, HttpRequest msg) throws Exception &#123;\n                    // 获取请求\n                    log.debug(msg.uri());\n\n                    // 返回响应\n                    DefaultFullHttpResponse response =\n       new DefaultFullHttpResponse(msg.protocolVersion(), HttpResponseStatus.OK);\n                    byte[] bytes = \"&lt;h1>Hello, world!&lt;/h1>\".getBytes();\n                    response.headers().setInt(CONTENT_LENGTH, bytes.length);\n                    response.content().writeBytes(bytes);\n                    // 写回响应\n                    ctx.writeAndFlush(response);\n                &#125;\n            &#125;);\n            /*ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                @Override\n  public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123;\n                    log.debug(\"&#123;&#125;\", msg.getClass());\n\n                    if (msg instanceof HttpRequest) &#123; // 请求行，请求头\n\n                    &#125; else if (msg instanceof HttpContent) &#123; //请求体\n\n                    &#125;\n                &#125;\n            &#125;);*/\n        &#125;\n    &#125;);\n    ChannelFuture channelFuture = serverBootstrap.bind(8080).sync();\n    channelFuture.channel().closeFuture().sync();\n&#125; catch (InterruptedException e) &#123;\n    log.error(\"server error\", e);\n&#125; finally &#123;\n    boss.shutdownGracefully();\n    worker.shutdownGracefully();\n&#125;\n\n自定义协议要素\n魔数，用来在第一时间判定是否是无效数据包\n版本号，可以支持协议的升级\n序列化算法，消息正文到底采用哪种序列化反序列化方式，可以由此扩展，例如：json、protobuf、hessian、jdk\n指令类型，是登录、注册、单聊、群聊… 跟业务相关\n请求序号，为了双工通信，提供异步能力\n正文长度\n消息正文\n\n编解码器根据上面的要素，设计一个登录请求消息和登录响应消息，并使用 Netty 完成收发\n@Slf4j\npublic class MessageCodec extends ByteToMessageCodec&lt;Message> &#123;\n\n    @Override\n    protected void encode(ChannelHandlerContext ctx, Message msg, ByteBuf out) throws Exception &#123;\n        // 1. 4 字节的魔数\n        out.writeBytes(new byte[]&#123;1, 2, 3, 4&#125;);\n        // 2. 1 字节的版本\n        out.writeByte(1);\n        // 3. 1 字节的序列化方式 jdk 0 , json 1\n        out.writeByte(0);\n        // 4. 1 字节的指令类型\n        out.writeByte(msg.getMessageType());\n        // 5. 4 个字节\n        out.writeInt(msg.getSequenceId());\n        // 无意义，对齐填充\n        out.writeByte(0xff);\n        // 6. 获取内容的字节数组\n        ByteArrayOutputStream bos = new ByteArrayOutputStream();\n        ObjectOutputStream oos = new ObjectOutputStream(bos);\n        oos.writeObject(msg);\n        byte[] bytes = bos.toByteArray();\n        // 7. 长度\n        out.writeInt(bytes.length);\n        // 8. 写入内容\n        out.writeBytes(bytes);\n    &#125;\n\n    @Override\n    protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object> out) throws Exception &#123;\n        int magicNum = in.readInt();\n        byte version = in.readByte();\n        byte serializerType = in.readByte();\n        byte messageType = in.readByte();\n        int sequenceId = in.readInt();\n        in.readByte();\n        int length = in.readInt();\n        byte[] bytes = new byte[length];\n        in.readBytes(bytes, 0, length);\n        ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(bytes));\n        Message message = (Message) ois.readObject();\n        log.debug(\"&#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;\", magicNum, version, serializerType, messageType, sequenceId, length);\n        log.debug(\"&#123;&#125;\", message);\n        out.add(message);\n    &#125;\n&#125;\n\n测试\nEmbeddedChannel channel = new EmbeddedChannel(\n    new LoggingHandler(),\n    new LengthFieldBasedFrameDecoder(\n        1024, 12, 4, 0, 0),\n    new MessageCodec()\n);\n// encode\nLoginRequestMessage message = new LoginRequestMessage(\"zhangsan\", \"123\", \"张三\");\n//        channel.writeOutbound(message);\n// decode\nByteBuf buf = ByteBufAllocator.DEFAULT.buffer();\nnew MessageCodec().encode(null, message, buf);\n\nByteBuf s1 = buf.slice(0, 100);\nByteBuf s2 = buf.slice(100, buf.readableBytes() - 100);\ns1.retain(); // 引用计数 2\nchannel.writeInbound(s1); // release 1\nchannel.writeInbound(s2);\n\n解读\n\n","slug":"Netty进阶","date":"2023-05-27T12:51:09.000Z","categories_index":"","tags_index":"Netty","author_index":"大宝贝的程序员"},{"id":"50b6ca39b1c5fcfd54bce8129993cbda","title":"Netty基础","content":"Netty 概述Netty 是一个异步的、基于事件驱动的网络应用框架，用于快速开发可维护、高性能的网络服务器和客户端\nNetty 的优势\nNetty vs NIO，工作量大，bug 多\n需要自己构建协议\n解决 TCP 传输问题，如粘包、半包\nepoll 空轮询导致 CPU 100%\n对 API 进行增强，使之更易用，如 FastThreadLocal &#x3D;&gt; ThreadLocal，ByteBuf &#x3D;&gt; ByteBuffer\n\n\nNetty vs 其它网络应用框架\nMina 由 apache 维护，将来 3.x 版本可能会有较大重构，破坏 API 向下兼容性，Netty 的开发迭代更迅速，API 更简洁、文档更优秀\n\n\n\n开发一个简单的服务器端和客户端\n客户端向服务器端发送 hello, world\n服务器仅接收，不返回\n\n加入依赖\n&lt;dependency>\n    &lt;groupId>io.netty&lt;/groupId>\n    &lt;artifactId>netty-all&lt;/artifactId>\n    &lt;version>4.1.39.Final&lt;/version>\n&lt;/dependency>\n\n服务器端new ServerBootstrap()\n    .group(new NioEventLoopGroup()) // 1\n    .channel(NioServerSocketChannel.class) // 2\n    .childHandler(new ChannelInitializer&lt;NioSocketChannel>() &#123; // 3\n        protected void initChannel(NioSocketChannel ch) &#123;\n            ch.pipeline().addLast(new StringDecoder()); // 5\n            ch.pipeline().addLast(new SimpleChannelInboundHandler&lt;String>() &#123; // 6\n                @Override\n                protected void channelRead0(ChannelHandlerContext ctx, String msg) &#123;\n                    System.out.println(msg);\n                &#125;\n            &#125;);\n        &#125;\n    &#125;)\n    .bind(8080); // 4\n\n代码解读\n\n1 处，创建 NioEventLoopGroup，可以简单理解为 线程池 + Selector 后面会详细展开\n\n2 处，选择服务 Scoket 实现类，其中 NioServerSocketChannel 表示基于 NIO 的服务器端实现，其它实现还有\n\n\n3 处，为啥方法叫 childHandler，是接下来添加的处理器都是给 SocketChannel 用的，而不是给 ServerSocketChannel。ChannelInitializer 处理器（仅执行一次），它的作用是待客户端 SocketChannel 建立连接后，执行 initChannel 以便添加更多的处理器\n\n4 处，ServerSocketChannel 绑定的监听端口\n\n5 处，SocketChannel 的处理器，解码 ByteBuf &#x3D;&gt; String\n\n6 处，SocketChannel 的业务处理器，使用上一个处理器的处理结果\n\n\n客户端new Bootstrap()\n    .group(new NioEventLoopGroup()) // 1\n    .channel(NioSocketChannel.class) // 2\n    .handler(new ChannelInitializer&lt;Channel>() &#123; // 3\n        @Override\n        protected void initChannel(Channel ch) &#123;\n            ch.pipeline().addLast(new StringEncoder()); // 8\n        &#125;\n    &#125;)\n    .connect(\"127.0.0.1\", 8080) // 4\n    .sync() // 5\n    .channel() // 6\n    .writeAndFlush(new Date() + \": hello world!\"); // 7\n\n代码解读\n\n1 处，创建 NioEventLoopGroup，同 Server\n\n2 处，选择客户 Socket 实现类，NioSocketChannel 表示基于 NIO 的客户端实现，其它实现还有\n\n\n3 处，添加 SocketChannel 的处理器，ChannelInitializer 处理器（仅执行一次），它的作用是待客户端 SocketChannel 建立连接后，执行 initChannel 以便添加更多的处理器\n\n4 处，指定要连接的服务器和端口\n\n5 处，Netty 中很多方法都是异步的，如 connect，这时需要使用 sync 方法等待 connect 建立连接完毕\n\n6 处，获取 channel 对象，它即为通道抽象，可以进行数据读写操作\n\n7 处，写入消息并清空缓冲区\n\n8 处，消息会经过通道 handler 处理，这里是将 String &#x3D;&gt; ByteBuf 发出\n\n数据经过网络传输，到达服务器端，服务器端 5 和 6 处的 handler 先后被触发，走完一个流程\n\n\n流程梳理\n\n\n\n\n\n\n\n\n\n\n把 channel 理解为数据的通道\n把 msg 理解为流动的数据，最开始输入是 ByteBuf，但经过 pipeline 的加工，会变成其它类型对象，最后输出又变成 ByteBuf\n把 handler 理解为数据的处理工序\n工序有多道，合在一起就是 pipeline，pipeline 负责发布事件（读、读取完成…）传播给每个 handler， handler 对自己感兴趣的事件进行处理（重写相应事件处理方法）\nhandler 分 Inbound 和 Outbound 两类\n\n\n把 eventLoop 理解为处理数据的工人\n工人可以管理多个 channel 的 io 操作，并且一旦工人负责了某个 channel，就要负责到底（绑定）\n工人既可以执行 io 操作，也可以进行任务处理，每位工人有任务队列，队列里可以堆放多个 channel 的待处理任务，任务分为普通任务、定时任务\n工人按照 pipeline 顺序，依次按照 handler 的规划（代码）处理数据，可以为每道工序指定不同的工人\n\n\n\n组件EventLoop事件循环对象\nEventLoop 本质是一个单线程执行器（同时维护了一个 Selector），里面有 run 方法处理 Channel 上源源不断的 io 事件。\n它的继承关系比较复杂\n\n一条线是继承自 j.u.c.ScheduledExecutorService 因此包含了线程池中所有的方法\n另一条线是继承自 netty 自己的 OrderedEventExecutor，\n提供了 boolean inEventLoop(Thread thread) 方法判断一个线程是否属于此 EventLoop\n提供了 parent 方法来看看自己属于哪个 EventLoopGroup\n\n\n\n事件循环组\nEventLoopGroup 是一组 EventLoop，Channel 一般会调用 EventLoopGroup 的 register 方法来绑定其中一个 EventLoop，后续这个 Channel 上的 io 事件都由此 EventLoop 来处理（保证了 io 事件处理时的线程安全）\n\n继承自 netty 自己的 EventExecutorGroup\n实现了 Iterable 接口提供遍历 EventLoop 的能力\n另有 next 方法获取集合中下一个 EventLoop\n\n\n\n以一个简单的实现为例：\n// 内部创建了两个 EventLoop, 每个 EventLoop 维护一个线程\nDefaultEventLoopGroup group = new DefaultEventLoopGroup(2);\nSystem.out.println(group.next());\nSystem.out.println(group.next());\nSystem.out.println(group.next());\n\n输出\nio.netty.channel.DefaultEventLoop@60f82f98\nio.netty.channel.DefaultEventLoop@35f983a6\nio.netty.channel.DefaultEventLoop@60f82f98\n\n也可以使用 for 循环\nDefaultEventLoopGroup group = new DefaultEventLoopGroup(2);\nfor (EventExecutor eventLoop : group) &#123;\n    System.out.println(eventLoop);\n&#125;\n\n输出\nio.netty.channel.DefaultEventLoop@60f82f98\nio.netty.channel.DefaultEventLoop@35f983a6\n\n关闭关闭 shutdownGracefully 方法。该方法会首先切换 EventLoopGroup 到关闭状态从而拒绝新的任务的加入，然后在任务队列的任务都处理完成后，停止线程的运行。从而确保整体应用是在正常有序的状态下退出的\n演示 NioEventLoop 处理 io 事件服务器端两个 nio worker 工人\nnew ServerBootstrap()\n    .group(new NioEventLoopGroup(1), new NioEventLoopGroup(2))\n    .channel(NioServerSocketChannel.class)\n    .childHandler(new ChannelInitializer&lt;NioSocketChannel>() &#123;\n        @Override\n        protected void initChannel(NioSocketChannel ch) &#123;\n            ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                @Override\n                public void channelRead(ChannelHandlerContext ctx, Object msg) &#123;\n                    ByteBuf byteBuf = msg instanceof ByteBuf ? ((ByteBuf) msg) : null;\n                    if (byteBuf != null) &#123;\n                        byte[] buf = new byte[16];\n                        ByteBuf len = byteBuf.readBytes(buf, 0, byteBuf.readableBytes());\n                        log.debug(new String(buf));\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n    &#125;).bind(8080).sync();\n\n客户端，启动三次，分别修改发送字符串为 zhangsan（第一次），lisi（第二次），wangwu（第三次）\npublic static void main(String[] args) throws InterruptedException &#123;\n    Channel channel = new Bootstrap()\n            .group(new NioEventLoopGroup(1))\n            .handler(new ChannelInitializer&lt;NioSocketChannel>() &#123;\n                @Override\n                protected void initChannel(NioSocketChannel ch) throws Exception &#123;\n                    System.out.println(\"init...\");\n                    ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n                &#125;\n            &#125;)\n            .channel(NioSocketChannel.class).connect(\"localhost\", 8080)\n            .sync()\n            .channel();\n\n    channel.writeAndFlush(ByteBufAllocator.DEFAULT.buffer().writeBytes(\"wangwu\".getBytes()));\n    Thread.sleep(2000);\n    channel.writeAndFlush(ByteBufAllocator.DEFAULT.buffer().writeBytes(\"wangwu\".getBytes()));\n\n最后输出\n22:03:34 [DEBUG] [nioEventLoopGroup-3-1] c.i.o.EventLoopTest - zhangsan       \n22:03:36 [DEBUG] [nioEventLoopGroup-3-1] c.i.o.EventLoopTest - zhangsan       \n22:05:36 [DEBUG] [nioEventLoopGroup-3-2] c.i.o.EventLoopTest - lisi           \n22:05:38 [DEBUG] [nioEventLoopGroup-3-2] c.i.o.EventLoopTest - lisi           \n22:06:09 [DEBUG] [nioEventLoopGroup-3-1] c.i.o.EventLoopTest - wangwu        \n22:06:11 [DEBUG] [nioEventLoopGroup-3-1] c.i.o.EventLoopTest - wangwu         \n\n可以看到两个工人轮流处理 channel，但工人与 channel 之间进行了绑定\n\n再增加两个非 nio 工人\nDefaultEventLoopGroup normalWorkers = new DefaultEventLoopGroup(2);\nnew ServerBootstrap()\n    .group(new NioEventLoopGroup(1), new NioEventLoopGroup(2))\n    .channel(NioServerSocketChannel.class)\n    .childHandler(new ChannelInitializer&lt;NioSocketChannel>() &#123;\n        @Override\n        protected void initChannel(NioSocketChannel ch)  &#123;\n            ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n            ch.pipeline().addLast(normalWorkers,\"myhandler\",\n              new ChannelInboundHandlerAdapter() &#123;\n                @Override\n                public void channelRead(ChannelHandlerContext ctx, Object msg) &#123;\n                    ByteBuf byteBuf = msg instanceof ByteBuf ? ((ByteBuf) msg) : null;\n                    if (byteBuf != null) &#123;\n                        byte[] buf = new byte[16];\n                        ByteBuf len = byteBuf.readBytes(buf, 0, byteBuf.readableBytes());\n                        log.debug(new String(buf));\n                    &#125;\n                &#125;\n            &#125;);\n        &#125;\n    &#125;).bind(8080).sync();\n\n客户端代码不变，启动三次，分别修改发送字符串为 zhangsan（第一次），lisi（第二次），wangwu（第三次）\n输出\n22:19:48 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52588] REGISTERED\n22:19:48 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52588] ACTIVE\n22:19:48 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52588] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 7a 68 61 6e 67 73 61 6e                         |zhangsan        |\n+--------+-------------------------------------------------+----------------+\n22:19:48 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52588] READ COMPLETE\n22:19:48 [DEBUG] [defaultEventLoopGroup-2-1] c.i.o.EventLoopTest - zhangsan        \n22:19:50 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52588] READ: 8B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 7a 68 61 6e 67 73 61 6e                         |zhangsan        |\n+--------+-------------------------------------------------+----------------+\n22:19:50 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x251562d5, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52588] READ COMPLETE\n22:19:50 [DEBUG] [defaultEventLoopGroup-2-1] c.i.o.EventLoopTest - zhangsan        \n22:20:24 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52612] REGISTERED\n22:20:24 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52612] ACTIVE\n22:20:25 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52612] READ: 4B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 6c 69 73 69                                     |lisi            |\n+--------+-------------------------------------------------+----------------+\n22:20:25 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52612] READ COMPLETE\n22:20:25 [DEBUG] [defaultEventLoopGroup-2-2] c.i.o.EventLoopTest - lisi            \n22:20:27 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52612] READ: 4B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 6c 69 73 69                                     |lisi            |\n+--------+-------------------------------------------------+----------------+\n22:20:27 [DEBUG] [nioEventLoopGroup-4-2] i.n.h.l.LoggingHandler - [id: 0x94b2a840, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52612] READ COMPLETE\n22:20:27 [DEBUG] [defaultEventLoopGroup-2-2] c.i.o.EventLoopTest - lisi            \n22:20:38 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52625] REGISTERED\n22:20:38 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52625] ACTIVE\n22:20:38 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52625] READ: 6B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 77 61 6e 67 77 75                               |wangwu          |\n+--------+-------------------------------------------------+----------------+\n22:20:38 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52625] READ COMPLETE\n22:20:38 [DEBUG] [defaultEventLoopGroup-2-1] c.i.o.EventLoopTest - wangwu          \n22:20:40 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52625] READ: 6B\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 77 61 6e 67 77 75                               |wangwu          |\n+--------+-------------------------------------------------+----------------+\n22:20:40 [DEBUG] [nioEventLoopGroup-4-1] i.n.h.l.LoggingHandler - [id: 0x79a26af9, L:&#x2F;127.0.0.1:8080 - R:&#x2F;127.0.0.1:52625] READ COMPLETE\n22:20:40 [DEBUG] [defaultEventLoopGroup-2-1] c.i.o.EventLoopTest - wangwu          \n\n可以看到，nio 工人和 非 nio 工人也分别绑定了 channel（LoggingHandler 由 nio 工人执行，而我们自己的 handler 由非 nio 工人执行）\n\nhandler 执行中如何切换线程？关键代码 io.netty.channel.AbstractChannelHandlerContext#invokeChannelRead()\nstatic void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) &#123;\n    final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, \"msg\"), next);\n    // 下一个 handler 的事件循环是否与当前的事件循环是同一个线程\n    EventExecutor executor = next.executor();\n    \n    // 是，直接调用\n    if (executor.inEventLoop()) &#123;\n        next.invokeChannelRead(m);\n    &#125; \n    // 不是，将要执行的代码作为任务提交给下一个事件循环处理（换人）\n    else &#123;\n        executor.execute(new Runnable() &#123;\n            @Override\n            public void run() &#123;\n                next.invokeChannelRead(m);\n            &#125;\n        &#125;);\n    &#125;\n&#125;\n\n\n如果两个 handler 绑定的是同一个线程，那么就直接调用\n否则，把要调用的代码封装为一个任务对象，由下一个 handler 的线程来调用\n\n演示 NioEventLoop 处理普通任务NioEventLoop 除了可以处理 io 事件，同样可以向它提交普通任务\nNioEventLoopGroup nioWorkers = new NioEventLoopGroup(2);\n\nlog.debug(\"server start...\");\nThread.sleep(2000);\nnioWorkers.execute(()->&#123;\n    log.debug(\"normal task...\");\n&#125;);\n\n输出\n22:30:36 [DEBUG] [main] c.i.o.EventLoopTest2 - server start...\n22:30:38 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - normal task...\n\n\n\n\n\n\n\n\n\n\n可以用来执行耗时较长的任务\n演示 NioEventLoop 处理定时任务NioEventLoopGroup nioWorkers = new NioEventLoopGroup(2);\n\nlog.debug(\"server start...\");\nThread.sleep(2000);\nnioWorkers.scheduleAtFixedRate(() -> &#123;\n    log.debug(\"running...\");\n&#125;, 0, 1, TimeUnit.SECONDS);\n\n输出\n22:35:15 [DEBUG] [main] c.i.o.EventLoopTest2 - server start...\n22:35:17 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - running...\n22:35:18 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - running...\n22:35:19 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - running...\n22:35:20 [DEBUG] [nioEventLoopGroup-2-1] c.i.o.EventLoopTest2 - running...\n...\n\n\n\n\n\n\n\n\n\n\n可以用来执行定时任务\nChannelchannel 的主要作用\n\nclose() 可以用来关闭 channel\ncloseFuture() 用来处理 channel 的关闭\nsync 方法作用是同步等待 channel 关闭\n而 addListener 方法是异步等待 channel 关闭\n\n\npipeline() 方法添加处理器\nwrite() 方法将数据写入\nwriteAndFlush() 方法将数据写入并刷出\n\nChannelFuture客户端代码\nnew Bootstrap()\n    .group(new NioEventLoopGroup())\n    .channel(NioSocketChannel.class)\n    .handler(new ChannelInitializer&lt;Channel>() &#123;\n        @Override\n        protected void initChannel(Channel ch) &#123;\n            ch.pipeline().addLast(new StringEncoder());\n        &#125;\n    &#125;)\n    .connect(\"127.0.0.1\", 8080)\n    .sync()\n    .channel()\n    .writeAndFlush(new Date() + \": hello world!\");\n\n拆开\nChannelFuture channelFuture = new Bootstrap()\n    .group(new NioEventLoopGroup())\n    .channel(NioSocketChannel.class)\n    .handler(new ChannelInitializer&lt;Channel>() &#123;\n        @Override\n        protected void initChannel(Channel ch) &#123;\n            ch.pipeline().addLast(new StringEncoder());\n        &#125;\n    &#125;)\n    .connect(\"127.0.0.1\", 8080); // 1\n\nchannelFuture.sync().channel().writeAndFlush(new Date() + \": hello world!\");\n\n\n1 处返回的是 ChannelFuture 对象，它的作用是利用 channel() 方法来获取 Channel 对象\n\n注意 connect 方法是异步的，意味着不等连接建立，方法执行就返回了。因此 channelFuture 对象中不能立刻获得到正确的 Channel 对象\n实验如下：\nChannelFuture channelFuture = new Bootstrap()\n    .group(new NioEventLoopGroup())\n    .channel(NioSocketChannel.class)\n    .handler(new ChannelInitializer&lt;Channel>() &#123;\n        @Override\n        protected void initChannel(Channel ch) &#123;\n            ch.pipeline().addLast(new StringEncoder());\n        &#125;\n    &#125;)\n    .connect(\"127.0.0.1\", 8080);\n\nSystem.out.println(channelFuture.channel()); // 1\nchannelFuture.sync(); // 2\nSystem.out.println(channelFuture.channel()); // 3\n\n\n执行到 1 时，连接未建立，打印 [id: 0x2e1884dd]\n执行到 2 时，sync 方法是同步等待连接建立完成\n执行到 3 时，连接肯定建立了，打印 [id: 0x2e1884dd, L:/127.0.0.1:57191 - R:/127.0.0.1:8080]\n\n除了用 sync 方法可以让异步操作同步以外，还可以使用回调的方式：\nChannelFuture channelFuture = new Bootstrap()\n    .group(new NioEventLoopGroup())\n    .channel(NioSocketChannel.class)\n    .handler(new ChannelInitializer&lt;Channel>() &#123;\n        @Override\n        protected void initChannel(Channel ch) &#123;\n            ch.pipeline().addLast(new StringEncoder());\n        &#125;\n    &#125;)\n    .connect(\"127.0.0.1\", 8080);\nSystem.out.println(channelFuture.channel()); // 1\nchannelFuture.addListener((ChannelFutureListener) future -> &#123;\n    System.out.println(future.channel()); // 2\n&#125;);\n\n\n执行到 1 时，连接未建立，打印 [id: 0x749124ba]\nChannelFutureListener 会在连接建立时被调用（其中 operationComplete 方法），因此执行到 2 时，连接肯定建立了，打印 [id: 0x749124ba, L:/127.0.0.1:57351 - R:/127.0.0.1:8080]\n\nCloseFuture@Slf4j\npublic class CloseFutureClient &#123;\n    public static void main(String[] args) throws InterruptedException &#123;\n        NioEventLoopGroup group new NioEventLoopGroup();\n        ChannelFuture channelFuture = new Bootstrap()\n                .group(group)\n                .channel(NioSocketChannel.class)\n                .handler(new ChannelInitializer&lt;NioSocketChannel>() &#123;\n                    @Override // 在连接建立后被调用\n                    protected void initChannel(NioSocketChannel ch) throws Exception &#123;\n                        ch.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG));\n                        ch.pipeline().addLast(new StringEncoder());\n                    &#125;\n                &#125;)\n                .connect(new InetSocketAddress(\"localhost\", 8080));\n        Channel channel = channelFuture.sync().channel();\n        log.debug(\"&#123;&#125;\", channel);\n        new Thread(()->&#123;\n            Scanner scanner = new Scanner(System.in);\n            while (true) &#123;\n                String line = scanner.nextLine();\n                if (\"q\".equals(line)) &#123;\n                    channel.close(); // close 异步操作 1s 之后\n//                    log.debug(\"处理关闭之后的操作\"); // 不能在这里善后\n                    break;\n                &#125;\n                channel.writeAndFlush(line);\n            &#125;\n        &#125;, \"input\").start();\n\n        // 获取 CloseFuture 对象， 1) 同步处理关闭， 2) 异步处理关闭\n        ChannelFuture closeFuture = channel.closeFuture();\n        /*log.debug(\"waiting close...\");\n        closeFuture.sync();\n        log.debug(\"处理关闭之后的操作\");*/\n        closeFuture.addListener(new ChannelFutureListener() &#123;\n            @Override\n            public void operationComplete(ChannelFuture future) throws Exception &#123;\n                log.debug(\"处理关闭之后的操作\");\n                group.shutdownGracefully();\n            &#125;\n        &#125;);\n    &#125;\n&#125;\n\n\n\n\n\n💡 异步提升的是什么\n为什么不在一个线程中去执行建立连接、去执行关闭 channel，那样不是也可以吗？非要用这么复杂的异步方式：比如一个线程发起建立连接，另一个线程去真正建立连接\n\n思考下面的场景，4 个医生给人看病，每个病人花费 20 分钟，而且医生看病的过程中是以病人为单位的，一个病人看完了，才能看下一个病人。假设病人源源不断地来，可以计算一下 4 个医生一天工作 8 小时，处理的病人总数是：4 * 8 * 3 = 96\n\n经研究发现，看病可以细分为四个步骤，经拆分后每个步骤需要 5 分钟，如下\n\n因此可以做如下优化，只有一开始，医生 2、3、4 分别要等待 5、10、15 分钟才能执行工作，但只要后续病人源源不断地来，他们就能够满负荷工作，并且处理病人的能力提高到了 4 * 8 * 12 效率几乎是原来的四倍\n\n要点\n\n单线程没法异步提高效率，必须配合多线程、多核 cpu 才能发挥异步的优势\n异步并没有缩短响应时间，反而有所增加\n合理进行任务拆分，也是利用异步的关键\n\nFuture &amp; Promise在异步处理时，经常用到这两个接口\n首先要说明 netty 中的 Future 与 jdk 中的 Future 同名，但是是两个接口，netty 的 Future 继承自 jdk 的 Future，而 Promise 又对 netty Future 进行了扩展\n\njdk Future 只能同步等待任务结束（或成功、或失败）才能得到结果\nnetty Future 可以同步等待任务结束得到结果，也可以异步方式得到结果，但都是要等任务结束\nnetty Promise 不仅有 netty Future 的功能，而且脱离了任务独立存在，只作为两个线程间传递结果的容器\n\n\n\n\n功能&#x2F;名称\njdk Future\nnetty Future\nPromise\n\n\n\ncancel\n取消任务\n-\n-\n\n\nisCanceled\n任务是否取消\n-\n-\n\n\nisDone\n任务是否完成，不能区分成功失败\n-\n-\n\n\nget\n获取任务结果，阻塞等待\n-\n-\n\n\ngetNow\n-\n获取任务结果，非阻塞，还未产生结果时返回 null\n-\n\n\nawait\n-\n等待任务结束，如果任务失败，不会抛异常，而是通过 isSuccess 判断\n-\n\n\nsync\n-\n等待任务结束，如果任务失败，抛出异常\n-\n\n\nisSuccess\n-\n判断任务是否成功\n-\n\n\ncause\n-\n获取失败信息，非阻塞，如果没有失败，返回null\n-\n\n\naddLinstener\n-\n添加回调，异步接收结果\n-\n\n\nsetSuccess\n-\n-\n设置成功结果\n\n\nsetFailure\n-\n-\n设置失败结果\n\n\n同步处理任务成功\nDefaultEventLoop eventExecutors = new DefaultEventLoop();\nDefaultPromise&lt;Integer> promise = new DefaultPromise&lt;>(eventExecutors);\n\neventExecutors.execute(()->&#123;\n    try &#123;\n        Thread.sleep(1000);\n    &#125; catch (InterruptedException e) &#123;\n        e.printStackTrace();\n    &#125;\n    log.debug(\"set success, &#123;&#125;\",10);\n    promise.setSuccess(10);\n&#125;);\n\nlog.debug(\"start...\");\nlog.debug(\"&#123;&#125;\",promise.getNow()); // 还没有结果\nlog.debug(\"&#123;&#125;\",promise.get());\n\n输出\n11:51:53 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start...\n11:51:53 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - null\n11:51:54 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set success, 10\n11:51:54 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - 10\n\n异步处理任务成功\nDefaultEventLoop eventExecutors = new DefaultEventLoop();\nDefaultPromise&lt;Integer> promise = new DefaultPromise&lt;>(eventExecutors);\n\n// 设置回调，异步接收结果\npromise.addListener(future -> &#123;\n    // 这里的 future 就是上面的 promise\n    log.debug(\"&#123;&#125;\",future.getNow());\n&#125;);\n\n// 等待 1000 后设置成功结果\neventExecutors.execute(()->&#123;\n    try &#123;\n        Thread.sleep(1000);\n    &#125; catch (InterruptedException e) &#123;\n        e.printStackTrace();\n    &#125;\n    log.debug(\"set success, &#123;&#125;\",10);\n    promise.setSuccess(10);\n&#125;);\n\nlog.debug(\"start...\");\n\n输出\n11:49:30 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start...\n11:49:31 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set success, 10\n11:49:31 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - 10\n\n同步处理任务失败 - sync &amp; get\nDefaultEventLoop eventExecutors = new DefaultEventLoop();\n        DefaultPromise&lt;Integer> promise = new DefaultPromise&lt;>(eventExecutors);\n\n        eventExecutors.execute(() -> &#123;\n            try &#123;\n                Thread.sleep(1000);\n            &#125; catch (InterruptedException e) &#123;\n                e.printStackTrace();\n            &#125;\n            RuntimeException e = new RuntimeException(\"error...\");\n            log.debug(\"set failure, &#123;&#125;\", e.toString());\n            promise.setFailure(e);\n        &#125;);\n\n        log.debug(\"start...\");\n        log.debug(\"&#123;&#125;\", promise.getNow());\n        promise.get(); // sync() 也会出现异常，只是 get 会再用 ExecutionException 包一层异常\n\n输出\n12:11:07 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start...\n12:11:07 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - null\n12:11:08 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set failure, java.lang.RuntimeException: error...\nException in thread &quot;main&quot; java.util.concurrent.ExecutionException: java.lang.RuntimeException: error...\n\tat io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:41)\n\tat com.itcast.oio.DefaultPromiseTest2.main(DefaultPromiseTest2.java:34)\nCaused by: java.lang.RuntimeException: error...\n\tat com.itcast.oio.DefaultPromiseTest2.lambda$main$0(DefaultPromiseTest2.java:27)\n\tat io.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:745)\n\n同步处理任务失败 - await\nDefaultEventLoop eventExecutors = new DefaultEventLoop();\nDefaultPromise&lt;Integer> promise = new DefaultPromise&lt;>(eventExecutors);\n\neventExecutors.execute(() -> &#123;\n    try &#123;\n        Thread.sleep(1000);\n    &#125; catch (InterruptedException e) &#123;\n        e.printStackTrace();\n    &#125;\n    RuntimeException e = new RuntimeException(\"error...\");\n    log.debug(\"set failure, &#123;&#125;\", e.toString());\n    promise.setFailure(e);\n&#125;);\n\nlog.debug(\"start...\");\nlog.debug(\"&#123;&#125;\", promise.getNow());\npromise.await(); // 与 sync 和 get 区别在于，不会抛异常\nlog.debug(\"result &#123;&#125;\", (promise.isSuccess() ? promise.getNow() : promise.cause()).toString());\n\n输出\n12:18:53 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start...\n12:18:53 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - null\n12:18:54 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set failure, java.lang.RuntimeException: error...\n12:18:54 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - result java.lang.RuntimeException: error...\n\n异步处理任务失败addListener（回调）\nDefaultEventLoop eventExecutors = new DefaultEventLoop();\nDefaultPromise&lt;Integer> promise = new DefaultPromise&lt;>(eventExecutors);\n\npromise.addListener(future -> &#123;\n    log.debug(\"result &#123;&#125;\", (promise.isSuccess() ? promise.getNow() : promise.cause()).toString());\n&#125;);\n\neventExecutors.execute(() -> &#123;\n    try &#123;\n        Thread.sleep(1000);\n    &#125; catch (InterruptedException e) &#123;\n        e.printStackTrace();\n    &#125;\n    RuntimeException e = new RuntimeException(\"error...\");\n    log.debug(\"set failure, &#123;&#125;\", e.toString());\n    promise.setFailure(e);\n&#125;);\n\nlog.debug(\"start...\");\n\n输出\n12:04:57 [DEBUG] [main] c.i.o.DefaultPromiseTest2 - start...\n12:04:58 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - set failure, java.lang.RuntimeException: error...\n12:04:58 [DEBUG] [defaultEventLoop-1-1] c.i.o.DefaultPromiseTest2 - result java.lang.RuntimeException: error...\n\nawait 死锁检查\nDefaultEventLoop eventExecutors = new DefaultEventLoop();\nDefaultPromise&lt;Integer> promise = new DefaultPromise&lt;>(eventExecutors);\n\neventExecutors.submit(()->&#123;\n    System.out.println(\"1\");\n    try &#123;\n        promise.await();\n        // 注意不能仅捕获 InterruptedException 异常\n        // 否则 死锁检查抛出的 BlockingOperationException 会继续向上传播\n        // 而提交的任务会被包装为 PromiseTask，它的 run 方法中会 catch 所有异常然后设置为 Promise 的失败结果而不会抛出\n    &#125; catch (Exception e) &#123; \n        e.printStackTrace();\n    &#125;\n    System.out.println(\"2\");\n&#125;);\neventExecutors.submit(()->&#123;\n    System.out.println(\"3\");\n    try &#123;\n        promise.await();\n    &#125; catch (Exception e) &#123;\n        e.printStackTrace();\n    &#125;\n    System.out.println(\"4\");\n&#125;);\n\n输出\n1\n2\n3\n4\nio.netty.util.concurrent.BlockingOperationException: DefaultPromise@47499c2a(incomplete)\n\tat io.netty.util.concurrent.DefaultPromise.checkDeadLock(DefaultPromise.java:384)\n\tat io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:212)\n\tat com.itcast.oio.DefaultPromiseTest.lambda$main$0(DefaultPromiseTest.java:27)\n\tat io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)\n\tat io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:73)\n\tat io.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:745)\nio.netty.util.concurrent.BlockingOperationException: DefaultPromise@47499c2a(incomplete)\n\tat io.netty.util.concurrent.DefaultPromise.checkDeadLock(DefaultPromise.java:384)\n\tat io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:212)\n\tat com.itcast.oio.DefaultPromiseTest.lambda$main$1(DefaultPromiseTest.java:36)\n\tat io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)\n\tat io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:73)\n\tat io.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\nHandler &amp; PipelineChannelHandler 用来处理 Channel 上的各种事件，分为入站、出站两种。所有 ChannelHandler 被连成一串，就是 Pipeline\n\n入站处理器通常是 ChannelInboundHandlerAdapter 的子类，主要用来读取客户端数据，写回结果\n出站处理器通常是 ChannelOutboundHandlerAdapter 的子类，主要对写回结果进行加工\n\n打个比喻，每个 Channel 是一个产品的加工车间，Pipeline 是车间中的流水线，ChannelHandler 就是流水线上的各道工序，而后面要讲的 ByteBuf 是原材料，经过很多工序的加工：先经过一道道入站工序，再经过一道道出站工序最终变成产品\n先搞清楚顺序，服务端\nnew ServerBootstrap()\n    .group(new NioEventLoopGroup())\n    .channel(NioServerSocketChannel.class)\n    .childHandler(new ChannelInitializer&lt;NioSocketChannel>() &#123;\n        protected void initChannel(NioSocketChannel ch) &#123;\n            ch.pipeline().addLast(new ChannelInboundHandlerAdapter()&#123;\n                @Override\n                public void channelRead(ChannelHandlerContext ctx, Object msg) &#123;\n                    System.out.println(1);\n                    ctx.fireChannelRead(msg); // 1\n                &#125;\n            &#125;);\n            ch.pipeline().addLast(new ChannelInboundHandlerAdapter()&#123;\n                @Override\n                public void channelRead(ChannelHandlerContext ctx, Object msg) &#123;\n                    System.out.println(2);\n                    ctx.fireChannelRead(msg); // 2\n                &#125;\n            &#125;);\n            ch.pipeline().addLast(new ChannelInboundHandlerAdapter()&#123;\n                @Override\n                public void channelRead(ChannelHandlerContext ctx, Object msg) &#123;\n                    System.out.println(3);\n                    ctx.channel().write(msg); // 3\n                &#125;\n            &#125;);\n            ch.pipeline().addLast(new ChannelOutboundHandlerAdapter()&#123;\n                @Override\n                public void write(ChannelHandlerContext ctx, Object msg, \n                                  ChannelPromise promise) &#123;\n                    System.out.println(4);\n                    ctx.write(msg, promise); // 4\n                &#125;\n            &#125;);\n            ch.pipeline().addLast(new ChannelOutboundHandlerAdapter()&#123;\n                @Override\n                public void write(ChannelHandlerContext ctx, Object msg, \n                                  ChannelPromise promise) &#123;\n                    System.out.println(5);\n                    ctx.write(msg, promise); // 5\n                &#125;\n            &#125;);\n            ch.pipeline().addLast(new ChannelOutboundHandlerAdapter()&#123;\n                @Override\n                public void write(ChannelHandlerContext ctx, Object msg, \n                                  ChannelPromise promise) &#123;\n                    System.out.println(6);\n                    ctx.write(msg, promise); // 6\n                &#125;\n            &#125;);\n        &#125;\n    &#125;)\n    .bind(8080);\n\n客户端\nnew Bootstrap()\n    .group(new NioEventLoopGroup())\n    .channel(NioSocketChannel.class)\n    .handler(new ChannelInitializer&lt;Channel>() &#123;\n        @Override\n        protected void initChannel(Channel ch) &#123;\n            ch.pipeline().addLast(new StringEncoder());\n        &#125;\n    &#125;)\n    .connect(\"127.0.0.1\", 8080)\n    .addListener((ChannelFutureListener) future -> &#123;\n        future.channel().writeAndFlush(\"hello,world\");\n    &#125;);\n\n服务器端打印：\n1\n2\n3\n6\n5\n4\n\n可以看到，ChannelInboundHandlerAdapter 是按照 addLast 的顺序执行的，而 ChannelOutboundHandlerAdapter 是按照 addLast 的逆序执行的。ChannelPipeline 的实现是一个 ChannelHandlerContext（包装了 ChannelHandler） 组成的双向链表\n\n\n入站处理器中，ctx.fireChannelRead(msg) 是 调用下一个入站处理器\n如果注释掉 1 处代码，则仅会打印 1\n如果注释掉 2 处代码，则仅会打印 1 2\n\n\n3 处的 ctx.channel().write(msg) 会 从尾部开始触发 后续出站处理器的执行\n如果注释掉 3 处代码，则仅会打印 1 2 3\n\n\n类似的，出站处理器中，ctx.write(msg, promise) 的调用也会 触发上一个出站处理器\n如果注释掉 6 处代码，则仅会打印 1 2 3 6\n\n\nctx.channel().write(msg) vs ctx.write(msg)\n都是触发出站处理器的执行\nctx.channel().write(msg) 从尾部开始查找出站处理器\nctx.write(msg) 是从当前节点找上一个出站处理器\n3 处的 ctx.channel().write(msg) 如果改为 ctx.write(msg) 仅会打印 1 2 3，因为节点3 之前没有其它出站处理器了\n6 处的 ctx.write(msg, promise) 如果改为 ctx.channel().write(msg) 会打印 1 2 3 6 6 6… 因为 ctx.channel().write() 是从尾部开始查找，结果又是节点6 自己\n\n\n\n图1 - 服务端 pipeline 触发的原始流程，图中数字代表了处理步骤的先后次序\n\nByteBuf是对字节数据的封装\n创建ByteBuf buffer = ByteBufAllocator.DEFAULT.buffer(10);\nlog(buffer);\n\n上面代码创建了一个默认的 ByteBuf（池化基于直接内存的 ByteBuf。直接内存则是在操作系统层面上分配的内存，不受Java虚拟机的管理。），初始容量是 10\n输出\nread index:0 write index:0 capacity:10\n\n其中 log 方法参考如下\nprivate static void log(ByteBuf buffer) &#123;\n    int length = buffer.readableBytes();\n    int rows = length / 16 + (length % 15 == 0 ? 0 : 1) + 4;\n    StringBuilder buf = new StringBuilder(rows * 80 * 2)\n        .append(\"read index:\").append(buffer.readerIndex())\n        .append(\" write index:\").append(buffer.writerIndex())\n        .append(\" capacity:\").append(buffer.capacity())\n        .append(NEWLINE);\n    appendPrettyHexDump(buf, buffer);\n    System.out.println(buf.toString());\n&#125;\n\n直接内存 vs 堆内存可以使用下面的代码来创建池化基于堆的 ByteBuf\nByteBuf buffer = ByteBufAllocator.DEFAULT.heapBuffer(10);\n\n也可以使用下面的代码来创建池化基于直接内存的 ByteBuf\nByteBuf buffer = ByteBufAllocator.DEFAULT.directBuffer(10);\n\n\n直接内存创建和销毁的代价昂贵，但读写性能高（少一次内存复制），适合配合池化功能一起用\n直接内存不被GC，因为这部分内存不受 JVM 垃圾回收的管理，但也要注意及时主动释放\n\n池化 vs 非池化池化的最大意义在于可以重用 ByteBuf，优点有\n\n没有池化，则每次都得创建新的 ByteBuf 实例，这个操作对直接内存代价昂贵，就算是堆内存，也会增加 GC 压力\n有了池化，则可以重用池中 ByteBuf 实例，并且采用了与 jemalloc 类似的内存分配算法提升分配效率\n高并发时，池化功能更节约内存，减少内存溢出的可能\n\n池化功能是否开启，可以通过下面的系统环境变量来设置\n-Dio.netty.allocator.type=&#123;unpooled|pooled&#125;\n\n\n4.1 以后，非 Android 平台默认启用池化实现，Android 平台启用非池化实现\n4.1 之前，池化功能还不成熟，默认是非池化实现\n\n组成ByteBuf 由四部分组成\n\n最开始读写指针都在 0 位置\n写入方法列表，省略一些不重要的方法\n\n\n\n方法签名\n含义\n备注\n\n\n\nwriteBoolean(boolean value)\n写入 boolean 值\n用一字节 01|00 代表 true|false\n\n\nwriteByte(int value)\n写入 byte 值\n\n\n\nwriteShort(int value)\n写入 short 值\n\n\n\nwriteInt(int value)\n写入 int 值\nBig Endian，即 0x250，写入后 00 00 02 50\n\n\nwriteIntLE(int value)\n写入 int 值\nLittle Endian，即 0x250，写入后 50 02 00 00\n\n\nwriteLong(long value)\n写入 long 值\n\n\n\nwriteChar(int value)\n写入 char 值\n\n\n\nwriteFloat(float value)\n写入 float 值\n\n\n\nwriteDouble(double value)\n写入 double 值\n\n\n\nwriteBytes(ByteBuf src)\n写入 netty 的 ByteBuf\n\n\n\nwriteBytes(byte[] src)\n写入 byte[]\n\n\n\nwriteBytes(ByteBuffer src)\n写入 nio 的 ByteBuffer\n\n\n\nint writeCharSequence(CharSequence sequence, Charset charset)\n写入字符串\n\n\n\n\n\n\n\n\n\n\n\n\n注意\n\n这些方法的未指明返回值的，其返回值都是 ByteBuf，意味着可以链式调用\n网络传输，默认习惯是 Big Endian\n\n先写入 4 个字节\nbuffer.writeBytes(new byte[]&#123;1, 2, 3, 4&#125;);\nlog(buffer);\n\n结果是\nread index:0 write index:4 capacity:10\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 01 02 03 04                                     |....            |\n+--------+-------------------------------------------------+----------------+\n\n再写入一个 int 整数，也是 4 个字节\nbuffer.writeInt(5);\nlog(buffer);\n\n结果是\nread index:0 write index:8 capacity:10\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 01 02 03 04 00 00 00 05                         |........        |\n+--------+-------------------------------------------------+----------------+\n\n\n\n还有一类方法是 set 开头的一系列方法，也可以写入数据，但不会改变写指针位置\n扩容再写入一个 int 整数时，容量不够了（初始容量是 10），这时会引发扩容\nbuffer.writeInt(6);\nlog(buffer);\n\n扩容规则是\n\n如何写入后数据大小未超过 512，则选择下一个 16 的整数倍，例如写入后大小为 12 ，则扩容后 capacity 是 16\n如果写入后数据大小超过 512，则选择下一个 2^n，例如写入后大小为 513，则扩容后 capacity 是 2^10&#x3D;1024（2^9&#x3D;512 已经不够了）\n扩容不能超过 max capacity 会报错\n\n结果是\nread index:0 write index:12 capacity:16\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 01 02 03 04 00 00 00 05 00 00 00 06             |............    |\n+--------+-------------------------------------------------+----------------+\n\n读取例如读了 4 次，每次一个字节\nSystem.out.println(buffer.readByte());\nSystem.out.println(buffer.readByte());\nSystem.out.println(buffer.readByte());\nSystem.out.println(buffer.readByte());\nlog(buffer);\n\n读过的内容，就属于废弃部分了，再读只能读那些尚未读取的部分\n1\n2\n3\n4\nread index:4 write index:12 capacity:16\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 00 00 00 05 00 00 00 06                         |........        |\n+--------+-------------------------------------------------+----------------+\n\n如果需要重复读取 int 整数 5，怎么办？\n可以在 read 前先做个标记 mark\nbuffer.markReaderIndex();\nSystem.out.println(buffer.readInt());\nlog(buffer);\n\n结果\n5\nread index:8 write index:12 capacity:16\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 00 00 00 06                                     |....            |\n+--------+-------------------------------------------------+----------------+\n\n这时要重复读取的话，重置到标记位置 reset\nbuffer.resetReaderIndex();\nlog(buffer);\n\n这时\nread index:4 write index:12 capacity:16\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 00 00 00 05 00 00 00 06                         |........        |\n+--------+-------------------------------------------------+----------------+\n\n还有种办法是采用 get 开头的一系列方法，这些方法不会改变 read index\nretain &amp; release由于 Netty 中有堆外内存的 ByteBuf 实现，堆外内存最好是手动来释放，而不是等 GC 垃圾回收。\n\nUnpooledHeapByteBuf 使用的是 JVM 内存，只需等 GC 回收内存即可\nUnpooledDirectByteBuf 使用的就是直接内存了，需要特殊的方法来回收内存\nPooledByteBuf 和它的子类使用了池化机制，需要更复杂的规则来回收内存\n\n\n\n\n\n\n\n\n\n\n回收内存的源码实现，请关注下面方法的不同实现\nprotected abstract void deallocate()\nNetty 这里采用了引用计数法来控制回收内存，每个 ByteBuf 都实现了 ReferenceCounted 接口\n\n每个 ByteBuf 对象的初始计数为 1\n调用 release 方法计数减 1，如果计数为 0，ByteBuf 内存被回收\n调用 retain 方法计数加 1，表示调用者没用完之前，其它 handler 即使调用了 release 也不会造成回收\n当计数为 0 时，底层内存会被回收，这时即使 ByteBuf 对象还在，其各个方法均无法正常使用\n\n谁来负责 release 呢？\n不是我们想象的（一般情况下）\nByteBuf buf = ...\ntry &#123;\n    ...\n&#125; finally &#123;\n    buf.release();\n&#125;\n\n请思考，因为 pipeline 的存在，一般需要将 ByteBuf 传递给下一个 ChannelHandler，如果在 finally 中 release 了，就失去了传递性（当然，如果在这个 ChannelHandler 内这个 ByteBuf 已完成了它的使命，那么便无须再传递）\n基本规则是，谁是最后使用者，谁负责 release，详细分析如下\n\n起点，对于 NIO 实现来讲，在 io.netty.channel.nio.AbstractNioByteChannel.NioByteUnsafe#read 方法中首次创建 ByteBuf 放入 pipeline（line 163 pipeline.fireChannelRead(byteBuf)）\n入站 ByteBuf 处理原则\n对原始 ByteBuf 不做处理，调用 ctx.fireChannelRead(msg) 向后传递，这时无须 release\n将原始 ByteBuf 转换为其它类型的 Java 对象，这时 ByteBuf 就没用了，必须 release\n如果不调用 ctx.fireChannelRead(msg) 向后传递，那么也必须 release\n注意各种异常，如果 ByteBuf 没有成功传递到下一个 ChannelHandler，必须 release\n假设消息一直向后传，那么 TailContext 会负责释放未处理消息（原始的 ByteBuf）\n\n\n出站 ByteBuf 处理原则\n出站消息最终都会转为 ByteBuf 输出，一直向前传，由 HeadContext flush 后 release\n\n\n异常处理原则\n有时候不清楚 ByteBuf 被引用了多少次，但又必须彻底释放，可以循环调用 release 直到返回 true\n\n\n\nTailContext 释放未处理消息逻辑\n// io.netty.channel.DefaultChannelPipeline#onUnhandledInboundMessage(java.lang.Object)\nprotected void onUnhandledInboundMessage(Object msg) &#123;\n    try &#123;\n        logger.debug(\n            \"Discarded inbound message &#123;&#125; that reached at the tail of the pipeline. \" +\n            \"Please check your pipeline configuration.\", msg);\n    &#125; finally &#123;\n        ReferenceCountUtil.release(msg);\n    &#125;\n&#125;\n\n具体代码\n// io.netty.util.ReferenceCountUtil#release(java.lang.Object)\npublic static boolean release(Object msg) &#123;\n    if (msg instanceof ReferenceCounted) &#123;\n        return ((ReferenceCounted) msg).release();\n    &#125;\n    return false;\n&#125;\n\nslice【零拷贝】的体现之一，对原始 ByteBuf 进行切片成多个 ByteBuf，切片后的 ByteBuf 并没有发生内存复制，还是使用原始 ByteBuf 的内存，切片后的 ByteBuf 维护独立的 read，write 指针\n\n例，原始 ByteBuf 进行一些初始操作\nByteBuf origin = ByteBufAllocator.DEFAULT.buffer(10);\norigin.writeBytes(new byte[]&#123;1, 2, 3, 4&#125;);\norigin.readByte();\nSystem.out.println(ByteBufUtil.prettyHexDump(origin));\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 02 03 04                                        |...             |\n+--------+-------------------------------------------------+----------------+\n\n这时调用 slice 进行切片，无参 slice 是从原始 ByteBuf 的 read index 到 write index 之间的内容进行切片，切片后的 max capacity 被固定为这个区间的大小，因此不能追加 write\nByteBuf slice = origin.slice();\nSystem.out.println(ByteBufUtil.prettyHexDump(slice));\n// slice.writeByte(5); 如果执行，会报 IndexOutOfBoundsException 异常\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 02 03 04                                        |...             |\n+--------+-------------------------------------------------+----------------+\n\n如果原始 ByteBuf 再次读操作（又读了一个字节）\norigin.readByte();\nSystem.out.println(ByteBufUtil.prettyHexDump(origin));\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 03 04                                           |..              |\n+--------+-------------------------------------------------+----------------+\n\n这时的 slice 不受影响，因为它有独立的读写指针\nSystem.out.println(ByteBufUtil.prettyHexDump(slice));\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 02 03 04                                        |...             |\n+--------+-------------------------------------------------+----------------+\n\n如果 slice 的内容发生了更改\nslice.setByte(2, 5);\nSystem.out.println(ByteBufUtil.prettyHexDump(slice));\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 02 03 05                                        |...             |\n+--------+-------------------------------------------------+----------------+\n\n这时，原始 ByteBuf 也会受影响，因为底层都是同一块内存\nSystem.out.println(ByteBufUtil.prettyHexDump(origin));\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 03 05                                           |..              |\n+--------+-------------------------------------------------+----------------+\n\n\n\n10）duplicate【零拷贝】的体现之一，就好比截取了原始 ByteBuf 所有内容，并且没有 max capacity 的限制，也是与原始 ByteBuf 使用同一块底层内存，只是读写指针是独立的\n\n11）copy会将底层内存数据进行深拷贝，因此无论读写，都与原始 ByteBuf 无关\n12）CompositeByteBuf【零拷贝】的体现之一，可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf，避免拷贝\n有两个 ByteBuf 如下\nByteBuf buf1 = ByteBufAllocator.DEFAULT.buffer(5);\nbuf1.writeBytes(new byte[]&#123;1, 2, 3, 4, 5&#125;);\nByteBuf buf2 = ByteBufAllocator.DEFAULT.buffer(5);\nbuf2.writeBytes(new byte[]&#123;6, 7, 8, 9, 10&#125;);\nSystem.out.println(ByteBufUtil.prettyHexDump(buf1));\nSystem.out.println(ByteBufUtil.prettyHexDump(buf2));\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 01 02 03 04 05                                  |.....           |\n+--------+-------------------------------------------------+----------------+\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 06 07 08 09 0a                                  |.....           |\n+--------+-------------------------------------------------+----------------+\n\n现在需要一个新的 ByteBuf，内容来自于刚才的 buf1 和 buf2，如何实现？\n方法1：\nByteBuf buf3 = ByteBufAllocator.DEFAULT\n    .buffer(buf1.readableBytes()+buf2.readableBytes());\nbuf3.writeBytes(buf1);\nbuf3.writeBytes(buf2);\nSystem.out.println(ByteBufUtil.prettyHexDump(buf3));\n\n结果\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 01 02 03 04 05 06 07 08 09 0a                   |..........      |\n+--------+-------------------------------------------------+----------------+\n\n这种方法好不好？回答是不太好，因为进行了数据的内存复制操作\n方法2：\nCompositeByteBuf buf3 = ByteBufAllocator.DEFAULT.compositeBuffer();\n// true 表示增加新的 ByteBuf 自动递增 write index, 否则 write index 会始终为 0\nbuf3.addComponents(true, buf1, buf2);\n\n结果是一样的\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 01 02 03 04 05 06 07 08 09 0a                   |..........      |\n+--------+-------------------------------------------------+----------------+\n\nCompositeByteBuf 是一个组合的 ByteBuf，它内部维护了一个 Component 数组，每个 Component 管理一个 ByteBuf，记录了这个 ByteBuf 相对于整体偏移量等信息，代表着整体中某一段的数据。\n\n优点，对外是一个虚拟视图，组合这些 ByteBuf 不会产生内存复制\n缺点，复杂了很多，多次操作会带来性能的损耗\n\nUnpooledUnpooled 是一个工具类，类如其名，提供了非池化的 ByteBuf 创建、组合、复制等操作\n【零拷贝】相关的 wrappedBuffer 方法，可以用来包装 ByteBuf\nByteBuf buf1 = ByteBufAllocator.DEFAULT.buffer(5);\nbuf1.writeBytes(new byte[]&#123;1, 2, 3, 4, 5&#125;);\nByteBuf buf2 = ByteBufAllocator.DEFAULT.buffer(5);\nbuf2.writeBytes(new byte[]&#123;6, 7, 8, 9, 10&#125;);\n\n// 当包装 ByteBuf 个数超过一个时, 底层使用了 CompositeByteBuf\nByteBuf buf3 = Unpooled.wrappedBuffer(buf1, buf2);\nSystem.out.println(ByteBufUtil.prettyHexDump(buf3));\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 01 02 03 04 05 06 07 08 09 0a                   |..........      |\n+--------+-------------------------------------------------+----------------+\n\n也可以用来包装普通字节数组，底层也不会有拷贝操作\nByteBuf buf4 = Unpooled.wrappedBuffer(new byte[]&#123;1, 2, 3&#125;, new byte[]&#123;4, 5, 6&#125;);\nSystem.out.println(buf4.getClass());\nSystem.out.println(ByteBufUtil.prettyHexDump(buf4));\n\n输出\nclass io.netty.buffer.CompositeByteBuf\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 01 02 03 04 05 06                               |......          |\n+--------+-------------------------------------------------+----------------+\n\n\n\nByteBuf 优势\n池化 - 可以重用池中 ByteBuf 实例，更节约内存，减少内存溢出的可能\n读写指针分离，不需要像 ByteBuffer 一样切换读写模式\n可以自动扩容\n支持链式调用，使用更流畅\n很多地方体现零拷贝，例如 slice、duplicate、CompositeByteBuf\n\n双向通信实现一个 echo server编写 server\nnew ServerBootstrap()\n    .group(new NioEventLoopGroup())\n    .channel(NioServerSocketChannel.class)\n    .childHandler(new ChannelInitializer&lt;NioSocketChannel>() &#123;\n        @Override\n        protected void initChannel(NioSocketChannel ch) &#123;\n            ch.pipeline().addLast(new ChannelInboundHandlerAdapter()&#123;\n                @Override\n                public void channelRead(ChannelHandlerContext ctx, Object msg) &#123;\n                    ByteBuf buffer = (ByteBuf) msg;\n                    System.out.println(buffer.toString(Charset.defaultCharset()));\n\n                    // 建议使用 ctx.alloc() 创建 ByteBuf\n                    //ctx.alloc().buffer()创建的ByteBuf对象初始容量为0，当我们往这个ByteBuf写入数据后，它会自动进行扩容。我们可以通过ctx.alloc().buffer(int initialCapacity)方法，来指定创建一个初始容量为initialCapacity的ByteBuf对象。\n                    ByteBuf response = ctx.alloc().buffer();\n                    response.writeBytes(buffer);\n                    ctx.writeAndFlush(response);\n\t\t\t\t\t\n                    // 思考：需要释放 buffer 吗\n                    // 思考：需要释放 response 吗\n                &#125;\n            &#125;);\n        &#125;\n    &#125;).bind(8080);\n\n编写 client\nNioEventLoopGroup group = new NioEventLoopGroup();\nChannel channel = new Bootstrap()\n    .group(group)\n    .channel(NioSocketChannel.class)\n    .handler(new ChannelInitializer&lt;NioSocketChannel>() &#123;\n        @Override\n        protected void initChannel(NioSocketChannel ch) throws Exception &#123;\n            ch.pipeline().addLast(new StringEncoder());\n            ch.pipeline().addLast(new ChannelInboundHandlerAdapter() &#123;\n                @Override\n                public void channelRead(ChannelHandlerContext ctx, Object msg) &#123;\n                    ByteBuf buffer = (ByteBuf) msg;\n                    System.out.println(buffer.toString(Charset.defaultCharset()));\n\n                    // 思考：需要释放 buffer 吗\n                &#125;\n            &#125;);\n        &#125;\n    &#125;).connect(\"127.0.0.1\", 8080).sync().channel();\n\nchannel.closeFuture().addListener(future -> &#123;\n    group.shutdownGracefully();\n&#125;);\n\nnew Thread(() -> &#123;\n    Scanner scanner = new Scanner(System.in);\n    while (true) &#123;\n        String line = scanner.nextLine();\n        if (\"q\".equals(line)) &#123;\n            channel.close();\n            break;\n        &#125;\n        channel.writeAndFlush(line);\n    &#125;\n&#125;).start();\n\n读和写的理解不是只有在 netty，nio 这样的多路复用 IO 模型时，读写才不会相互阻塞，才可以实现高效的双向通信，但实际上，Java Socket 是全双工的：在任意时刻，线路上存在A 到 B 和 B 到 A 的双向信号传输。即使是阻塞 IO，读和写是可以同时进行的，只要分别采用读线程和写线程即可，读不会阻塞写、写也不会阻塞读\n例如\npublic class TestServer &#123;\n    public static void main(String[] args) throws IOException &#123;\n        ServerSocket ss = new ServerSocket(8888);\n        Socket s = ss.accept();\n\n        new Thread(() -> &#123;\n            try &#123;\n                BufferedReader reader = new BufferedReader(new InputStreamReader(s.getInputStream()));\n                while (true) &#123;\n                    System.out.println(reader.readLine());\n                &#125;\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;).start();\n\n        new Thread(() -> &#123;\n            try &#123;\n                BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(s.getOutputStream()));\n                // 例如在这个位置加入 thread 级别断点，可以发现即使不写入数据，也不妨碍前面线程读取客户端数据\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    writer.write(String.valueOf(i));\n                    writer.newLine();\n                    writer.flush();\n                &#125;\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;).start();\n    &#125;\n&#125;\n\n客户端\npublic class TestClient &#123;\n    public static void main(String[] args) throws IOException &#123;\n        Socket s = new Socket(\"localhost\", 8888);\n\n        new Thread(() -> &#123;\n            try &#123;\n                BufferedReader reader = new BufferedReader(new InputStreamReader(s.getInputStream()));\n                while (true) &#123;\n                    System.out.println(reader.readLine());\n                &#125;\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;).start();\n\n        new Thread(() -> &#123;\n            try &#123;\n                BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(s.getOutputStream()));\n                for (int i = 0; i &lt; 100; i++) &#123;\n                    writer.write(String.valueOf(i));\n                    writer.newLine();\n                    writer.flush();\n                &#125;\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;).start();\n    &#125;\n&#125;\n\n\n\n\n\n\n\n\n\n","slug":"Netty基础","date":"2023-05-26T02:36:45.000Z","categories_index":"","tags_index":"Netty","author_index":"大宝贝的程序员"},{"id":"c9f2b21b690f4db6367244e90c766293","title":"NIO","content":"NIO 基础non-blocking io 非阻塞 IO三大组件Channel &amp; Bufferchannel 有一点类似于 stream，它就是读写数据的双向通道，可以从 channel 将数据读入 buffer，也可以将 buffer 的数据写入 channel，而之前的 stream 要么是输入，要么是输出，channel 比 stream 更为底层\n常见的 Channel 有\n\nFileChannel\nDatagramChannel\nSocketChannel\nServerSocketChannel\n\nbuffer 则用来缓冲读写数据，常见的 buffer 有\n\nByteBuffer\nMappedByteBuffer\nDirectByteBuffer\nHeapByteBuffer\n\n\nShortBuffer\nIntBuffer\nLongBuffer\nFloatBuffer\nDoubleBuffer\nCharBuffer\n\nSelector多线程版设计\ngraph TD\nsubgraph 多线程版\nt1(thread) --> s1(socket1)\nt2(thread) --> s2(socket2)\nt3(thread) --> s3(socket3)\nend\n\n 多线程版缺点:\n\n内存占用高\n线程上下文切换成本高\n只适合连接数少的场景\n\n线程池版设计\ngraph TD\nsubgraph 线程池版\nt4(thread) --> s4(socket1)\nt5(thread) --> s5(socket2)\nt4(thread) -.-> s6(socket3)\nt5(thread) -.-> s7(socket4)\nend\n\n线程池版缺点\n\n阻塞模式下，线程仅能处理一个 socket 连接\n仅适合短连接场景\n\nselector 版设计\nselector 的作用就是配合一个线程来管理多个 channel，获取这些 channel 上发生的事件，这些 channel 工作在非阻塞模式下，不会让线程吊死在一个 channel 上。适合连接数特别多，但流量低的场景（low traffic）\ngraph TD\nsubgraph selector 版\nthread --> selector\nselector --> c1(channel)\nselector --> c2(channel)\nselector --> c3(channel)\nend\n\n调用 selector 的 select() 会阻塞直到 channel 发生了读写就绪事件，这些事件发生，select 方法就会返回这些事件交给 thread 来处理\nByteBuffer有一普通文本文件 data.txt，内容为\n1234567890abcd\n\n使用 FileChannel 来读取文件内容\n@Slf4j\npublic class ChannelDemo1 &#123;\n    public static void main(String[] args) &#123;\n        try (RandomAccessFile file = new RandomAccessFile(\"helloword/data.txt\", \"rw\")) &#123;\n            FileChannel channel = file.getChannel();\n            ByteBuffer buffer = ByteBuffer.allocate(10);\n            do &#123;\n                // 向 buffer 写入\n                int len = channel.read(buffer);\n                log.debug(\"读到字节数：&#123;&#125;\", len);\n                if (len == -1) &#123;\n                    break;\n                &#125;\n                // 切换 buffer 读模式\n                buffer.flip();\n                while(buffer.hasRemaining()) &#123;\n                    log.debug(\"&#123;&#125;\", (char)buffer.get());\n                &#125;\n                // 切换 buffer 写模式\n                buffer.clear();\n            &#125; while (true);\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n输出\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：10\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 1\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 2\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 3\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 4\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 5\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 6\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 7\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 8\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 9\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 0\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：4\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - a\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - b\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - c\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - d\n10:39:03 [DEBUG] [main] c.i.n.ChannelDemo1 - 读到字节数：-1\n\nByteBuffer 正确使用姿势\n向 buffer 写入数据，例如调用 channel.read(buffer)\n调用 flip() 切换至读模式\n从 buffer 读取数据，例如调用 buffer.get()\n调用 clear() 或 compact() 切换至写模式\n重复 1~4 步骤\n\nByteBuffer buffer &#x3D; ByteBuffer.allocate(10);\nByteBuffer 结构ByteBuffer 有以下重要属性\n\ncapacity\nposition\nlimit\n\n一开始\tByteBuffer buffer &#x3D; ByteBuffer.allocate(10);\n\n写模式下，position 是写入位置，limit 等于容量，下图表示写入了 4 个字节后的状态\n\nflip 动作发生后，position 切换为读取位置，limit 切换为读取限制\n\n读取 4 个字节后，状态\n\nclear 动作发生后，状态\n\ncompact 方法，是把未读完的部分向前压缩，然后切换至写模式\n\nByteBuffer 常见方法分配空间\n可以使用 allocate 方法为 ByteBuffer 分配空间，其它 buffer 类也有该方法\nBytebuffer buf = ByteBuffer.allocate(16);\n向 buffer 写入数据\n有两种办法\n\n调用 channel 的 read 方法\n\nint readBytes = channel.read(buf);\n\n调用 buffer 自己的 put 方法\n\nbuf.put((byte)127);\n从 buffer 读取数据\n有两种办法\n\n调用 channel 的 write 方法\n\nint writeBytes = channel.write(buf);\n\n调用 buffer 自己的 get 方法\n\nbyte b = buf.get();\nget 方法会让 position 读指针向后走，如果想重复读取数据\n\n可以调用 rewind 方法将 position 重新置为 0\n或者调用 get(int i) 方法获取索引 i 的内容，它不会移动读指针\n\nmark 和 reset\nmark 是在读取时，做一个标记，即使 position 改变，只要调用 reset 就能回到 mark 的位置\n💡 rewind 和 flip 都会清除 mark 位置\n字符串与 ByteBuffer 互转\nByteBuffer buffer1 = StandardCharsets.UTF_8.encode(\"你好\");\nByteBuffer buffer2 = Charset.forName(\"utf-8\").encode(\"你好\");\n\ndebug(buffer1);\ndebug(buffer2);\n\nCharBuffer buffer3 = StandardCharsets.UTF_8.decode(buffer1);\nSystem.out.println(buffer3.getClass());\nSystem.out.println(buffer3.toString());\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| e4 bd a0 e5 a5 bd                               |......          |\n+--------+-------------------------------------------------+----------------+\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| e4 bd a0 e5 a5 bd                               |......          |\n+--------+-------------------------------------------------+----------------+\nclass java.nio.HeapCharBuffer\n你好\n\nBuffer 是非线程安全的\nScattering Reads 分散读分散读取，有一个文本文件 3parts.txt\nonetwothree\n\n使用如下方式读取，可以将数据填充至多个 buffer\ntry (RandomAccessFile file = new RandomAccessFile(\"3parts.txt\", \"rw\")) &#123;\n    FileChannel channel = file.getChannel();\n    ByteBuffer a = ByteBuffer.allocate(3);\n    ByteBuffer b = ByteBuffer.allocate(3);\n    ByteBuffer c = ByteBuffer.allocate(5);\n    channel.read(new ByteBuffer[]&#123;a, b, c&#125;);\n    a.flip();\n    b.flip();\n    c.flip();\n    debug(a);\n    debug(b);\n    debug(c);\n&#125; catch (IOException e) &#123;\n    e.printStackTrace();\n&#125;\n\n结果\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 6f 6e 65                                        |one             |\n+--------+-------------------------------------------------+----------------+\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 74 77 6f                                        |two             |\n+--------+-------------------------------------------------+----------------+\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 74 68 72 65 65                                  |three           |\n+--------+-------------------------------------------------+----------------+\n\nGathering Writes聚集再写使用如下方式写入，可以将多个 buffer 的数据填充至 channel\ntry (RandomAccessFile file = new RandomAccessFile(\"3parts.txt\", \"rw\")) &#123;\n    FileChannel channel = file.getChannel();\n    ByteBuffer d = ByteBuffer.allocate(4);\n    ByteBuffer e = ByteBuffer.allocate(4);\n    //原文件有10个字，从第11个开始\n    channel.position(11);\n\n    d.put(new byte[]&#123;'f', 'o', 'u', 'r'&#125;);\n    e.put(new byte[]&#123;'f', 'i', 'v', 'e'&#125;);\n    d.flip();\n    e.flip();\n    debug(d);\n    debug(e);\n    channel.write(new ByteBuffer[]&#123;d, e&#125;);\n&#125; catch (IOException e) &#123;\n    e.printStackTrace();\n&#125;\n\n输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 66 6f 75 72                                     |four            |\n+--------+-------------------------------------------------+----------------+\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 66 69 76 65                                     |five            |\n+--------+-------------------------------------------------+----------------+\n\n文件内容\nonetwothreefourfive\n\n文件编程FileChannelFileChannel 只能工作在阻塞模式下\n获取\n不能直接打开 FileChannel，必须通过 FileInputStream、FileOutputStream 或者 RandomAccessFile 来获取 FileChannel，它们都有 getChannel 方法\n\n通过 FileInputStream 获取的 channel 只能读\n通过 FileOutputStream 获取的 channel 只能写\n通过 RandomAccessFile 是否能读写根据构造 RandomAccessFile 时的读写模式决定\n\n读取\n会从 channel 读取数据填充 ByteBuffer，返回值表示读到了多少字节，-1 表示到达了文件的末尾\n写入\n写入的正确姿势如下， SocketChannel\nByteBuffer buffer = ...;\nbuffer.put(...); // 存入数据\nbuffer.flip();   // 切换读模式\n\nwhile(buffer.hasRemaining()) &#123;\n    channel.write(buffer);\n&#125;\n\n在 while 中调用 channel.write 是因为 write 方法并不能保证一次将 buffer 中的内容全部写入 channel\n关闭\nchannel 必须关闭，不过调用了 FileInputStream、FileOutputStream 或者 RandomAccessFile 的 close 方法会间接地调用 channel 的 close 方法\n位置获取当前位置\nlong pos = channel.position();\n\n设置当前位置\nlong newPos = ...;\nchannel.position(newPos);\n\n设置当前位置时，如果设置为文件的末尾\n\n这时读取会返回 -1 \n这时写入，会追加内容，但要注意如果 position 超过了文件末尾，再写入时在新内容和原末尾之间会有空洞（00）\n\n强制写入操作系统出于性能的考虑，会将数据缓存，不是立刻写入磁盘。可以调用 force(true)  方法将文件内容和元数据（文件的权限等信息）立刻写入磁盘\n两个 Channel 传输数据String FROM = \"data.txt\";\nString TO = \"to.txt\";\nlong start = System.nanoTime();\ntry (FileChannel from = new FileInputStream(FROM).getChannel();\n     FileChannel to = new FileOutputStream(TO).getChannel();\n    ) &#123;\n    from.transferTo(0, from.size(), to);\n&#125; catch (IOException e) &#123;\n    e.printStackTrace();\n&#125;\nlong end = System.nanoTime();\nSystem.out.println(\"transferTo 用时：\" + (end - start) / 1000_000.0);\n\n输出\ntransferTo 用时：8.2011\n\n\n\n超过 2g 大小的文件传输\npublic class TestFileChannelTransferTo &#123;\n    public static void main(String[] args) &#123;\n        try (\n                FileChannel from = new FileInputStream(\"data.txt\").getChannel();\n                FileChannel to = new FileOutputStream(\"to.txt\").getChannel();\n        ) &#123;\n            // 效率高，底层会利用操作系统的零拷贝进行优化\n            long size = from.size();\n            // left 变量代表还剩余多少字节\n            for (long left = size; left > 0; ) &#123;\n                System.out.println(\"position:\" + (size - left) + \" left:\" + left);\n                left -= from.transferTo((size - left), left, to);\n            &#125;\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n实际传输一个超大文件\nposition:0 left:7769948160\nposition:2147483647 left:5622464513\nposition:4294967294 left:3474980866\nposition:6442450941 left:1327497219\n\nPathjdk7 引入了 Path 和 Paths 类\n\nPath 用来表示文件路径\nPaths 是工具类，用来获取 Path 实例\n\nPath source = Paths.get(\"1.txt\"); // 相对路径 使用 user.dir 环境变量来定位 1.txt\n\nPath source = Paths.get(\"d:\\\\1.txt\"); // 绝对路径 代表了  d:\\1.txt\n\nPath source = Paths.get(\"d:/1.txt\"); // 绝对路径 同样代表了  d:\\1.txt\n\nPath projects = Paths.get(\"d:\\\\data\", \"projects\"); // 代表了  d:\\data\\projects\n\n\n. 代表了当前路径\n.. 代表了上一级路径\n\n例如目录结构如下\nd:\n\t|- data\n\t\t|- projects\n\t\t\t|- a\n\t\t\t|- b\n\n代码\nPath path = Paths.get(\"d:\\\\data\\\\projects\\\\a\\\\..\\\\b\");\nSystem.out.println(path);\nSystem.out.println(path.normalize()); // 正常化路径\n\n会输出\nd:\\data\\projects\\a\\..\\b\nd:\\data\\projects\\b\n\nFiles检查文件是否存在\nPath path = Paths.get(\"helloword/data.txt\");\nSystem.out.println(Files.exists(path));\n\n\n\n创建一级目录\nPath path = Paths.get(\"helloword/d1\");\nFiles.createDirectory(path);\n\n\n如果目录已存在，会抛异常 FileAlreadyExistsException\n不能一次创建多级目录，否则会抛异常 NoSuchFileException\n\n创建多级目录用\nPath path = Paths.get(\"helloword/d1/d2\");\nFiles.createDirectories(path);\n\n\n\n拷贝文件\nPath source = Paths.get(\"helloword/data.txt\");\nPath target = Paths.get(\"helloword/target.txt\");\n\nFiles.copy(source, target);\n\n\n如果文件已存在，会抛异常 FileAlreadyExistsException\n\n如果希望用 source 覆盖掉 target，需要用 StandardCopyOption 来控制\nFiles.copy(source, target, StandardCopyOption.REPLACE_EXISTING);\n\n\n\n移动文件\nPath source = Paths.get(\"helloword/data.txt\");\nPath target = Paths.get(\"helloword/data.txt\");\n\nFiles.move(source, target, StandardCopyOption.ATOMIC_MOVE);\n\n\nStandardCopyOption.ATOMIC_MOVE 保证文件移动的原子性\n\n删除文件\nPath target = Paths.get(\"helloword/target.txt\");\n\nFiles.delete(target);\n\n\n如果文件不存在，会抛异常 NoSuchFileException\n\n删除目录\nPath target = Paths.get(\"helloword/d1\");\n\nFiles.delete(target);\n\n\n如果目录还有内容，会抛异常 DirectoryNotEmptyException\n\n网络编程非阻塞 vs 阻塞阻塞\n阻塞模式下，相关方法都会导致线程暂停\nServerSocketChannel.accept 会在没有连接建立时让线程暂停\nSocketChannel.read 会在没有数据可读时让线程暂停\n阻塞的表现其实就是线程暂停了，暂停期间不会占用 cpu，但线程相当于闲置\n\n\n单线程下，阻塞方法之间相互影响，几乎不能正常工作，需要多线程支持\n但多线程下，有新的问题，体现在以下方面\n32 位 jvm 一个线程 320k，64 位 jvm 一个线程 1024k，如果连接数过多，必然导致 OOM，并且线程太多，反而会因为频繁上下文切换导致性能降低\n可以采用线程池技术来减少线程数和线程上下文切换，但治标不治本，如果有很多连接建立，但长时间 inactive，会阻塞线程池中所有线程，因此不适合长连接，只适合短连接\n\n\n\n使用 nio 来理解阻塞模式, 单线程\n服务器端\n// 0. ByteBuffer\nByteBuffer buffer = ByteBuffer.allocate(16);\n// 1. 创建了服务器\nServerSocketChannel ssc = ServerSocketChannel.open();\n\n// 2. 绑定监听端口\nssc.bind(new InetSocketAddress(8080));\n\n// 3. 连接集合\nList&lt;SocketChannel> channels = new ArrayList&lt;>();\nwhile (true) &#123;\n    // 4. accept 建立与客户端连接， SocketChannel 用来与客户端之间通信\n    log.debug(\"connecting...\");\n    SocketChannel sc = ssc.accept(); // 阻塞方法，线程停止运行\n    log.debug(\"connected... &#123;&#125;\", sc);\n    channels.add(sc);\n    for (SocketChannel channel : channels) &#123;\n        // 5. 接收客户端发送的数据\n        log.debug(\"before read... &#123;&#125;\", channel);\n        channel.read(buffer); // 阻塞方法，线程停止运行\n        buffer.flip();\n        debugRead(buffer);\n        buffer.clear();\n        log.debug(\"after read...&#123;&#125;\", channel);\n    &#125;\n&#125;\n\n客户端\nSocketChannel sc = SocketChannel.open();\nsc.connect(new InetSocketAddress(\"localhost\", 8080));\nSystem.out.println(\"waiting...\");\n\n非阻塞\n非阻塞模式下，相关方法都不会让线程暂停\n在 ServerSocketChannel.accept 在没有连接建立时，会返回 null，继续运行\nSocketChannel.read 在没有数据可读时，会返回 0，线程不必阻塞，可以去执行其它 SocketChannel 的 read 或是去执行 ServerSocketChannel.accept \n写数据时，线程只等待数据写入 Channel 即可，无需等 Channel 通过网络把数据发送出去\n\n\n但非阻塞模式下，即使没有连接建立和可读数据，线程仍然在不断运行，白白浪费了 cpu\n数据复制过程中，线程实际还是阻塞的（AIO 改进的地方）\n\n使用 nio 来理解非阻塞模式, 单线程\n服务器端，客户端代码不变\n// 0. ByteBuffer\nByteBuffer buffer = ByteBuffer.allocate(16);\n// 1. 创建了服务器\nServerSocketChannel ssc = ServerSocketChannel.open();\nssc.configureBlocking(false); // 非阻塞模式\n// 2. 绑定监听端口\nssc.bind(new InetSocketAddress(8080));\n// 3. 连接集合\nList&lt;SocketChannel> channels = new ArrayList&lt;>();\nwhile (true) &#123;\n    // 4. accept 建立与客户端连接， SocketChannel 用来与客户端之间通信\n    SocketChannel sc = ssc.accept(); // 非阻塞，线程还会继续运行，如果没有连接建立，但sc是null\n    if (sc != null) &#123;\n        log.debug(\"connected... &#123;&#125;\", sc);\n        sc.configureBlocking(false); // 非阻塞模式\n        channels.add(sc);\n    &#125;\n    for (SocketChannel channel : channels) &#123;\n        // 5. 接收客户端发送的数据\n        int read = channel.read(buffer);//非阻塞，线程仍然会继续运行，没有读到数据，read 返回 0\n        if (read > 0) &#123;\n            buffer.flip();\n            debugRead(buffer);\n            buffer.clear();\n            log.debug(\"after read...&#123;&#125;\", channel);\n        &#125;\n    &#125;\n&#125;\n\n\n\n多路复用单线程可以配合 Selector 完成对多个 Channel 可读写事件的监控，这称之为多路复用\n\n多路复用仅针对网络 IO、普通文件 IO 没法利用多路复用\n如果不用 Selector 的非阻塞模式，线程大部分时间都在做无用功，而 Selector 能够保证\n有可连接事件时才去连接\n有可读事件才去读取\n有可写事件才去写入\n限于网络传输能力，Channel 未必时时可写，一旦 Channel 可写，会触发 Selector 的可写事件\n\n\n\n\n\nSelector\ngraph TD\nsubgraph selector 版\nthread --> selector\nselector --> c1(channel)\nselector --> c2(channel)\nselector --> c3(channel)\nend\n\n好处\n\n一个线程配合 selector 就可以监控多个 channel 的事件，事件发生线程才去处理。避免非阻塞模式下所做无用功\n让这个线程能够被充分利用\n节约了线程的数量\n减少了线程上下文切换\n\n创建Selector selector = Selector.open();\n\n绑定 Channel 事件也称之为注册事件，绑定的事件 selector 才会关心 \nchannel.configureBlocking(false);\nSelectionKey key = channel.register(selector, 绑定事件);\n\n\nchannel 必须工作在非阻塞模式\nFileChannel 没有非阻塞模式，因此不能配合 selector 一起使用\n绑定的事件类型可以有\nconnect - 客户端连接成功时触发\naccept - 服务器端成功接受连接时触发\nread - 数据可读入时触发，有因为接收能力弱，数据暂不能读入的情况\nwrite - 数据可写出时触发，有因为发送能力弱，数据暂不能写出的情况\n\n\n\n监听 Channel 事件可以通过下面三种方法来监听是否有事件发生，方法的返回值代表有多少 channel 发生了事件\n方法1，阻塞直到绑定事件发生\nint count = selector.select();\n\n方法2，阻塞直到绑定事件发生，或是超时（时间单位为 ms）\nint count = selector.select(long timeout);\n\n方法3，不会阻塞，也就是不管有没有事件，立刻返回，自己根据返回值检查是否有事件\nint count = selector.selectNow();\n\nselect 何时不阻塞\n事件发生时\n客户端发起连接请求，会触发 accept 事件\n客户端发送数据过来，客户端正常、异常关闭时，都会触发 read 事件，另外如果发送的数据大于 buffer 缓冲区，会触发多次读取事件\nchannel 可写，会触发 write 事件\n在 linux 下 nio bug 发生时\n\n\n调用 selector.wakeup()\n调用 selector.close()\nselector 所在线程 interrupt\n\n处理 accept 事件客户端代码为\npublic class Client &#123;\n    public static void main(String[] args) &#123;\n        try (Socket socket = new Socket(\"localhost\", 8080)) &#123;\n            System.out.println(socket);\n            socket.getOutputStream().write(\"world\".getBytes());\n            System.in.read();\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n\n\n服务器端代码为\n@Slf4j\npublic class ChannelDemo6 &#123;\n    public static void main(String[] args) &#123;\n        try (ServerSocketChannel channel = ServerSocketChannel.open()) &#123;\n            channel.bind(new InetSocketAddress(8080));\n            System.out.println(channel);\n            Selector selector = Selector.open();\n            channel.configureBlocking(false);\n            channel.register(selector, SelectionKey.OP_ACCEPT);\n\n            while (true) &#123;\n                int count = selector.select();\n//                int count = selector.selectNow();\n                log.debug(\"select count: &#123;&#125;\", count);\n//                if(count &lt;= 0) &#123;\n//                    continue;\n//                &#125;\n\n                // 获取所有事件\n                Set&lt;SelectionKey> keys = selector.selectedKeys();\n\n                // 遍历所有事件，逐一处理\n                Iterator&lt;SelectionKey> iter = keys.iterator();\n                while (iter.hasNext()) &#123;\n                    SelectionKey key = iter.next();\n                    // 判断事件类型\n                    if (key.isAcceptable()) &#123;\n                        ServerSocketChannel c = (ServerSocketChannel) key.channel();\n                        // 必须处理\n                        SocketChannel sc = c.accept();\n                        log.debug(\"&#123;&#125;\", sc);\n                    &#125;\n                    // 处理完毕，必须将事件移除\n                    iter.remove();\n                &#125;\n            &#125;\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n 事件发生后，要么处理，要么取消（cancel），不能什么都不做，否则下次该事件仍会触发，这是因为 nio 底层使用的是水平触发\n处理 read 事件@Slf4j\npublic class ChannelDemo6 &#123;\n    public static void main(String[] args) &#123;\n        try (ServerSocketChannel channel = ServerSocketChannel.open()) &#123;\n            channel.bind(new InetSocketAddress(8080));\n            System.out.println(channel);\n            Selector selector = Selector.open();\n            channel.configureBlocking(false);\n            channel.register(selector, SelectionKey.OP_ACCEPT);\n\n            while (true) &#123;\n                int count = selector.select();\n//                int count = selector.selectNow();\n                log.debug(\"select count: &#123;&#125;\", count);\n//                if(count &lt;= 0) &#123;\n//                    continue;\n//                &#125;\n\n                // 获取所有事件\n                Set&lt;SelectionKey> keys = selector.selectedKeys();\n\n                // 遍历所有事件，逐一处理\n                Iterator&lt;SelectionKey> iter = keys.iterator();\n                while (iter.hasNext()) &#123;\n                    SelectionKey key = iter.next();\n                    // 判断事件类型\n                    if (key.isAcceptable()) &#123;\n                        ServerSocketChannel c = (ServerSocketChannel) key.channel();\n                        // 必须处理\n                        SocketChannel sc = c.accept();\n                        sc.configureBlocking(false);\n                        sc.register(selector, SelectionKey.OP_READ);\n                        log.debug(\"连接已建立: &#123;&#125;\", sc);\n                    &#125; else if (key.isReadable()) &#123;\n                        SocketChannel sc = (SocketChannel) key.channel();\n                        ByteBuffer buffer = ByteBuffer.allocate(128);\n                        int read = sc.read(buffer);\n                        if(read == -1) &#123;\n//cancel 会取消注册在 selector 上的 channel，并从 keys 集合中删除 key 后续不会再监听事件\n                            key.cancel();\n                            sc.close();\n                        &#125; else &#123;\n                            buffer.flip();\n                            debug(buffer);\n                        &#125;\n                    &#125;\n                    // 处理完毕，必须将事件移除\n                    iter.remove();\n                &#125;\n            &#125;\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n开启两个客户端，修改一下发送文字，输出\nsun.nio.ch.ServerSocketChannelImpl[&#x2F;0:0:0:0:0:0:0:0:8080]\n21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1\n21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - 连接已建立: java.nio.channels.SocketChannel[connected local&#x3D;&#x2F;127.0.0.1:8080 remote&#x3D;&#x2F;127.0.0.1:60367]\n21:16:39 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 68 65 6c 6c 6f                                  |hello           |\n+--------+-------------------------------------------------+----------------+\n21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1\n21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - 连接已建立: java.nio.channels.SocketChannel[connected local&#x3D;&#x2F;127.0.0.1:8080 remote&#x3D;&#x2F;127.0.0.1:60378]\n21:16:59 [DEBUG] [main] c.i.n.ChannelDemo6 - select count: 1\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 77 6f 72 6c 64                                  |world           |\n+--------+-------------------------------------------------+----------------+\n\n为何要 iter.remove()\n\n\n\n\n\n\n\n\n因为 select 在事件发生后，就会将相关的 key 放入 selectedKeys 集合，但不会在处理完后从 selectedKeys 集合中移除，需要我们自己编码删除。例如\n\n第一次触发了 ssckey 上的 accept 事件，没有移除 ssckey \n第二次触发了 sckey 上的 read 事件，但这时 selectedKeys 中还有上次的 ssckey ，在处理时因为没有真正的 serverSocket 连上了，就会导致空指针异常\n\n处理消息的边界\n一种思路是固定消息长度，数据包大小一样，服务器按预定长度读取，缺点是浪费带宽\n另一种思路是按分隔符拆分，缺点是效率低\nTLV 格式，即 Type 类型、Length 长度、Value 数据，类型和长度已知的情况下，就可以方便获取消息大小，分配合适的 buffer，缺点是 buffer 需要提前分配，如果内容过大，则影响 server 吞吐量\nHttp 1.1 是 TLV 格式\nHttp 2.0 是 LTV 格式\n\n\n\nsequenceDiagram \nparticipant c1 as 客户端1\nparticipant s as 服务器\nparticipant b1 as ByteBuffer1\nparticipant b2 as ByteBuffer2\nc1 ->> s: 发送 01234567890abcdef3333\\r\ns ->> b1: 第一次 read 存入 01234567890abcdef\ns ->> b2: 扩容\nb1 ->> b2: 拷贝 01234567890abcdef\ns ->> b2: 第二次 read 存入 3333\\r\nb2 ->> b2: 01234567890abcdef3333\\r\n\n服务器端\nprivate static void split(ByteBuffer source) &#123;\n    source.flip();\n    for (int i = 0; i &lt; source.limit(); i++) &#123;\n        // 找到一条完整消息\n        if (source.get(i) == '\\n') &#123;\n            int length = i + 1 - source.position();\n            // 把这条完整消息存入新的 ByteBuffer\n            ByteBuffer target = ByteBuffer.allocate(length);\n            // 从 source 读，向 target 写\n            for (int j = 0; j &lt; length; j++) &#123;\n                target.put(source.get());\n            &#125;\n            debugAll(target);\n        &#125;\n    &#125;\n    source.compact(); // 0123456789abcdef  position 16 limit 16\n&#125;\n\npublic static void main(String[] args) throws IOException &#123;\n    // 1. 创建 selector, 管理多个 channel\n    Selector selector = Selector.open();\n    ServerSocketChannel ssc = ServerSocketChannel.open();\n    ssc.configureBlocking(false);\n    // 2. 建立 selector 和 channel 的联系（注册）\n    // SelectionKey 就是将来事件发生后，通过它可以知道事件和哪个channel的事件\n    SelectionKey sscKey = ssc.register(selector, 0, null);\n    // key 只关注 accept 事件\n    sscKey.interestOps(SelectionKey.OP_ACCEPT);\n    log.debug(\"sscKey:&#123;&#125;\", sscKey);\n    ssc.bind(new InetSocketAddress(8080));\n    while (true) &#123;\n        // 3. select 方法, 没有事件发生，线程阻塞，有事件，线程才会恢复运行\n        // select 在事件未处理时，它不会阻塞, 事件发生后要么处理，要么取消，不能置之不理\n        selector.select();\n        // 4. 处理事件, selectedKeys 内部包含了所有发生的事件\n        Iterator&lt;SelectionKey> iter = selector.selectedKeys().iterator(); // accept, read\n        while (iter.hasNext()) &#123;\n            SelectionKey key = iter.next();\n            // 处理key 时，要从 selectedKeys 集合中删除，否则下次处理就会有问题\n            iter.remove();\n            log.debug(\"key: &#123;&#125;\", key);\n            // 5. 区分事件类型\n            if (key.isAcceptable()) &#123; // 如果是 accept\n                ServerSocketChannel channel = (ServerSocketChannel) key.channel();\n                SocketChannel sc = channel.accept();\n                sc.configureBlocking(false);\n                ByteBuffer buffer = ByteBuffer.allocate(16); // attachment\n                // 将一个 byteBuffer 作为附件关联到 selectionKey 上\n                SelectionKey scKey = sc.register(selector, 0, buffer);\n                scKey.interestOps(SelectionKey.OP_READ);\n                log.debug(\"&#123;&#125;\", sc);\n                log.debug(\"scKey:&#123;&#125;\", scKey);\n            &#125; else if (key.isReadable()) &#123; // 如果是 read\n                try &#123;\n                    SocketChannel channel = (SocketChannel) key.channel(); // 拿到触发事件的channel\n                    // 获取 selectionKey 上关联的附件\n                  ByteBuffer buffer = (ByteBuffer) key.attachment();\n                  int read = channel.read(buffer); // 如果是正常断开，read 的方法的返回值是 -1\n                    if(read == -1) &#123;\n                        key.cancel();\n                    &#125; else &#123;\n                        split(buffer);\n                        // 需要扩容\n                        if (buffer.position() == buffer.limit()) &#123;\n                     ByteBuffer newBuffer = ByteBuffer.allocate(buffer.capacity() * 2);\n                            buffer.flip();\n                            newBuffer.put(buffer); // 0123456789abcdef3333\\n\n                            key.attach(newBuffer);\n                        &#125;\n                    &#125;\n\n                &#125; catch (IOException e) &#123;\n                    e.printStackTrace();\n                    key.cancel();  // 因为客户端断开了,因此需要将 key 取消（从 selector 的 keys 集合中真正删除 key）\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n客户端\nSocketChannel sc = SocketChannel.open();\nsc.connect(new InetSocketAddress(\"localhost\", 8080));\nSocketAddress address = sc.getLocalAddress();\n// sc.write(Charset.defaultCharset().encode(\"hello\\nworld\\n\"));\nsc.write(Charset.defaultCharset().encode(\"0123\\n456789abcdef\"));\nsc.write(Charset.defaultCharset().encode(\"0123456789abcdef3333\\n\"));\nSystem.in.read();\n\nByteBuffer 大小分配\n每个 channel 都需要记录可能被切分的消息，因为 ByteBuffer 不能被多个 channel 共同使用，因此需要为每个 channel 维护一个独立的 ByteBuffer\nByteBuffer 不能太大，比如一个 ByteBuffer 1Mb 的话，要支持百万连接就要 1Tb 内存，因此需要设计大小可变的 ByteBuffer\n一种思路是首先分配一个较小的 buffer，例如 4k，如果发现数据不够，再分配 8k 的 buffer，将 4k buffer 内容拷贝至 8k buffer，优点是消息连续容易处理，缺点是数据拷贝耗费性能，参考实现 http://tutorials.jenkov.com/java-performance/resizable-array.html\n另一种思路是用多个数组组成 buffer，一个数组不够，把多出来的内容写入新的数组，与前面的区别是消息存储不连续解析复杂，优点是避免了拷贝引起的性能损耗\n\n\n\n处理 write 事件一次无法写完例子\n非阻塞模式下，无法保证把 buffer 中所有数据都写入 channel，因此需要追踪 write 方法的返回值（代表实际写入字节数）\n用 selector 监听所有 channel 的可写事件，每个 channel 都需要一个 key 来跟踪 buffer，但这样又会导致占用内存过多，就有两阶段策略\n当消息处理器第一次写入消息时，才将 channel 注册到 selector 上\nselector 检查 channel 上的可写事件，如果所有的数据写完了，就取消 channel 的注册\n如果不取消，会每次可写均会触发 write 事件\n\n\n\npublic class WriteServer &#123;\n\n    public static void main(String[] args) throws IOException &#123;\n        ServerSocketChannel ssc = ServerSocketChannel.open();\n        ssc.configureBlocking(false);\n        ssc.bind(new InetSocketAddress(8080));\n\n        Selector selector = Selector.open();\n        ssc.register(selector, SelectionKey.OP_ACCEPT);\n\n        while(true) &#123;\n            selector.select();\n\n            Iterator&lt;SelectionKey> iter = selector.selectedKeys().iterator();\n            while (iter.hasNext()) &#123;\n                SelectionKey key = iter.next();\n                iter.remove();\n                if (key.isAcceptable()) &#123;\n                    SocketChannel sc = ssc.accept();\n                    sc.configureBlocking(false);\n                    SelectionKey sckey = sc.register(selector, SelectionKey.OP_READ);\n                    // 1. 向客户端发送内容\n                    StringBuilder sb = new StringBuilder();\n                    for (int i = 0; i &lt; 3000000; i++) &#123;\n                        sb.append(\"a\");\n                    &#125;\n                    ByteBuffer buffer = Charset.defaultCharset().encode(sb.toString());\n                    int write = sc.write(buffer);\n                    // 3. write 表示实际写了多少字节\n                    System.out.println(\"实际写入字节:\" + write);\n                    // 4. 如果有剩余未读字节，才需要关注写事件\n                    if (buffer.hasRemaining()) &#123;\n                        // read 1  write 4\n                        // 在原有关注事件的基础上，多关注 写事件\n                        sckey.interestOps(sckey.interestOps() + SelectionKey.OP_WRITE);\n                        // 把 buffer 作为附件加入 sckey\n                        sckey.attach(buffer);\n                    &#125;\n                &#125; else if (key.isWritable()) &#123;\n                    ByteBuffer buffer = (ByteBuffer) key.attachment();\n                    SocketChannel sc = (SocketChannel) key.channel();\n                    int write = sc.write(buffer);\n                    System.out.println(\"实际写入字节:\" + write);\n                    if (!buffer.hasRemaining()) &#123; // 写完了\n          //只要向 channel 发送数据时，socket 缓冲可写，这个事件会频繁触发，因此应当只在 socket 缓冲区\t\t   //写不下时再关注可写事件，数据写完之后再取消关注\n                        key.interestOps(key.interestOps() - SelectionKey.OP_WRITE);\n                        key.attach(null);\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n客户端\npublic class WriteClient &#123;\n    public static void main(String[] args) throws IOException &#123;\n        Selector selector = Selector.open();\n        SocketChannel sc = SocketChannel.open();\n        sc.configureBlocking(false);\n        sc.register(selector, SelectionKey.OP_CONNECT | SelectionKey.OP_READ);\n        sc.connect(new InetSocketAddress(\"localhost\", 8080));\n        int count = 0;\n        while (true) &#123;\n            selector.select();\n            Iterator&lt;SelectionKey> iter = selector.selectedKeys().iterator();\n            while (iter.hasNext()) &#123;\n                SelectionKey key = iter.next();\n                iter.remove();\n                if (key.isConnectable()) &#123;\n                    System.out.println(sc.finishConnect());\n                &#125; else if (key.isReadable()) &#123;\n                    ByteBuffer buffer = ByteBuffer.allocate(1024 * 1024);\n                    count += sc.read(buffer);\n                    buffer.clear();\n                    System.out.println(count);\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n在这个例子中，ServerSocketChannel 和 SocketChannel 的通信流程如下：\n1.首先，服务器端调用 ServerSocketChannel.open() 方法 打开一个 ServerSocketChannel 通道，并调用 configureBlocking(false) 将其设置为非阻塞模式。\n2.接着，服务器端调用 ServerSocketChannel.bind() 方法绑定监听的端口，并通过调用 Selector.open() 创建一个 Selector 对象。然后，通过 ServerSocketChannel.register() 方法将 ServerSocketChannel 注册到 Selector 中，监听 SelectionKey.ACCEPT 事件。\n3.当客户端发起连接请求时，服务器端的 Selector 监听到该请求并触发事件，服务器端通过 Selector.selectedKeys() 方法获取该事件对应的 SelectionKey，并通过 SelectionKey.channel() 获取其对应的 ServerSocketChannel 对象。\n4.服务器端通过 ServerSocketChannel.accept() 方法响应连接请求，并返回一个新的 SocketChannel 对象。\n5.服务器端通过 SocketChannel.configureBlocking(false) 将 SocketChannel 对象设置为非阻塞模式，并将其注册到 Selector 对象上，监听 SelectionKey.OP_READ 事件。\n6.客户端和服务器端均已经在各自的 Selector 对象上注册了 OP_READ 事件，表示可以互相传输数据。\n7.当 Selector 监听到客户端发送的数据时，会触发 SelectableChannel 的 OP_READ 事件，此时服务器端通过 Selector.selectedKeys() 方法获取该事件对应的 SelectionKey，并通过 SelectionKey.channel() 获取其对应的 SocketChannel 对象。\n8.通过 SocketChannel 的 read() 方法读取数据，并通过 Selector.register() 方法向 Selector 上注册 SelectionKey.OP_WRITE 事件，表示服务器需要给客户端发送数据。\n9.当 Selector 监听到服务器调用 SocketChannel 的 write() 方法准备发送数据时，会触发 SelectableChannel 的 OP_WRITE 事件，此时客户端通过 Selector.selectedKeys() 方法获取该事件对应的 SelectionKey，并通过 SelectionKey.channel() 获取其对应的 SocketChannel 对象。\n10.通过 SocketChannel 的 write() 方法向客户端发送数据。\n以上即为 SocketChannel 和 ServerSocketChannel 之间通信的基本流程，通过 Selector 监听事件并触发处理，实现了多个 Channel 之间的 I&#x2F;O 通信。在实际应用中，还需要处理异常情况、关闭 Channel 和 Selector 等操作。\n利用多线程优化前面的代码只有一个选择器，没有充分利用多核 cpu，如何改进呢？\n分两组选择器\n\n单线程配一个选择器，专门处理 accept 事件\n创建 cpu 核心数的线程，每个线程配一个选择器，轮流处理 read 事件\n\npublic class ChannelDemo7 &#123;\n    public static void main(String[] args) throws IOException &#123;\n        new BossEventLoop().register();\n    &#125;\n\n\n    @Slf4j\n    static class BossEventLoop implements Runnable &#123;\n        private Selector boss;\n        private WorkerEventLoop[] workers;\n        private volatile boolean start = false;\n        AtomicInteger index = new AtomicInteger();\n\n        public void register() throws IOException &#123;\n            if (!start) &#123;\n                ServerSocketChannel ssc = ServerSocketChannel.open();\n                ssc.bind(new InetSocketAddress(8080));\n                ssc.configureBlocking(false);\n                boss = Selector.open();\n                SelectionKey ssckey = ssc.register(boss, 0, null);\n                ssckey.interestOps(SelectionKey.OP_ACCEPT);\n                workers = initEventLoops();\n                new Thread(this, \"boss\").start();\n                log.debug(\"boss start...\");\n                start = true;\n            &#125;\n        &#125;\n\n        public WorkerEventLoop[] initEventLoops() &#123;\n//        EventLoop[] eventLoops = new EventLoop[Runtime.getRuntime().availableProcessors()];\n            WorkerEventLoop[] workerEventLoops = new WorkerEventLoop[2];\n            for (int i = 0; i &lt; workerEventLoops.length; i++) &#123;\n                workerEventLoops[i] = new WorkerEventLoop(i);\n            &#125;\n            return workerEventLoops;\n        &#125;\n\n        @Override\n        public void run() &#123;\n            while (true) &#123;\n                try &#123;\n                    boss.select();\n                    Iterator&lt;SelectionKey> iter = boss.selectedKeys().iterator();\n                    while (iter.hasNext()) &#123;\n                        SelectionKey key = iter.next();\n                        iter.remove();\n                        if (key.isAcceptable()) &#123;\n                            ServerSocketChannel c = (ServerSocketChannel) key.channel();\n                            SocketChannel sc = c.accept();\n                            sc.configureBlocking(false);\n                            log.debug(\"&#123;&#125; connected\", sc.getRemoteAddress());\n                      workers[index.getAndIncrement() % workers.length].register(sc);\n                        &#125;\n                    &#125;\n                &#125; catch (IOException e) &#123;\n                    e.printStackTrace();\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n\n    @Slf4j\n    static class WorkerEventLoop implements Runnable &#123;\n        private Selector worker;\n        private volatile boolean start = false;\n        private int index;\n\n     private final ConcurrentLinkedQueue&lt;Runnable> tasks = new ConcurrentLinkedQueue&lt;>();\n\n        public WorkerEventLoop(int index) &#123;\n            this.index = index;\n        &#125;\n\n        public void register(SocketChannel sc) throws IOException &#123;\n            if (!start) &#123;\n                worker = Selector.open();\n                new Thread(this, \"worker-\" + index).start();\n                start = true;\n            &#125;\n            tasks.add(() -> &#123;\n                try &#123;\n                    SelectionKey sckey = sc.register(worker, 0, null);\n                    sckey.interestOps(SelectionKey.OP_READ);\n                    worker.selectNow();\n                &#125; catch (IOException e) &#123;\n                    e.printStackTrace();\n                &#125;\n            &#125;);\n            worker.wakeup();\n        &#125;\n\n        @Override\n        public void run() &#123;\n            while (true) &#123;\n                try &#123;\n                    worker.select();\n                    Runnable task = tasks.poll();\n                    if (task != null) &#123;\n                        task.run();\n                    &#125;\n                    Set&lt;SelectionKey> keys = worker.selectedKeys();\n                    Iterator&lt;SelectionKey> iter = keys.iterator();\n                    while (iter.hasNext()) &#123;\n                        SelectionKey key = iter.next();\n                        if (key.isReadable()) &#123;\n                            SocketChannel sc = (SocketChannel) key.channel();\n                            ByteBuffer buffer = ByteBuffer.allocate(128);\n                            try &#123;\n                                int read = sc.read(buffer);\n                                if (read == -1) &#123;\n                                    key.cancel();\n                                    sc.close();\n                                &#125; else &#123;\n                                    buffer.flip();\n                                    log.debug(\"&#123;&#125; message:\", sc.getRemoteAddress());\n                                    debugAll(buffer);\n                                &#125;\n                            &#125; catch (IOException e) &#123;\n                                e.printStackTrace();\n                                key.cancel();\n                                sc.close();\n                            &#125;\n                        &#125;\n                        iter.remove();\n                    &#125;\n                &#125; catch (IOException e) &#123;\n                    e.printStackTrace();\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n\n\nUDP\nUDP 是无连接的，client 发送数据不会管 server 是否开启\nserver 这边的 receive 方法会将接收到的数据存入 byte buffer，但如果数据报文超过 buffer 大小，多出来的数据会被默默抛弃\n\n首先启动服务器端\npublic class UdpServer &#123;\n    public static void main(String[] args) &#123;\n        try (DatagramChannel channel = DatagramChannel.open()) &#123;\n            channel.socket().bind(new InetSocketAddress(9999));\n            System.out.println(\"waiting...\");\n            ByteBuffer buffer = ByteBuffer.allocate(32);\n            channel.receive(buffer);\n            buffer.flip();\n            debug(buffer);\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n输出\nwaiting...\n\n\n\n运行客户端\npublic class UdpClient &#123;\n    public static void main(String[] args) &#123;\n        try (DatagramChannel channel = DatagramChannel.open()) &#123;\n            ByteBuffer buffer = StandardCharsets.UTF_8.encode(\"hello\");\n            InetSocketAddress address = new InetSocketAddress(\"localhost\", 9999);\n            channel.send(buffer, address);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n接下来服务器端输出\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 68 65 6c 6c 6f                                  |hello           |\n+--------+-------------------------------------------------+----------------+\n\nNIO vs BIOstream vs channel\nstream 不会自动缓冲数据，channel 会利用系统提供的发送缓冲区、接收缓冲区（更为底层）\nstream 仅支持阻塞 API，channel 同时支持阻塞、非阻塞 API，网络 channel 可配合 selector 实现多路复用\n二者均为全双工，即读写可以同时进行\n\nIO 模型同步阻塞、同步非阻塞、同步多路复用、异步阻塞（没有此情况）、异步非阻塞\n\n同步：线程自己去获取结果（一个线程）\n异步：线程自己不去获取结果，而是由其它线程送结果（至少两个线程）\n\n当调用一次 channel.read 或 stream.read 后，会切换至操作系统内核态来完成真正数据读取，而读取又分为两个阶段，分别为：\n\n等待数据阶段\n复制数据阶段\n\n\n\n阻塞 IO\n\n\n非阻塞  IO\n\n\n多路复用\n\n\n信号驱动\n\n异步 IO\n\n\n阻塞 IO vs 多路复用\n\n\n\n\n零拷贝传统 IO 问题传统的 IO 将一个文件通过 socket 写出\nFile f = new File(\"helloword/data.txt\");\nRandomAccessFile file = new RandomAccessFile(file, \"r\");\n\nbyte[] buf = new byte[(int)f.length()];\nfile.read(buf);\n\nSocket socket = ...;\nsocket.getOutputStream().write(buf);\n\n内部工作流程是这样的：\n\n\njava 本身并不具备 IO 读写能力，因此 read 方法调用后，要从 java 程序的用户态切换至内核态，去调用操作系统（Kernel）的读能力，将数据读入内核缓冲区。这期间用户线程阻塞，操作系统使用 DMA（Direct Memory Access）来实现文件读，其间也不会使用 cpu\n\n\n\n\n\n\n\n\n\nDMA 也可以理解为硬件单元，用来解放 cpu 完成文件 IO\n\n从内核态切换回用户态，将数据从内核缓冲区读入用户缓冲区（即 byte[] buf），这期间 cpu 会参与拷贝，无法利用 DMA\n\n调用 write 方法，这时将数据从用户缓冲区（byte[] buf）写入 socket 缓冲区，cpu 会参与拷贝\n\n接下来要向网卡写数据，这项能力 java 又不具备，因此又得从用户态切换至内核态，调用操作系统的写能力，使用 DMA 将 socket 缓冲区的数据写入网卡，不会使用 cpu\n\n\n可以看到中间环节较多，java 的 IO 实际不是物理设备级别的读写，而是缓存的复制，底层的真正读写是操作系统来完成的\n\n用户态与内核态的切换发生了 3 次，这个操作比较重量级\n数据拷贝了共 4 次\n\nNIO 优化通过 DirectByteBuf \n\nByteBuffer.allocate(10)  HeapByteBuffer 使用的还是 java 内存\nByteBuffer.allocateDirect(10)  DirectByteBuffer 使用的是操作系统内存\n\n\n大部分步骤与优化前相同，不再赘述。唯有一点：java 可以使用 DirectByteBuf 将堆外内存映射到 jvm 内存中来直接访问使用\n\n这块内存不受 jvm 垃圾回收的影响，因此内存地址固定，有助于 IO 读写\njava 中的 DirectByteBuf 对象仅维护了此内存的虚引用，内存回收分成两步\nDirectByteBuf 对象被垃圾回收，将虚引用加入引用队列\n通过专门线程访问引用队列，根据虚引用释放堆外内存\n\n\n减少了一次数据拷贝，用户态与内核态的切换次数没有减少\n\n进一步优化（底层采用了 linux 2.1 后提供的 sendFile 方法），java 中对应着两个 channel 调用 transferTo&#x2F;transferFrom 方法拷贝数据\n\n\njava 调用 transferTo 方法后，要从 java 程序的用户态切换至内核态，使用 DMA将数据读入内核缓冲区，不会使用 cpu\n数据从内核缓冲区传输到 socket 缓冲区，cpu 会参与拷贝\n最后使用 DMA 将 socket 缓冲区的数据写入网卡，不会使用 cpu\n\n可以看到\n\n只发生了一次用户态与内核态的切换\n数据拷贝了 3 次\n\n进一步优化（linux 2.4）\n\n\njava 调用 transferTo 方法后，要从 java 程序的用户态切换至内核态，使用 DMA将数据读入内核缓冲区，不会使用 cpu\n只会将一些 offset 和 length 信息拷入 socket 缓冲区，几乎无消耗\n使用 DMA 将 内核缓冲区的数据写入网卡，不会使用 cpu\n\n整个过程仅只发生了一次用户态与内核态的切换，数据拷贝了 2 次。所谓的【零拷贝】，并不是真正无拷贝，而是在不会拷贝重复数据到 jvm 内存中，零拷贝的优点有\n\n更少的用户态与内核态的切换\n不利用 cpu 计算，减少 cpu 缓存伪共享\n零拷贝适合小文件传输\n\nAIO（Asynchronous I&#x2F;O，异步I&#x2F;O）是一种 I&#x2F;O 操作的技术AIO 用来解决数据复制阶段的阻塞问题\n\n同步意味着，在进行读写操作时，线程需要等待结果，还是相当于闲置\n异步意味着，在进行读写操作时，线程不必等待结果，而是将来由操作系统来通过回调方式由另外的线程来获得结果\n\n\n\n\n\n\n\n\n\n\n异步模型需要底层操作系统（Kernel）提供支持\n\nWindows 系统通过 IOCP 实现了真正的异步 IO\nLinux 系统异步 IO 在 2.6 版本引入，但其底层实现还是用多路复用模拟了异步 IO，性能没有优势\n\n文件 AIO先来看看 AsynchronousFileChannel\n@Slf4j\npublic class AioDemo1 &#123;\n    public static void main(String[] args) throws IOException &#123;\n        try&#123;\n            AsynchronousFileChannel s = \n                AsynchronousFileChannel.open(\n                \tPaths.get(\"1.txt\"), StandardOpenOption.READ);\n            ByteBuffer buffer = ByteBuffer.allocate(2);\n            log.debug(\"begin...\");\n            s.read(buffer, 0, null, new CompletionHandler&lt;Integer, ByteBuffer>() &#123;\n                @Override\n                public void completed(Integer result, ByteBuffer attachment) &#123;\n                    log.debug(\"read completed...&#123;&#125;\", result);\n                    buffer.flip();\n                    debug(buffer);\n                &#125;\n\n                @Override\n                public void failed(Throwable exc, ByteBuffer attachment) &#123;\n                    log.debug(\"read failed...\");\n                &#125;\n            &#125;);\n\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n        log.debug(\"do other things...\");\n        System.in.read();\n    &#125;\n&#125;\n\n输出\n13:44:56 [DEBUG] [main] c.i.aio.AioDemo1 - begin...\n13:44:56 [DEBUG] [main] c.i.aio.AioDemo1 - do other things...\n13:44:56 [DEBUG] [Thread-5] c.i.aio.AioDemo1 - read completed...2\n         +-------------------------------------------------+\n         |  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |\n+--------+-------------------------------------------------+----------------+\n|00000000| 61 0d                                           |a.              |\n+--------+-------------------------------------------------+----------------+\n\n可以看到\n\n响应文件读取成功的是另一个线程 Thread-5\n主线程并没有 IO 操作阻塞\n\n守护线程默认文件 AIO 使用的线程都是守护线程，所以最后要执行 System.in.read() 以避免守护线程意外结束\n网络 AIOpublic class AioServer &#123;\n    public static void main(String[] args) throws IOException &#123;\n        AsynchronousServerSocketChannel ssc = AsynchronousServerSocketChannel.open();\n        ssc.bind(new InetSocketAddress(8080));\n        ssc.accept(null, new AcceptHandler(ssc));\n        System.in.read();\n    &#125;\n\n    private static void closeChannel(AsynchronousSocketChannel sc) &#123;\n        try &#123;\n            System.out.printf(\"[%s] %s close\\n\", Thread.currentThread().getName(), sc.getRemoteAddress());\n            sc.close();\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n\n    private static class ReadHandler implements CompletionHandler&lt;Integer, ByteBuffer> &#123;\n        private final AsynchronousSocketChannel sc;\n\n        public ReadHandler(AsynchronousSocketChannel sc) &#123;\n            this.sc = sc;\n        &#125;\n\n        @Override\n        public void completed(Integer result, ByteBuffer attachment) &#123;\n            try &#123;\n                if (result == -1) &#123;\n                    closeChannel(sc);\n                    return;\n                &#125;\n                System.out.printf(\"[%s] %s read\\n\", Thread.currentThread().getName(), sc.getRemoteAddress());\n                attachment.flip();\n                System.out.println(Charset.defaultCharset().decode(attachment));\n                attachment.clear();\n                // 处理完第一个 read 时，需要再次调用 read 方法来处理下一个 read 事件\n                sc.read(attachment, attachment, this);\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n\n        @Override\n        public void failed(Throwable exc, ByteBuffer attachment) &#123;\n            closeChannel(sc);\n            exc.printStackTrace();\n        &#125;\n    &#125;\n\n    private static class WriteHandler implements CompletionHandler&lt;Integer, ByteBuffer> &#123;\n        private final AsynchronousSocketChannel sc;\n\n        private WriteHandler(AsynchronousSocketChannel sc) &#123;\n            this.sc = sc;\n        &#125;\n\n        @Override\n        public void completed(Integer result, ByteBuffer attachment) &#123;\n            // 如果作为附件的 buffer 还有内容，需要再次 write 写出剩余内容\n            if (attachment.hasRemaining()) &#123;\n                sc.write(attachment);\n            &#125;\n        &#125;\n\n        @Override\n        public void failed(Throwable exc, ByteBuffer attachment) &#123;\n            exc.printStackTrace();\n            closeChannel(sc);\n        &#125;\n    &#125;\n\n    private static class AcceptHandler implements CompletionHandler&lt;AsynchronousSocketChannel, Object> &#123;\n        private final AsynchronousServerSocketChannel ssc;\n\n        public AcceptHandler(AsynchronousServerSocketChannel ssc) &#123;\n            this.ssc = ssc;\n        &#125;\n\n        @Override\n        public void completed(AsynchronousSocketChannel sc, Object attachment) &#123;\n            try &#123;\n                System.out.printf(\"[%s] %s connected\\n\", Thread.currentThread().getName(), sc.getRemoteAddress());\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n            ByteBuffer buffer = ByteBuffer.allocate(16);\n            // 读事件由 ReadHandler 处理\n            sc.read(buffer, buffer, new ReadHandler(sc));\n            // 写事件由 WriteHandler 处理\n            sc.write(Charset.defaultCharset().encode(\"server hello!\"), ByteBuffer.allocate(16), new WriteHandler(sc));\n            // 处理完第一个 accpet 时，需要再次调用 accept 方法来处理下一个 accept 事件\n            ssc.accept(null, this);\n        &#125;\n\n        @Override\n        public void failed(Throwable exc, Object attachment) &#123;\n            exc.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n\n\n","slug":"NIO","date":"2023-05-25T02:38:47.000Z","categories_index":"","tags_index":"IO","author_index":"大宝贝的程序员"},{"id":"a48e6dda0c21e40880cba7e763278b04","title":"Docker","content":"Docker的用途Docker如何解决大型项目依赖关系复杂，不同组件依赖的兼容性问题？\n\nDocker允许开发中将应用、依赖、函数库、配置一起打包，形成可移植镜像\nDocker应用运行在容器中，使用沙箱机制，相互隔离\n\nDocker如何解决开发、测试、生产环境有差异的问题？\n\nDocker镜像中包含完整运行环境，包括系统函数库，仅依赖系统的Linux内核，因此可以在任意Linux操作系统上运行\n\nDocker是一个快速交付应用、运行应用的技术，具备下列优势：\n\n可以将程序及其依赖、运行环境一起打包为一个镜像，可以迁移到任意Linux操作系统\n运行时利用沙箱机制形成隔离容器，各个应用互不干扰\n启动、移除都可以通过一行命令完成，方便快捷\n\nDocker和虚拟机的区别Docker可以让一个应用在任何操作系统中非常方便的运行。而以前我们接触的虚拟机，也能在一个操作系统中，运行另外一个操作系统，保护系统中的任何应用。\n虚拟机（virtual machine）是在操作系统中模拟硬件设备，然后运行另一个操作系统，比如在 Windows 系统里面运行 Ubuntu 系统，这样就可以运行任意的Ubuntu应用了。\nDocker仅仅是封装函数库，并没有模拟完整的操作系统。\n对比：\n\n\n\n特性\nDocker\n虚拟机\n\n\n\n性能\n接近原生\n性能较差\n\n\n磁盘占用\n一般为MB\n一般为GB\n\n\n启动\n秒级\n分钟级\n\n\nDocker和虚拟机的差异：\n\ndocker是一个系统进程；虚拟机是在操作系统中的操作系统\n\ndocker体积小、启动速度快、性能好；虚拟机体积大、启动速度慢、性能一般\n\n\nDocker架构镜像和容器Docker中有几个重要的概念：\n镜像（Image）：Docker将应用程序及其所需的依赖、函数库、环境、配置等文件打包在一起，称为镜像。\n容器（Container）：镜像中的应用程序运行后形成的进程就是容器，只是Docker会给容器进程做隔离，对外不可见。\n一切应用最终都是代码组成，都是硬盘中的一个个的字节形成的文件。只有运行时，才会加载到内存，形成进程。\n镜像，就是把一个应用在硬盘上的文件、及其运行环境、部分系统函数库文件一起打包形成的文件包。这个文件包是只读的。\n容器，就是将这些文件中编写的程序、函数加载到内存中允许，形成进程，只不过要隔离起来。因此一个镜像可以启动多次，形成多个容器进程。\nDockerHub开源应用程序非常多，打包这些应用往往是重复的劳动。为了避免这些重复劳动，人们就会将自己打包的应用镜像，例如Redis、MySQL镜像放到网络上，共享使用，就像GitHub的代码共享一样。\n\nDockerHub：DockerHub是一个官方的Docker镜像的托管平台。这样的平台称为Docker Registry。\n\n国内也有类似于DockerHub 的公开服务，比如 网易云镜像服务、阿里云镜像库等。\n\n\nDocker架构Docker是一个CS架构的程序，由两部分组成：\n\n服务端(server)：Docker守护进程，负责处理Docker指令，管理镜像、容器等\n\n客户端(client)：通过命令或RestAPI向Docker服务端发送指令。可以在本地或远程向服务端发送指令。\n\n\n\n安装Docker卸载yum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-selinux \\\n                  docker-engine-selinux \\\n                  docker-engine \\\n                  docker-ce\n\n安装docker安装yum工具\nyum install -y yum-utils \\\n           device-mapper-persistent-data \\\n           lvm2 --skip-broken\n\n然后更新本地镜像源：\nyum-config-manager \\\n    --add-repo \\\n    https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n    \nsed -i 's/download.docker.com/mirrors.aliyun.com\\/docker-ce/g' /etc/yum.repos.d/docker-ce.repo\n\nyum makecache fast\n\n然后输入命令：\nyum install -y docker-ce\n\ndocker启动相关命令# 关闭\nsystemctl stop firewalld\n# 禁止开机启动防火墙\nsystemctl disable firewalld\n\nsystemctl start docker  # 启动docker服务\n\nsystemctl stop docker  # 停止docker服务\n\nsystemctl restart docker  # 重启docker服务\n\ndocker -v\t\t\t#查看docker版本\n\nCentOS7安装DockerCompose下载Linux下需要通过命令下载：\n# 安装\ncurl -L https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n\n修改文件权限# 修改权限\nchmod +x /usr/local/bin/docker-compose\n\nBase自动补全命令# 补全命令\ncurl -L https://raw.githubusercontent.com/docker/compose/1.29.1/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n\n如果这里出现错误，需要修改自己的hosts文件：\necho \"199.232.68.133 raw.githubusercontent.com\" >> /etc/hosts\n\nDocker的基本操作镜像操作镜像名称\n镜像的名称组成：\n\n镜名称一般分两部分组成：[repository]:[tag]。\n在没有指定tag时，默认是latest，代表最新版本的镜像\n\nmysql:5.7\t中mysql就是repository，5.7就是tag，合一起就是镜像名称，代表5.7版本的MySQL镜像。\n镜像命令\n需求：从DockerHub中拉取一个nginx镜像并查看\n\n首先去镜像仓库搜索nginx镜像，比如DockerHub:\n\n根据查看到的镜像名称，拉取自己需要的镜像，通过命令：docker pull nginx\n\n通过命令：docker images 查看拉取到的镜像\n\n\n需求：利用docker save将nginx镜像导出磁盘，然后再通过load加载回来\n\n利用docker xx --help命令查看docker save和docker load的语法；如docker save --help\n\n​\t命令格式：\ndocker save -o [保存的目标文件名称] [镜像名称]\n\n\n使用docker save导出镜像到磁盘\n\ndocker save -o nginx.tar nginx:latest\n\n\n使用docker load加载镜像\n\n​\t\t先删除本地的nginx镜像：docker rmi nginx:latest\n​\t\t然后运行命令，加载本地文件：docker load -i nginx.tar\n容器操作容器相关命令容器操作的命令如图：\n\n容器保护三个状态：\n\n运行：进程正常运行\n暂停：进程暂停，CPU不再运行，并不释放内存\n停止：进程终止，回收进程占用的内存、CPU等资源\n\n容器操作的常用命令\n\ndocker run：创建并运行一个容器，处于运行状态\n\ndocker pause：让一个运行的容器暂停\n\ndocker unpause：让一个容器从暂停状态恢复运行\n\ndocker stop：停止一个运行的容器\n\ndocker start：让一个停止的容器再次运行\n\ndocker rm：删除一个容器\n\n查看容器日志的命令：\n\ndocker logs\n添加 -f 参数可以持续查看日志\n\n\n查看容器状态：\n\ndocker ps\ndocker ps -a 查看所有容器，包括已经停止的\n\n\n\n需求：创建并运行一个容器；例如nginx\n# 创建并运行一个容器\ndocker run --name containerName -p 80:80 -d nginx\n\n命令解读：\n\ndocker run ：创建并运行一个容器\n–name : 给容器起一个名字\n-p ：将宿主机端口与容器端口映射，冒号左侧是宿主机端口，右侧是容器端口\n-d：后台运行容器\nnginx：镜像名称，例如nginx\n-v  ：挂载一个数据卷到某个容器内目录\n\n默认情况下，容器是隔离环境，我们直接访问宿主机的80端口，肯定访问不到容器中的nginx。现在，将容器的80与宿主机的80关联起来，当我们访问宿主机的80端口时，就会被映射到容器的80，这样就能访问到nginx。\n\n需求：案例-进入容器，修改文件；如：进入Nginx容器，修改HTML文件内容\n进入容器。进入刚刚创建的nginx容器的命令为：\ndocker exec containerName -it  bash\n\n命令解读：\n\ndocker exec ：进入容器内部，执行一个命令\n\n-it : 给当前进入的容器创建一个标准输入、输出终端，允许我们与容器交互\n\ncontainerName ：要进入的容器的名称\n\nbash：进入容器后执行的命令，bash是一个linux终端交互命令\n\n\n进入nginx的HTML所在目录 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html\n容器内部会模拟一个独立的Linux文件系统，看起来如同一个linux服务器一样\nnginx的环境、配置、运行文件全部都在这个文件系统中，包括我们要修改的html文件。\n查看DockerHub网站中的nginx页面，可以知道nginx的html目录位置在/usr/share/nginx/html\n数据卷（容器数据管理）在之前的nginx案例中，修改nginx的html页面时，需要进入nginx内部。并且因为没有编辑器，修改文件也很麻烦。这就是因为容器与数据（容器内文件）耦合带来的后果。要解决这个问题，必须将数据与容器解耦，这就要用到数据卷了。\n数据卷（volume）是一个虚拟目录，指向宿主机文件系统中的某个目录。\n\n一旦完成数据卷挂载，对容器的一切操作都会作用在数据卷对应的宿主机目录了。这样，我们操作宿主机的&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;html目录，就等于操作容器内的&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html目录了\n数据集操作命令数据卷操作的基本语法如下：docker volume [COMMAND]\ndocker volume命令是数据卷操作，根据命令后跟随的command来确定下一步的操作：\n\ncreate 创建一个volume\ninspect 显示一个或多个volume的信息,包括关联的宿主机目录位置\nls 列出所有的volume\nprune 删除未使用的volume\nrm 删除一个或多个指定的volume\n\n创建和查看数据卷需求：创建一个数据卷，并查看数据卷在宿主机的目录位置，以html为例\n创建数据卷\ndocker volume create html\n\n查看所有数据\ndocker volume ls\n\n 查看数据卷详细信息卷\ndocker volume inspect html\n\n可以看到html这个数据卷关联的宿主机目录\n挂载数据卷可以通过 -v 参数来挂载一个数据卷到某个容器内目录，命令格式如下：\ndocker run \\\n  --name mn \\\n  -v html:/root/html \\\n  -p 8080:80\n  nginx \\\n\n-v html:/root/htm ：把html数据卷挂载到容器内的&#x2F;root&#x2F;html这个目录中\nDockerfile自定义镜像常见的镜像在DockerHub就能找到，但是我们自己写的项目就必须自己构建镜像了。\nDockerfile语法构建自定义的镜像时，并不需要一个个文件去拷贝，打包。\n我们只需要告诉Docker，我们的镜像的组成，需要哪些BaseImage、需要拷贝什么文件、需要安装什么依赖、启动脚本是什么，将来Docker会帮助我们构建镜像。\nDockerfile就是一个文本文件，其中包含一个个的**指令(Instruction)**，用指令来说明要执行什么操作来构建镜像。每一个指令都会形成一层Layer。\n\n\n\n指令\n说明\n示例\n\n\n\nFROM\n指定基础镜像\nFROM centos:7\n\n\nEVN\n设置环境变量，可在后面指令使用\nEVN key value\n\n\nCOPY\n拷贝本地文件到镜像的指定目录\nCOPY .&#x2F;mysql-5.7.rpm  &#x2F;tmp\n\n\nRUN\n执行Linux的shell命令，一般是安装过程的命令\nRUN yum install gcc\n\n\nEXPOSE\n指定容器运行时监听的端口，是给镜像使用者看的\nEXPOSE 8080\n\n\nENTRYPOINT\n镜像中应用的启动命令，容器运行时调用\nENTRYPOINT java-jar  xx.jar\n\n\n案例：基于Ubuntu镜像构建一个新镜像，运行一个java项目\n\n步骤1：新建一个空文件夹docker-demo\n\n步骤2：拷贝课前资料中的docker-demo.jar文件到docker-demo这个目录\n\n步骤3：拷贝课前资料中的jdk8.tar.gz文件到docker-demo这个目录\n\n步骤4：拷贝课前资料提供的Dockerfile到docker-demo这个目录\n\n其中的内容如下：\n# 指定基础镜像\nFROM ubuntu:16.04\n# 配置环境变量，JDK的安装目录\nENV JAVA_DIR=/usr/local\n\n# 拷贝jdk和java项目的包\nCOPY ./jdk8.tar.gz $JAVA_DIR/\nCOPY ./docker-demo.jar /tmp/app.jar\n\n# 安装JDK\nRUN cd $JAVA_DIR \\\n &amp;&amp; tar -xf ./jdk8.tar.gz \\\n &amp;&amp; mv ./jdk1.8.0_144 ./java8\n\n# 配置环境变量\nENV JAVA_HOME=$JAVA_DIR/java8\nENV PATH=$PATH:$JAVA_HOME/bin\n\n# 暴露端口\nEXPOSE 8090\n# 入口，java项目的启动命令\nENTRYPOINT java -jar /tmp/app.jar\n\n步骤5：将准备好的docker-demo上传到虚拟机任意目录，然后进入docker-demo目录下\n\n步骤6：运行命令：\ndocker build -t javaweb:1.0 .\n\n构建一个名为”javaweb:1.0”的Docker镜像。这个镜像将会使用当前目录下的Dockerfile文件进行构建。如果您有Dockerfile文件和相应的应用程序代码在同一个目录下，运行该命令将会生成一个包含应用程序的镜像。请注意，最后一个”.”表示构建上下文的路径是当前目录。\n\n\n\n\n\n\n\n\n\n注意：上下文路径包含了我们构建此 Docker 镜像所需要的全部文件。\n\n\n","slug":"Docker","date":"2023-05-24T11:24:04.000Z","categories_index":"","tags_index":"Docker","author_index":"大宝贝的程序员"},{"id":"f515c75be93536d047fb1ea7dd55160d","title":"SQL优化","content":"SQL优化插入数据insert\n如果我们需要一次性往数据库表中插入多条记录，可以从以下三个方面进行优化。\n 优化方案一\t\t批量插入数据\nInsert into tb_test values(1,&#39;Tom&#39;),(2,&#39;Cat&#39;),(3,&#39;Jerry&#39;); \n\n 优化方案二\t\t手动控制事务\nstart transaction;\ninsert into tb_test values(1,&#39;Tom&#39;),(2,&#39;Cat&#39;),(3,&#39;Jerry&#39;);\ninsert into tb_test values(4,&#39;Tom&#39;),(5,&#39;Cat&#39;),(6,&#39;Jerry&#39;);\ninsert into tb_test values(7,&#39;Tom&#39;),(8,&#39;Cat&#39;),(9,&#39;Jerry&#39;);\ncommit;\n\n优化方案三\t\t主键顺序插入，性能要高于乱序插入。\n主键乱序插入 : 8 1 9 21 88 2 4 15 89 5 7 3\n主键顺序插入 : 1 2 3 4 5 7 8 9 15 21 88 89\n\n大批量插入数据\n如果一次性需要插入大批量数据(比如: 几百万的记录)，使用insert语句插入性能较低，此时可以使用MySQL数据库提供的load指令进行插入。操作如下：\n-- 客户端连接服务端时，加上参数 -–local-infile\nmysql –-local-infile -u root -p\n\n-- 设置全局参数local_infile为1，开启从本地加载文件导入数据的开关\nset global local_infile &#x3D; 1;\n\n-- 执行load指令将准备好的数据，加载到表结构中\nload data local infile &#39;&#x2F;root&#x2F;sql1.log&#39; into table tb_user fields\nterminated by &#39;,&#39; lines terminated by &#39;\\n&#39; ;\n\n示例演示:\nCREATE TABLE &#96;tb_user&#96; (\n&#96;id&#96; INT(11) NOT NULL AUTO_INCREMENT,\n&#96;username&#96; VARCHAR(50) NOT NULL,\n&#96;password&#96; VARCHAR(50) NOT NULL,\n&#96;name&#96; VARCHAR(20) NOT NULL,\n&#96;birthday&#96; DATE DEFAULT NULL,\n&#96;sex&#96; CHAR(1) DEFAULT NULL,\nPRIMARY KEY (&#96;id&#96;),\nUNIQUE KEY &#96;unique_user_username&#96; (&#96;username&#96;)\n) ENGINE&#x3D;INNODB DEFAULT CHARSET&#x3D;utf8 ;\n\n设置参数\n-- 客户端连接服务端时，加上参数 -–local-infile\nmysql –-local-infile -u root -p\n\n-- 设置全局参数local_infile为1，开启从本地加载文件导入数据的开关\nset global local_infile &#x3D; 1;\n\n load加载数据\nload data local infile &#39;&#x2F;root&#x2F;load_user_100w_sort.sql&#39; into table tb_user fields terminated by &#39;,&#39; lines terminated by &#39;\\n&#39; ;\n\n在load时，主键顺序插入性能高于乱序插入\n主键优化主键顺序插入的性能是要高于乱序插入的具体原因\n数据组织方式\n在InnoDB存储引擎中，表数据都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表(index organized table IOT)。行数据，都是存储在聚集索引的叶子节点上的。而我们之前也讲解过InnoDB的逻辑结构图：\n","slug":"SQL优化","date":"2023-05-23T13:48:45.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"84c6604e94d88eec227a0b74aad54f6e","title":"Spring MVC","content":"Spring MVCWebMvcConfigurerWebMvcConfigurer是一个Spring MVC的配置接口，它提供了一些方法来进行各种配置。WebConfig类实现了这个接口，因此可以利用这些方法来配置应用程序。例如：addInterceptors()：用于添加拦截器。addResourceHandlers()：用于配置静态资源处理器。configureContentNegotiation()：用于配置内容协商策略。configureDefaultServletHandling()：用于配置静态资源处理。通过实现这些方法，可以细粒度地控制Spring MVC框架的行为。WebConfig类可以被视为是一个替代Spring MVC框架默认配置的类，它可以根据开发者的需求来提供不同的配置，从而实现个性化的应用程序需求。\n","slug":"Spring-MVC","date":"2023-05-20T12:40:51.000Z","categories_index":"","tags_index":"Spring,Spring MVC","author_index":"大宝贝的程序员"},{"id":"12e6f0c825691db3e38970eadda0c57d","title":"InnoDB存储引擎_MVCC原理","content":"InnoDB引擎逻辑存储结构InnoDB的逻辑存储结构如下图所示:\n\n 表空间\n表空间是InnoDB存储引擎逻辑结构的最高层， 如果用户启用了参数 innodb_file_per_table(在8.0版本中默认开启) ，则每张表都会有一个表空间（xxx.ibd），一个mysql实例可以对应多个表空间，用于存储记录、索引等数据。\n段\n段，分为数据段（Leaf node segment）、索引段（Non-leaf node segment）、回滚段（Rollback segment），InnoDB是索引组织表，数据段就是B+树的叶子节点， 索引段即为B+树的非叶子节点。段用来管理多个Extent（区）。\n区\n区，表空间的单元结构，每个区的大小为1M。 默认情况下， InnoDB存储引擎页大小为16K， 即一个区中一共有64个连续的页。\n 页\n页，是InnoDB 存储引擎磁盘管理的最小单元，每个页的大小默认为 16KB。为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。\n行\n行，InnoDB 存储引擎数据是按行进行存放的。\n在行中，默认有两个隐藏字段：\n​\t\tTrx_id：每次对某条记录进行改动时，都会把对应的事务id赋值给trx_id隐藏列。\n​\t\tRoll_pointer：每次对某条引记录进行改动时，都会把旧的版本写入到undo日志中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。\n架构MySQL5.5 版本开始，默认使用InnoDB存储引擎，它擅长事务处理，具有崩溃恢复特性，在日常开发中使用非常广泛。下面是InnoDB架构图，左侧为内存结构，右侧为磁盘结构。\n\n内存结构\n主要分为这么四大块儿： Buffer Pool、Change Buffer、Adaptive Hash Index、Log Buffer。 \n Buffer Pool\nInnoDB存储引擎基于磁盘文件存储，访问物理硬盘和在内存中进行访问，速度相差很大，为了尽可能弥补这两者之间的I&#x2F;O效率的差值，就需要把经常使用的数据加载到缓冲池中，避免每次访问都进行磁盘I&#x2F;O\n在InnoDB的缓冲池中不仅缓存了索引页和数据页，还包含了undo页、插入缓存、自适应哈希索引以及InnoDB的锁信息等等\n缓冲池 Buffer Pool，是主内存中的一个区域，里面可以缓存磁盘上经常操作的真实数据，在执行增删改查操作时，先操作缓冲池中的数据（若缓冲池没有数据，则从磁盘加载并缓存），然后再以一定频率刷新到磁盘，从而减少磁盘IO，加快处理速度。\n缓冲池以Page页为单位，底层采用链表数据结构管理Page。根据状态，将Page分为三种类型：\n​\tfree page：空闲page，未被使用。\n​\tclean page：被使用page，数据没有被修改过。\n​\tdirty page：脏页，被使用page，数据被修改过，页中数据与磁盘的数据产生了不一致。\n在专用服务器上，通常将多达80％的物理内存分配给缓冲池 。参数设置：show variables like  &#39;innodb_buffer_pool_size&#39;;\nChange Buffer\nChange Buffer，更改缓冲区（针对于非唯一的二级索引页），在执行DML语句时，如果这些数据Page没有在Buffer Pool中，不会直接操作磁盘，而会将数据变更存在更改缓冲区 Change Buffer中，在未来数据被读取时，再将数据合并恢复到Buffer Pool中，再将合并后的数据刷新到磁盘中。\nChange Buffer的意义是什么呢?\n与聚集索引不同，二级索引通常是非唯一的，并且以相对随机的顺序插入二级索引。同样，删除和更新可能会影响索引树中不相邻的二级索引页，如果每一次都操作磁盘，会造成大量的磁盘IO。有了ChangeBuffer之后，我们可以在缓冲池中进行合并处理，减少磁盘IO。\n Adaptive Hash Index\n参数： adaptive_hash_index\n自适应hash索引，用于优化对Buffer Pool数据的查询。MySQL的InnoDB引擎中虽然没有直接支持hash索引，但是给我们提供了一个功能就是这个自适应hash索引。因为前面我们讲到过，hash索引在进行等值匹配时，一般性能是要高于B+树的，因为hash索引一般只需要一次IO即可，而B+树，可能需要几次匹配，所以hash索引的效率要高，但是hash索引又不适合做范围查询、模糊匹配等。\nInnoDB存储引擎会监控对表上各索引页的查询，如果观察到在特定的条件下hash索引可以提升速度，则建立hash索引，称之为自适应hash索引。自适应哈希索引，无需人工干预，是系统根据情况自动完成。\n Log Buffer\ninnodb_log_buffer_size：缓冲区大小\ninnodb_flush_log_at_trx_commit：日志刷新到磁盘时机，取值主要包含以下三个：\n​\t\t1: 日志在每次事务提交时写入并刷新到磁盘，默认值。\n​\t\t0: 每秒将日志写入并刷新到磁盘一次。\n​\t\t2: 日志在每次事务提交后写入，并每秒刷新到磁盘一次。\nLog Buffer：日志缓冲区，用来保存要写入到磁盘中的log日志数据（redo log 、undo log），默认大小为 16MB，日志缓冲区的日志会定期刷新到磁盘中。如果需要更新、插入或删除许多行的事务，增加日志缓冲区的大小可以节省磁盘 I&#x2F;O。\n磁盘结构InnoDB体系结构的磁盘结构：\n\n System Tablespace\n参数：innodb_data_file_path\n系统表空间是一个特殊的InnoDB表空间，它是用来存储系统表和索引的地方。虽然你可以在其他表空间中创建表，在系统表空间中包含这些表和索引数据，允许InnoDB使用更少的内存和磁盘空间，因为系统表空间是被当做一个单独的文件处理。系统表空间，默认的文件名叫 ibdata1。\n当你使用InnoDB引擎时，系统表空间在存储InnoDB的缓存池中发挥着重要的作用。MySQL使用缓存机制来最小化对硬盘I&#x2F;O访问的次数，来提高InnoDB性能。系统表空间是其中之一，InnoDB会将读取的数据缓存到系统表空间中，并使用系统表空间将存储的数据进行共享，避免了不必要的副本。\n系统表空间是通过配置文件参数innodb_data_file_path来定义的。通常，系统表空间会被定义为一个更改缓冲区和一个UNDO日志空间的共同文件。在MySQL 5.x版本中，由于InnoDB存储一些特殊的元数据和事务信息，所以系统表空间还包含了InnoDB数据字典、undolog等，用来支持事务和锁等机制的实现。\nFile-Per-Table Tablespaces\n开关参数：innodb_file_per_table ，该参数默认开启。\n如果开启了innodb_file_per_table开关 ，则每个表的文件表空间包含单个InnoDB表的数据和索引 ，并存储在文件系统上的单个数据文件中。我们每创建一个表，都会产生一个表空间文件\n General Tablespaces\n通用表空间，需要通过 CREATE TABLESPACE 语法创建通用表空间，在创建表时，可以指定该表空间。\n创建表空间\nCREATE TABLESPACE ts_name ADD DATAFILE 'file_name' ENGINE = engine_name;\n-- 创建表时指定表空间\nCREATE TABLE xxx ... TABLESPACE ts_name;\n\nUndo Tablespaces\n撤销表空间，MySQL实例在初始化时会自动创建两个默认的undo表空间初始大小16M，用于存储undo log日志\nTemporary Tablespaces\nInnoDB 使用会话临时表空间和全局临时表空间。存储用户创建的临时表等数据。\nDoublewrite Buffer Files\n双写缓冲区，InnoDB引擎将数据页从Buffer Pool刷新到磁盘前，先将数据页写入双写缓冲区文件中，便于系统异常时恢复数据。\n涉及文件：#ib_16384_0.dblwr、#ib_16384_1.dblwr\nRedo Log\n重做日志，是用来实现事务的持久性。该日志文件由两部分组成：重做日志缓冲（redo log buffer）以及重做日志文件（redo log）,前者是在内存中，后者在磁盘中。当事务提交之后会把所有修改信息都会存到该日志中, 用于在刷新脏页到磁盘时,发生错误时, 进行数据恢复使用\n以循环方式写入重做日志文件，涉及两个文件：ib_logfile0、iblogfile1\n后台线程\n在InnoDB的后台线程中，分为4类：\nMaster Thread 、IO Thread、Purge Thread、Page Cleaner Thread\n Master Thread\n核心后台线程，负责调度其他线程，还负责将缓冲池中的数据异步刷新到磁盘中, 保持数据的一致性，还包括脏页的刷新、合并插入缓存、undo页的回收 。\n IO Thread\n在InnoDB存储引擎中大量使用了AIO来处理IO请求, 这样可以极大地提高数据库的性能，而IO Thread主要负责这些IO请求的回调。\n查看到InnoDB的状态信息，其中就包含IO Thread信息\nshow engine innodb status \\G;\n\n\n\n\n线程类型\n默认个数\n职责\n\n\n\nRead thread\n4\n负责读操作\n\n\nWrite thread\n4\n负责写操作\n\n\nLog thread\n1\n负责将日志缓冲区刷新到磁盘\n\n\nInsert buffer thread\n1\n负责将写缓冲区内容刷新到磁盘\n\n\n Purge Thread\n主要用于回收事务已经提交了的undo log，在事务提交之后，undo log可能不用了，就用它来回收。\nPage Cleaner Thread\n协助 Master Thread 刷新脏页到磁盘的线程，它可以减轻 Master Thread 的工作压力，减少阻塞。\n事务原理我们研究事务的原理，就是研究MySQL的InnoDB引擎是如何保证事务的这四大特性的。\n而对于这四大特性，实际上分为两个部分。 其中的原子性、一致性、持久化，实际上是由InnoDB中的两份日志来保证的，一份是redo log日志，一份是undo log日志。 而隔离性是通过数据库的锁，加上MVCC来保证的。\n\nredo log\n重做日志，记录的是事务提交时数据页的物理修改，是用来实现事务的持久性。\n该日志文件由两部分组成：重做日志缓冲（redo log buffer）以及重做日志文件（redo log file）,前者是在内存中，后者在磁盘中。当事务提交之后会把所有修改信息都存到该日志文件中, 用于在刷新脏页到磁盘,发生错误时, 进行数据恢复使用。\n如果没有redolog，可能会存在什么问题的？\n\n在InnoDB引擎中的内存结构中，主要的内存区域就是缓冲池，在缓冲池中缓存了很多的数据页。 当我们在一个事务中，执行多个增删改的操作时，InnoDB引擎会先操作缓冲池中的数据，如果缓冲区没有对应的数据，会通过后台线程将磁盘中的数据加载出来，存放在缓冲区中，然后将缓冲池中的数据修改，修改后的数据页我们称为脏页。 而脏页则会在一定的时机，通过后台线程刷新到磁盘中，从而保证缓冲区与磁盘的数据一致。 而缓冲区的脏页数据并不是实时刷新的，而是一段时间之后将缓冲区的数据刷新到磁盘中，假如刷新到磁盘的过程出错了，而提示给用户事务提交成功，数据却没有持久化下来，这就出现问题了，没有保证事务的持久性。\n在InnoDB中提供了一份日志 redo log，接下来我们再来看看，通过redolog如何解决这个问题。\n\n有了redolog之后，当对缓冲区的数据进行增删改之后，会首先将操作的数据页的变化，记录在redolog buffer中。在事务提交时，会将redo log buffer中的数据刷新到redo log磁盘文件中。过一段时间之后，如果刷新缓冲区的脏页到磁盘时，发生错误，此时就可以借助于redo log进行数据恢复，这样就保证了事务的持久性。 而如果脏页成功刷新到磁盘 或 或者涉及到的数据已经落盘，此时redolog就没有作用了，就可以删除了，所以存在的两个redo log文件是循环写的。\n那为什么每一次提交事务，要刷新redo log 到磁盘中呢，而不是直接将buffer pool中的脏页刷新到磁盘呢 ?\n因为在业务操作中，我们操作数据一般都是随机读写磁盘的，而不是顺序读写磁盘。 而redo log在往磁盘文件中写入数据，由于是日志文件，所以都是顺序写的。顺序写的效率，要远大于随机写。 这种先写日志的方式，称之为 WAL（Write-Ahead Logging）\nundo log\n回滚日志，用于记录数据被修改前的信息 , 作用包含两个 : 提供回滚(保证事务的原子性) 和MVCC(多版本并发控制) \nundo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。当执行rollback时，就可以从undo log中的逻辑记录读取到相应的内容并进行回滚。\nUndo log销毁：undo log在事务执行时产生，事务提交时，并不会立即删除undo log，因为这些日志可能还用于MVCC。\nUndo log存储：undo log采用段的方式进行管理和记录，存放在前面介绍的 rollback segment回滚段中，内部包含1024个undo log segment。\nMVCCMVCC全称为Multi-Version Concurrency Control，即多版本并发控制。它是一种用于实现数据库事务的并发控制方式，主要应用于多用户、多事务同时执行的环境下，用来保证事务的隔离性和并发性。\nMVCC的主要思想是为每个数据库记录维护多个版本，每个版本都对应着不同的事务更新。这样，即使有多个事务并发执行，每个事务看到的都是一致性的数据，而不会发生脏读、不可重复读等问题。\n当前读\n读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。对于我们日常的操作，如：select … lock in share mode(共享锁)，select … for update、update、insert、delete(排他锁)都是一种当前读。\n\n即使是在默认的RR隔离级别下，事务A中依然可以读取到事务B最新提交的内容，因为在查询语句后面加上了 lock in share mode 共享锁，此时是当前读操作。当然，当我们加排他锁的时候，也是当前读操作。\n 快照读\n简单的select（不加锁）就是快照读，快照读，读取的是记录数据的可见版本，有可能是历史数据，不加锁，是非阻塞读。\nRead Committed：每次select，都生成一个快照读。\nRepeatable Read：开启事务后第一个select语句才是快照读的地方。\nSerializable：快照读会退化为当前读。\n演示RR\n\n到即使事务B提交了数据,事务A中也查询不到。 原因就是因为普通的select是快照读，而在当前默认的RR隔离级别下，开启事务后第一个select语句才是快照读的地方，后面执行相同的select语句都是从快照中获取数据，可能不是当前的最新数据，这样也就保证了可重复读。\n MVCC\n全称 Multi-Version Concurrency Control，多版本并发控制。指维护一个数据的多个版本，使得读写操作没有冲突，快照读为MySQL实现MVCC提供了一个非阻塞读功能。MVCC的具体实现，还需要依赖于数据库记录中的三个隐式字段、undo log日志、readView。\n隐藏字段\n我们创建了一张表，在查看表结构的时候，就可以显式的看到这张表的字段。实际上除了显式字段以外，InnoDB还会自动的给我们添加三个隐藏字段及其含义分别是\n\n\n\n隐藏字段\n含义\n\n\n\nDB_TRX_ID\n最近修改事务ID，记录插入这条记录或最后一次修改该记录的事务ID。\n\n\nDB_ROLL_PTR\n回滚指针，指向这条记录的上一个版本，用于配合undo log，指向上一个版本\n\n\nDB_ROW_ID\n隐藏主键，如果表结构没有指定主键，将会生成该隐藏字段\n\n\n测试\n查看有主键的表 stu\n进入服务器中的 &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;hj&#x2F; , 查看stu的表结构信息, 通过如下指令:\nibd2sdi stu.ibd\n\n除了我们建表时指定的字段以外，还有额外的两个字段 分别是：DB_TRX_ID 、 DB_ROLL_PTR ，因为该表有主键，所以没有DB_ROW_ID隐藏字段\n查看没有主键的表 employee\nibd2sdi employee.ibd\n\n处理我们建表时指定的字段以外，还有额外的三个字段 分别是：DB_TRX_ID 、 DB_ROLL_PTR 、DB_ROW_ID，因为employee表是没有指定主键的\n 版本链\n有一张表原始数据为：\n\nDB_TRX_ID : 代表最近修改事务ID，记录插入这条记录或最后一次修改该记录的事务ID，是自增的。\nDB_ROLL_PTR ： 由于这条数据是才插入的，没有被更新过，所以该字段值为null。\n有四个并发事务同时在访问这张表\n\n当事务2执行第一条修改语句时，会记录undo log日志，记录数据变更之前的样子; 然后更新记录，并且记录本次操作的事务ID，回滚指针，回滚指针用来指定如果发生回滚，回滚到哪一个版本。\n\n紧接着事务三操作\n\n当事务3执行第一条修改语句时，也会记录undo log日志，记录数据变更之前的样子; 然后更新记录，并且记录本次操作的事务ID，回滚指针，回滚指针用来指定如果发生回滚，回滚到哪一个版本。\n\n最终发现不同事务或相同事务对同一条记录进行修改，会导致该记录的undolog生成一条记录版本链表，链表的头部是最新的旧记录，链表尾部是最早的旧记录。\nreadview\nReadView（读视图）是快照读SQL执行时MVCC提取数据的依据，记录并维护系统当前活跃的未提交事务的id。在使用 MVCC 进行事务并发控制时，数据库需要维护一个 ReadView(读视图)，它是快照读在 SQL 执行过程中提取数据的依据。\nReadView 记录着系统当前活跃的事务的 id。一个事务被视作“活跃”的条件是它已经启动，但尚未提交或回滚。也就是说，只有在提交或回滚时，事务才算结束，它的 id 才被从 ReadView 中移除。\nReadView 中的事务 id 用来判断一个快照读的数据版本是否可见。当一个事务执行快照读时，它会读取 ReadView 的事务 id，用于确定事务开始前那一刻的数据版本。如果在它之前启动的活跃事务已经对数据进行了修改，那么这些修改的数据版本对该事务来说是不可见的，因为它的事务 id 在 ReadView 中。而读视图只会看到在它之前启动的事务的更新，它之后启动的事务的更新它是不可见的。\n因此，保持 ReadView 中的 id 维护了系统当前的并发状态，使得快照读能够在一致性的基础上提取数据，避免了不合适的结果。\nReadView中包含了四个核心字段：\n\n\n\n字段\n含义\n\n\n\nm_ids\n当前活跃的事务ID集合\n\n\nmin_trx_id\n最小活跃事务ID\n\n\nmax_trx_id\n预分配事务ID，当前最大事务ID+1（因为事务ID是自增的）\n\n\ncreator_trx_id\nReadView创建者的事务ID\n\n\n而在readview中就规定了版本链数据的访问规则：\n​\ttrx_id 代表当前undo log版本链对应事务ID。\n\n\n\n条件\n是否可以访问\n说明\n\n\n\ntrx_id &#x3D;&#x3D; creator_trx_id\n可以访问该版本\n成立，说明数据是当前这个事务更改的。\n\n\ntrx_id &lt; min_trx_id\n可以访问该版本\n成立，说明数据已经提交了。\n\n\ntrx_id &gt; max_trx_id\n不可以访问该版本\n成立，说明该事务是在ReadView生成后才开启。\n\n\nmin_trx_id &lt;&#x3D; trx_id &lt;&#x3D; max_trx_id\n如果trx_id不在m_ids中，是可以访问该版本的\n成立，说明数据已经提交。\n\n\n不同的隔离级别，生成ReadView的时机不同：\n​\tREAD COMMITTED ：在事务中每一次执行快照读时生成ReadView。\n​\tREPEATABLE READ：仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。\n原理分析\nRC隔离级别\nRC隔离级别下，在事务中每一次执行快照读时生成ReadView。\n我们就来分析事务5中，两次快照读读取数据，是如何获取数据的?\n在事务5中，查询了两次id为30的记录，由于隔离级别为Read Committed，所以每一次进行快照读都会生成一个ReadView，那么两次生成的ReadView如下。\n\n那么这两次快照读在获取数据时，就需要根据所生成的ReadView以及ReadView的版本链访问规则，到undo log版本链中匹配数据，最终决定此次快照读返回的数据。\n先来看第一次快照读具体的读取过程：\n\n\n在进行匹配时，会从undo log的版本链，从上到下进行挨个匹配：\n先匹配 DB_TRX_ID &#x3D; 4，也就是将4带入右侧的匹配规则中。 ①不满足 ②不满足 ③不满足 ④也不满足 ，都不满足，则继续匹配undo log版本链的下一条。\n再匹配第二条DB_TRX_ID &#x3D; 3，将3带入右侧的匹配规则中。①不满足 ②不满足 ③不满足 ④也不满足 ，都不满足，则继续匹配undo log版本链的下一条\n再匹配第三条DB_TRX_ID &#x3D; 2，将2带入右侧的匹配规则中。①不满足 ②满足 终止匹配，此次快照读，返回的数据就是版本链中记录的这条数据\n再来看第二次快照读具体的读取过程:\n\n\n在进行匹配时，会从undo log的版本链，从上到下进行挨个匹配：\n先匹配 DB_TRX_ID &#x3D; 4，将4带入右侧的匹配规则中。 ①不满足 ②不满足 ③不满足 ④也不满足 ，都不满足，则继续匹配undo log版本链的下一条。\n再匹配DB_TRX_ID &#x3D; 3，将3带入右侧的匹配规则中。①不满足 ②满足 。终止匹配，此次快照读，返回的数据就是版本链中记录的这条数据。\n RR隔离级别\nRR隔离级别下，仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。 而RR 是可重复读，在一个事务中，执行两次相同的select语句，查询到的结果是一样的。\n\n在RR隔离级别下，只是在事务中第一次快照读时生成ReadView，后续都是复用该ReadView，那么既然ReadView都一样， ReadView的版本链匹配规则也一样， 那么最终快照读返回的结果也是一样的\n所以MVCC的实现原理就是通过 InnoDB表的隐藏字段、UndoLog 版本链、ReadView来实现的。而MVCC + 锁，则实现了事务的隔离性。 而一致性则是由redolog 与 undolog保证\n\n","slug":"InnoDB存储引擎-MVCC原理","date":"2023-05-20T02:00:56.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"083e3a4ebfcb19d327b53b7d73aaddd3","title":"MySQL_锁","content":"锁​\t\t锁是计算机协调多个进程或线程并发访问某一资源的机制。在数据库中，除传统的计算资源（CPU、RAM、I&#x2F;O）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。\nMySQL中的锁，按照锁的粒度分，分为以下三类：\n\n全局锁：锁定数据库中的所有表。\n\n表级锁：每次操作锁住整张表。\n\n行级锁：每次操作锁住对应的行数据\n\n\n全局锁​\t\t全局锁就是对整个数据库实例加锁，加锁后整个实例就处于只读状态，后续的DML的写语句，DDL语句，已经更新操作的事务提交语句都将被阻塞。\n​\t\t其典型的使用场景是做全库的逻辑备份，对所有的表进行锁定，从而获取一致性视图，保证数据的完整性。\n为什么全库逻辑备份，就需要加全就锁呢？\n不加全局锁，可能存在的问题。\n假设在数据库中存在这样三张表: tb_stock 库存表，tb_order 订单表，tb_orderlog 订单日志表。\n\n​\t\t在进行数据备份时，先备份了tb_stock库存表。\n​\t\t然后接下来，在业务系统中，执行了下单操作，扣减库存，生成订单（更新tb_stock表，插入tb_order表）。\n​\t\t然后再执行备份 tb_order表的逻辑。\n​\t\t业务中执行插入订单日志操作。\n最后，又备份了tb_orderlog表。\n此时备份出来的数据，是存在问题的。因为备份出来的数据，tb_stock表与tb_order表的数据不一致(有最新操作的订单信息,但是库存数没减)\n此时就可以借助于MySQL的全局锁来解决\n\n对数据库进行进行逻辑备份之前，先对整个数据库加上全局锁，一旦加了全局锁之后，其他的DDL、DML全部都处于阻塞状态，但是可以执行DQL语句，也就是处于只读状态，而数据备份就是查询操作。那么数据在进行逻辑备份的过程中，数据库中的数据就是不会发生变化的，这样就保证了数据的一致性和完整性。\n语法\n 加全局锁\nflush tables with read lock ;\n\n 数据备份\nmysqldump [--single-transaction] -uroot –p1234 hj > hj.sql\n--   mysqldump: 是MySQL备份工具。\n--   -u : 指定连接数据库所用的用户名\n--   –p : 指定连接数据库所用的密码\n--   hj : 要备份的数据库名称。\n--   > hj.sql : 将备份文件输出到itcast.sql文件中，使用\">\"符号是将备份文件的内容导出到指定文件。\n\n释放锁\nunlock tables;\n\n数据库中加全局锁，是一个比较重的操作，存在以下问题：\n\n如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆。\n\n如果在从库上备份，那么在备份期间从库不能执行主库同步过来的二进制日志（binlog），会导致主从延迟。\n\n\n在InnoDB引擎中，我们可以在备份时加上参数 –single-transaction 参数来完成不加锁的一致性数据备份。\n该命令执行期间，在备份开始时使用START TRANSACTION语句来开启一个事务，这个事务会在备份期间一直执行，隔离级别默认为“可重复读”级别。在备份完成后，使用COMMIT语句来提交事务，这样就可以保证备份是一致性的，不会受到正在进行的其他事务的影响。\n需要注意的是，使用--single-transaction参数只适用于没有写锁的表。如果有表正在执行DDL语句，或者有使用ALTER TABLE，OPTIMIZE TABLE等语句的话，这些表仍然会被加锁，备份也会受到影响。\n表级锁表级锁，每次操作锁住整张表。锁定粒度大，发生锁冲突的概率最高，并发度最低。应用在MyISAM、InnoDB、BDB等存储引擎中。\n对于表级锁，主要分为以下三类：\n\n表锁\n\n​\t\t- 表共享读锁（read lock）\n​\t\t- 表独占写锁（write lock）\n\n元数据锁（meta data lock，MDL）\n\n意向锁\n\n\n表锁语法：\n-- 加锁：\nlock tables 表名... read/write\n-- 释放锁\nunlock tables / 客户端断开连接\n\n 读锁\n\n读写演示：\n\n对指定表加了读锁，加锁的会话未解锁时执行DML&#x2F;DDL会报错；\n\n读锁不会影响其他会话的读，但是会阻塞（不是报错）其他会话的写。\n写锁\n\n左侧为客户端一，对指定表加了写锁，会阻塞右侧客户端的读和写。\n结论: 读锁不会阻塞其他客户端的读，但是会阻塞写。写锁既会阻塞其他客户端的读，又会阻塞其他客户端的写。\n元数据锁\t\nmeta data lock , 元数据锁，简写MDL。\nMDL加锁过程是系统自动控制，无需显式使用，在访问一张表的时候会自动加上。MDL锁主要作用是维护表元数据的数据一致性，在表上有活动事务的时候，不可以对元数据进行写入操作。为了避免DML与DDL冲突，保证读写的正确性。\nMDL锁能够确保在进行表结构修改时，不会同时存在其他读和写操作，避免并发操作导致的数据冲突和错误。在MySQL5.5中引入了MDL，当对一张表进行增删改查的时候，加MDL读锁(共享)；当对表结构进行变更操作的时候，加MDL写锁(排他)。\n常见的SQL操作时，所添加的元数据锁：\n\n\n\n对应SQL\n锁类型\n说明\n\n\n\nlock tables xxx read &#x2F; write\nSHARED_READ_ONLY &#x2F; SHARED_NO_READ_WRITE\n\n\n\nselect 、select … lock in share mode\nSHARED_READ\n与SHARED_READ、SHARED_WRITE兼容，与EXCLUSIVE互斥\n\n\ninsert 、update、delete、select … for update\nSHARED_WRITE\n与SHARED_READ、SHARED_WRITE兼容，与EXCLUSIVE互斥\n\n\nalter table …\nEXCLUSIVE\n与其他的MDL都互斥\n\n\n我们可以通过下面的SQL，来查看数据库中的元数据锁的情况：\nselect object_type,object_schema,object_name,lock_type,lock_duration from performance_schema.metadata_locks ;\n\n锁的兼容演示\n\n\nSHARED_READ与SHARED_WRITE兼容\n\n\nSHARED_READ与EXCLUSIVE互斥\n意向锁\n为了避免DML在执行时，加的行锁与表锁的冲突，在InnoDB中引入了意向锁，使得表锁不用检查每行数据是否加锁，使用意向锁来减少表锁的检查。\n假如没有意向锁，客户端一对表加了行锁后，客户端二如何给表加表锁呢，来通过示意图简单分析一下：\n首先客户端一，开启一个事务，然后执行DML操作，在执行DML语句时，会对涉及到的行加行锁。\n\n当客户端二，想对这张表加表锁时，会检查当前表是否有对应的行锁，如果没有，则添加表锁，此时就会从第一行数据，检查到最后一行数据，效率较低。\n\n有了意向锁之后 :\n客户端一，在执行DML操作时，会对涉及的行加行锁，同时也会对该表加上意向锁。\n\n而其他客户端，在对这张表加表锁的时候，会根据该表上所加的意向锁来判定是否可以成功加表锁，而不用逐行判断行锁情况了。\n\n 分类\n意向共享锁(IS): 由语句select … lock in share mode添加 。与表锁共享锁(read)兼容，与表锁排他锁(write)互斥。\n意向排他锁(IX): 由insert、update、delete、select…for update添加 。与表锁共享锁(read)及排他锁(write)都互斥，意向锁之间不会互斥。\n一旦事务提交了，意向共享锁、意向排他锁，都会自动释放。\n查看意向锁及行锁的加锁情况：\nselect object_schema,object_name,index_name,lock_type,lock_mode,lock_data from performance_schema.data_locks;\n\n意向锁演示\n意向共享锁(IS)与表锁共享锁兼容，与表锁排他锁互斥\n\n\n 意向排他锁(IX)与表读锁、写锁都是互斥的\n\n\n行级锁行级锁，每次操作锁住对应的行数据。锁定粒度最小，发生锁冲突的概率最低，并发度最高。应用在InnoDB存储引擎中.\nInnoDB的数据是基于索引组织的，行锁是通过对索引上的索引项加锁来实现的，而不是对记录加的\n锁。对于行级锁，主要分为以下三类：\n\n行锁（Record Lock）：锁定单个行记录的锁，防止其他事务对此行进行update和delete。在RC、RR隔离级别下都支持\n\n\n\n间隙锁（Gap Lock）：锁定索引记录间隙（不含该记录），确保索引记录间隙不变，防止其他事务在这个间隙进行insert，产生幻读。在RR隔离级别下都支持\n\n\n\n临键锁（Next-Key Lock）：行锁和间隙锁组合，同时锁住数据，并锁住数据前面的间隙Gap。在RR隔离级别下支持。\n\n\n行锁\nInnoDB实现了以下两种类型的行锁：\n\n共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排它锁。\n\n排他锁（X）：允许获取排他锁的事务更新数据，阻止其他事务获得相同数据集的共享锁和排他锁。\n\n\n两种行锁的兼容情况如下:\n\n\n\n当前锁的类型\n请求锁的类型\n兼容情况\n\n\n\nS(共享锁)\nS(共享锁)\n兼容\n\n\nS(共享锁)\nX(排他锁)\n冲突\n\n\nX(排他锁)\nX(排他锁)\n冲突\n\n\nX(排他锁)\nS(共享锁)\n冲突\n\n\n常见的SQL语句，在执行时，所加的行锁如下：\n\n\n\nSQL\n行锁类型\n说明\n\n\n\nINSERT …\n排他锁\n自动加锁\n\n\nUPDATE …\n排他锁\n自动加锁\n\n\nDELETE …\n排他锁\n自动加锁\n\n\nSELECT（正常）\n不加任何锁\n\n\n\nSELECT … LOCK IN SHARE MODE\n共享锁\n需要手动在SELECT之后加LOCK IN SHARE MODE\n\n\nSELECT … FOR UPDATE\n排他锁\n需要手动在SELECT之后加FOR UPDATE\n\n\n默认情况下，InnoDB使用next-key 锁来实现 REPEATABLE READ 隔离级别，它是一种组合锁，包含索引记录锁和间隙锁。该锁的主要作用是在执行索引扫描时，对于所查找的每条记录，都要获取记录锁以及对应的间隙锁，以保证事务在读取数据时不会出现幻读问题。\n\n针对唯一索引进行检索时，对已存在的记录进行等值匹配时，将会自动优化为行锁。\n\nInnoDB的行锁是针对于索引加的锁，不通过索引条件检索数据，那么InnoDB将对表中的所有记录加锁，此时 就会升级为表锁。\n\n\n查看意向锁及行锁的加锁情况：\nselect object_schema,object_name,index_name,lock_type,lock_mode,lock_data from performance_schema.data_locks;\n\n 演示：表结构如下\nCREATE TABLE `stu` (\n`id` int NOT NULL PRIMARY KEY AUTO_INCREMENT,\n`name` varchar(255) DEFAULT NULL,\n`age` int NOT NULL\n) ENGINE = InnoDB CHARACTER SET = utf8mb4;\n\n select…lock in share mode，加共享锁，共享锁与共享锁之间兼容。\n\n\n共享锁与排他锁之间互斥。\n\n\n无索引行锁升级为表锁\n\n\n开启事务，并执行update语句，更新name为tom的数据 。然后在另一个会话中更新name为rose的记录，却不能直接执行，会处于阻塞状态。原因就是，会话一根据name字段进行更新时，name字段是没有索引的，如果没有索引，此时行锁会升级为表锁（因为行锁是对索引项加的锁，而name没有索引）\n再针对name字段建立索引，索引建立之后，再次做一个测试：\n\n\n这样就说明，我们根据索引字段进行更新操作，就可以避免行锁升级为表锁的情况。\n间隙锁&amp;临键锁\n默认情况下，InnoDB在 REPEATABLE READ事务隔离级别运行，InnoDB使用 next-key 锁进行搜索和索引扫描，以防止幻读。临键锁（Next-Key Lock）是InnoDB引擎在实现间隙锁（Gap Lock）时所加的锁，其作用是锁定间隙并保护间隙内的下一个记录，以防止其他事务在间隙内插入记录。\n\n索引上的等值查询(唯一索引)，给不存在的记录加锁时, 优化为间隙锁 。\n\n索引上的等值查询(非唯一普通索引)，向右遍历时最后一个值不满足查询需求时，next-key锁退化为间隙锁。\n\n索引上的范围查询(唯一索引)–会访问到不满足条件的第一个值为止。\n\n\n间隙锁唯一目的是防止其他事务插入间隙。间隙锁可以共存，一个事务采用的间隙锁不会阻止另一个事务在同一间隙上采用间隙锁。\n示例演示\n 索引上的等值查询(唯一索引)，给不存在的记录加锁时, 优化为间隙锁 。\n\n\n查询（加共享锁）或者更新不存在的记录时，行锁（S或者X）会优化成间隙锁（GAP）,当在加了间隙锁的范围内insert插入数据时会阻塞，update&#x2F;delete不会阻塞。\n索引上的等值查询(非唯一普通索引)，向右遍历时最后一个值不满足查询需求时，next-key锁退化为间隙锁。 \nInnoDB的B+树索引，叶子节点是有序的双向链表。 假如，我们要根据这个二级索引查询值为18的数据，并加上共享锁，我们是只锁定18这一行就可以了吗？ 并不是，因为是非唯一索引，这个结构中可能有多个18的存在，所以，在加锁时会继续往后找，找到一个不满足条件的值（当前案例中也就是29）。此时会对18加临键锁，并对29之前的间隙加锁。\n\n假设我们有一个索引列a，对于值1到5之间的区间，如果我们设置了临键锁，那么除了a&#x3D;1和a&#x3D;5两条索引记录上的锁，还会锁住1&lt;a&lt;2, 2&lt;a&lt;3, 3&lt;a&lt;4和4&lt;a&lt;5这四个间隙上，以保护在该区间范围内下一个可能插入的记录不被其他事务插入。\n示例\n\n\n索引上的范围查询(唯一索引)–会访问到不满足条件的第一个值为止\n\n查询的条件为id&gt;&#x3D;19，并添加共享锁。 此时我们可以根据数据库表中现有的数据，将数据分为三个部分：\n[19]\n(19,25]\n(25,+∞]\n所以数据库数据在加锁是，就是将19加了行锁，25的临键锁（包含25及25之前的间隙），正无穷的临键锁(正无穷及之前的间隙)。\n","slug":"MySQL-锁","date":"2023-05-19T01:39:57.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"7a80ec680e8c46b9ee58692b58f8ab63","title":"基于session实现认证","content":"什么是认证认证 ：用户认证就是判断一个用户的身份是否合法的过程，用户去访问系统资源时系统要求验证用户的身份信息，身份合法方可继续访问，不合法则拒绝访问。常见的用户身份认证方式有：用户名密码登录，二维码登录，手机短信登录，指纹认证等方式。\n什么是会话用户认证通过后，为了避免用户的每次操作都进行认证可将用户的信息保证在会话中。会话就是系统为了保持当前用户的登录状态所提供的机制，常见的有基于session方式、基于token方式等。\n基于session的认证方式如下图：\n\n它的交互流程是，用户认证成功后，在服务端生成用户相关的数据保存在session(当前会话)中，发给客户端sesssion_id 存放到 cookie 中，这样用户客户端请求时带上 session_id 就可以验证服务器端是否存在 session 数据，以此完成用户的合法校验，当用户退出系统或session过期销毁时,客户端的session_id也就无效了。\n基于token方式如下图：\n\n它的交互流程是，用户认证成功后，服务端生成一个token发给客户端，客户端可以放到 cookie 或 localStorage等存储中，每次请求时带上 token，服务端收到token通过验证后即可确认用户身份。\n优略分析\n基于session的认证方式由Servlet规范定制，服务端要存储session信息需要占用内存资源，客户端需要支持cookie；基于token的方式则一般不需要服务端存储token，并且不限制客户端的存储方式。如今移动互联网时代更多类型的客户端需要接入系统，系统多是采用前后端分离的架构进行实现，所以基于token的方式更适合。\n什么是授权授权是用户认证通过根据用户的权限来控制用户访问资源的过程，拥有资源的访问权限则正常访问，没有权限则拒绝访问\n基于角色的访问控制\nRBAC基于角色的访问控制（Role-Based Access Control）是按角色进行授权，判断逻辑：\nif(主体.hasRole(\"总经理角色id\"))&#123;\n查询工资\n&#125;\n\n查询工资所需要的角色变化为总经理和部门经理\n需要修改判断逻辑为“判断用户的角色是否是总经理或部门经理”\nif(主体.hasRole(\"总经理角色id\") || 主体.hasRole(\"部门经理角色id\"))&#123;\n查询工资\n&#125;\n\n就需要修改授权的相关代码，系统可扩展性差。\n基于资源的访问控制\nRBAC基于资源的访问控制（Resource-Based Access Control）是按资源（或权限）进行授权。\n授权代码可以表示为：\nif(主体.hasPermission(&quot;查询工资权限标识&quot;))&#123;\n查询工资\n&#125;\n\n优点：系统设计时定义好查询工资的权限标识，即使查询工资所需要的角色变化为总经理和部门经理也不需要修改授权代码，系统可扩展性强。\n基于Session的认证方式\n基于Session的认证机制由Servlet规范定制，Servlet容器已实现，用户通过HttpSession的操作方法即可实现。\n本案例工程使用maven进行构建，使用SpringMVC、Servlet3.0实现。\n","slug":"基于session实现认证","date":"2023-05-18T14:10:28.000Z","categories_index":"","tags_index":"认证授权","author_index":"大宝贝的程序员"},{"id":"ca0fb70bccc39b11cac4043db766085d","title":"MySQL_存储引擎_索引","content":"MySQL存储引擎MySQL体系结构\n连接层\n最上层是一些客户端和链接服务，包含本地sock 通信和大多数基于客户端&#x2F;服务端工具实现的类似于\nTCP&#x2F;IP的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程\n池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务\n器也会为安全接入的每个客户端验证它所具有的操作权限。\n服务层\n第二层架构主要完成大多数的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化，部\n分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如 过程、函数等。在该层，服务器会解\n析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询的顺序，是否利用索引等，\n最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存，如果缓存空间足够大，\n这样在解决大量读操作的环境中能够很好的提升系统的性能。\n引擎层\n存储引擎层， 存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API和存储引擎进行通\n信。不同的存储引擎具有不同的功能，这样我们可以根据自己的需要，来选取合适的存储引擎。数据库\n中的索引是在存储引擎层实现的\n存储层\n数据存储层， 主要是将数据(如: redolog、undolog、数据、索引、二进制日志、错误日志、查询\n日志、慢查询日志等)存储在文件系统之上，并完成与存储引擎的交互。\n其他数据库相比\nMySQL有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎上，插件式的存储引擎架构，将查询处理和其他的系统任务以及数据的存储提取分离，这种架构可以根据业务的需求和实际需要选择合适的存储引擎\n存储引擎介绍存储引擎就是存储数据、建立索引、更新&#x2F;查询数据等技术的实现方式 。存储引擎是基于表的，而不是\n基于库的，所以存储引擎也可被称为表类型。我们可以在创建表的时候，来指定选择的存储引擎，如果\n没有指定将自动选择默认的存储引擎。\tMySQL默认存储引擎: InnoDB\n 建表时指定存储引擎\nCREATE TABLE 表名(\n字段1 字段1类型 [ COMMENT 字段1注释 ] ,\n......\n字段n 字段n类型 [COMMENT 字段n注释 ]\n) ENGINE = INNODB [ COMMENT 表注释 ] ;\n\n查询当前数据库支持的存储引擎\nshow engines;\n\n存储引擎特点InnoDB\nInnoDB是一种兼顾高可靠性和高性能的通用存储引擎，在 MySQL 5.5 之后，InnoDB是默认的MySQL 存储引擎。\n特点\n\nDML操作遵循ACID模型，支持事务；\n\n行级锁，提高并发访问性能；\n\n支持外键FOREIGN KEY约束，保证数据的完整性和正确性；\n\n\n文件\nxxx.ibd：xxx代表的是表名，innoDB引擎的每张表都会对应这样一个表空间文件，存储该表的表结构（frm-早期的 、sdi-新版的）、数据和索引。\n查看参数：innodb_file_per_table\nshow variables like 'innodb_file_per_table';\n\n如果该参数开启，代表对于InnoDB引擎的表，每一张表都对应一个ibd文件。 \n每一个ibd文件就对应一张表，比如：我们有一张表 account，就有这样的一个account.ibd文件，而在这个ibd文件中不仅存放表结构、数据，还会存放该表对应的索引信息。 而该文件是基于二进制存储的，不能直接基于记事本打开，我们可以使用mysql提供的一个指令 ibd2sdi ，通过该指令就可以从ibd文件中提取sdi信息，而sdi数据字典信息中就包含该表show variables like ‘innodb_file_per_table’; \n 逻辑存储结构\n\n表空间 : InnoDB存储引擎逻辑结构的最高层，ibd文件其实就是表空间文件，在表空间中可以\n包含多个Segment段。\n段 : 表空间是由各个段组成的， 常见的段有数据段、索引段、回滚段等。InnoDB中对于段的管\n理，都是引擎自身完成，不需要人为对其控制，一个段中包含多个区。\n区 : 区是表空间的单元结构，每个区的大小为1M。 默认情况下， InnoDB存储引擎页大小为\n16K， 即一个区中一共有64个连续的页。\n页 : 页是组成区的最小单元，页也是InnoDB存储引擎磁盘管理的最小单元，每个页的大小默\n认为 16KB。为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。\n行 : InnoDB 存储引擎是面向行的，也就是说数据是按行进行存放的，在每一行中除了定义表时\n所指定的字段以外，还包含两个隐藏字段(后面会详细介绍)\nMyISAM\nMyISAM是MySQL早期的默认存储引擎。\n特点\n\n不支持事务，不支持外键\n\n支持表锁，不支持行锁\n\n访问速度快\n\n\n 文件\nxxx.sdi：存储表结构信息\nxxx.MYD：存储数据\nxxx.MYI：存储索引\nMemory\nMemory引擎的表数据是存储在内存中的，由于受到硬件问题、或断电问题的影响，只能将这些表作为临时表或缓存使用\n特点\n内存存放\n hash索引（默认）\n文件\nxxx.sdi：存储表结构信息\n区别及特点\n\n\n特点\nInnoDB\nMyISAM\nMemory\n\n\n\n存储限制\n64TB\n存在受限\n存在受限\n\n\n事务安全\n支持\n不支持\n不支持\n\n\n锁机制\n表锁，行锁\n表锁\n表锁\n\n\nB+tree索引\n支持\n支持\n支持\n\n\nHash索引\n不支持\n不支持\n支持\n\n\n全文索引\n支持(5.6版本之后)\n支持\n不支持\n\n\n空间使用\n高\n低\nN&#x2F;A\n\n\n内存使用\n高\n低\n中等\n\n\n批量插入速度\n低\n高\n高\n\n\n支持外键\n支持\n不支持\n不支持\n\n\n存储引擎选择\n在选择存储引擎时，应该根据应用系统的特点选择合适的存储引擎。对于复杂的应用系统，还可以根据实际情况选择多种存储引擎进行组合。\nInnoDB: 是Mysql的默认存储引擎，支持事务、外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询之外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。\nMyISAM ： 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。\nMEMORY：将所有数据保存在内存中，访问速度快，通常用于临时表及缓存。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，而且无法保障数据的安全性。\n索引索引（index）是帮助MySQL高效获取数据的数据结构(有序)。在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据， 这样就可以在这些数据结构上实现高级查找算法这种数据结构就是索引。\n在无索引情况下，就需要从第一行开始扫描，一直扫描到最后一行，我们称之为 全表扫描，性能很低。\n索引的优略 \n优势\n\n提高数据检索的效率，降低数据库的IO成本\n通过索引列对数据进行排序，降低数据排序的成本，降低CPU的消耗。\n\n劣势\n\n索引列也是要占用空间的。\n索引大大提高了查询效率，同时却也降低更新表的速度，如对表进行INSERT、UPDATE、DELETE时，效率降低。\n\n索引结构\nMySQL的索引是在存储引擎层实现的，不同的存储引擎有不同的索引结构，主要包含以下几种：\n\n\n\n索引结构\n描述\n\n\n\nB+Tree索引\n最常见的索引类型，大部分引擎都支持 B+ 树索引\n\n\nHash索引\n底层数据结构是用哈希表实现的, 只有精确匹配索引列的查询才有效, 不支持范围查询\n\n\nR-tree(空间索引）\n空间索引是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少\n\n\nFull-text(全文索引)\n是一种通过建立倒排索引,快速匹配文档的方式。类似于Lucene,Solr,ES\n\n\n不同的存储引擎对于索引结构的支持情况。\n\n\n\n索引\nInnoDB\nMyISAM\nMemory\n\n\n\nB+tree索引\n支持\n支持\n支持\n\n\nHash 索引\n不支持\n不支持\n支持\n\n\nR-tree 索引\n不支持\n支持\n不支持\n\n\nFull-text\n5.6版本之后支持\n支持\n不支持\n\n\n我们平常所说的索引，如果没有特别指明，都是指B+树结构组织的索引。\n二叉树\n​\t假如说MySQL的索引结构采用二叉树的数据结构，比较理想的结构如下：\n\n如果主键是顺序插入的，则会形成一个单向链表，结构如下：\n\n所以，如果选择二叉树作为索引结构，会存在以下缺点：\n\n顺序插入时，会形成一个链表，查询性能大大降低。\n\n大数据量情况下，层级较深，检索速度慢。\n\n\n我们可以选择红黑树，红黑树是一颗自平衡二叉树，那这样即使是顺序插入数据，最终形成的数据结构也是一颗平衡的二叉树,结构如下\n\n但是，即使如此，由于红黑树也是一颗二叉树，所以也会存在一个缺点：\n\n大数据量情况下，层级较深，检索速度慢\n\n所以，在MySQL的索引结构中，并没有选择二叉树或者红黑树，而选择的是B+Tree。\nB-Tree\n​\tB树是一种多叉路衡查找树，相对于二叉树，B树每个节点可以有多个分支，即多叉。以一颗最大度数（树的度数指的是一个节点的子节点个数）为5(5阶)的b-tree为例，那这个B树每个节点最多存储4个key，5个指针：\n\n特点：\n\n5阶的B树，每一个节点最多存储4个key，对应5个指针。\n\n一旦节点存储的key数量到达5，就会裂变，中间元素向上分裂。\n\n在B树中，非叶子节点和叶子节点都会存放数据。\n\n\nB+Tree\n​\tB+Tree是B-Tree的变种，我们以一颗最大度数（max-degree）为4（4阶）的b+tree为例，来看一下其结构示意图：\n\n我们可以看到，两部分：\n\n绿色框框起来的部分，是索引部分，仅仅起到索引数据的作用，不存储数据。\n\n红色框框起来的部分，是数据存储部分，在其叶子节点中要存储具体的数据。\n\n\nB+Tree 与 B-Tree相比主要有以下三点区别：\n\n所有的数据都会出现在叶子节点。\n\n叶子节点形成一个单向链表。\n\n非叶子节点仅仅起到索引数据作用，具体的数据都是在叶子节点存放的。\n\n\nMySQL中优化之后的B+Tree\n​\tMySQL索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能，利于排序。\n\nHash\n​\tMySQL中除了支持B+Tree索引，还支持一种索引类型—Hash索引。\n 结构\n​\t哈希索引就是采用一定的hash算法，将键值换算成新的hash值，映射到对应的槽位上，然后存储在hash表中。如果两个(或多个)键值，映射到一个相同的槽位上，他们就产生了hash冲突（也称为hash碰撞），可以通过链表来解决。\n\n特点\n\nHash索引只能用于对等比较(&#x3D;，in)，不支持范围查询（between，&gt;，&lt; ，…）\n\n无法利用索引完成排序操作\n\n查询效率高，通常(不存在hash冲突的情况)只需要一次检索就可以了，效率通常要高于B+tree索引\n\n\n 支持hash的存储引擎\n在MySQL中，支持hash索引的是Memory存储引擎。 而InnoDB中具有自适应hash功能，hash索引是InnoDB存储引擎根据B+Tree索引在指定条件下自动构建的。\n为什么InnoDB存储引擎选择使用B+tree索引结构?\n\n相对于二叉树，层级更少，搜索效率高；\n\n对于B-tree，无论是叶子节点还是非叶子节点，都会保存数据，这样导致一页中存储\n\n的键值减少，指针跟着减少，要同样保存大量数据，只能增加树的高度，导致性能降低；\n\n相对Hash索引，B+tree支持范围匹配及排序操作；\n\n\n索引分类在MySQL数据库，将索引的具体类型主要分为以下几类：主键索引、唯一索引、常规索引、全文索引。\n\n\n\n分类\n含义\n特点\n关键字\n\n\n\n主键索引\n针对于表中主键创建的索引\n默认自动创建, 只能有一个\nPRIMARY\n\n\n唯一索引\n避免同一个表中某数据列中的值重复\n可以有多个\nUNIQUE\n\n\n常规索引\n快速定位特定数据\n可以有多个\n\n\n\n全文索引\n全文索引查找的是文本中的关键词，而不是比较索引中的值\n可以有多个\nFULLTEXT\n\n\n聚集索引&amp;二级索引\n在InnoDB存储引擎中，根据索引的存储形式，又可以分为以下两种：\n\n\n\n分类\n含义\n特点\n\n\n\n聚集索引(Clustered Index)\n将数据存储与索引放到了一块，索引结构的叶子节点保存了行数据\n必须有,而且只有一个\n\n\n二级索引(Secondary Index)\n将数据与索引分开存储，索引结构的叶子节点关联的是对应的主键\n可以存在多个\n\n\n聚集索引选取规则:\n\n如果存在主键，主键索引就是聚集索引。\n\n如果不存在主键，将使用第一个唯一（UNIQUE）索引作为聚集索引。\n\n如果表没有主键，或没有合适的唯一索引，则InnoDB会自动生成一个rowid作为隐藏的聚集索引\n\n\n聚集索引和二级索引的具体结构如下：\n\n聚集索引的叶子节点下挂的是这一行的数据 。\n二级索引的叶子节点下挂的是该字段值对应的主键值。\n当我们执行如下的SQL语句时，具体的查找过程是这样子的：\n\n\n由于是根据name字段进行查询，所以先根据name&#x3D;’Arm’到name字段的二级索引中进行匹配查找。但是在二级索引中只能查找到 Arm 对应的主键值 10。\n\n由于查询返回的数据是*，所以此时，还需要根据主键值10，到聚集索引中查找10对应的记录，最终找到10对应的行row。\n\n最终拿到这一行的数据，直接返回即可。\n\n\n回表查询： 这种先到二级索引中查找数据，找到主键值，然后再到聚集索引中根据主键值，获取数据的方式，就称之为回表查询。\nInnoDB主键索引的B+tree高度为多高呢?\n一般是三层，如果树的高度为3，则可以存储 2200w 左右的记录\n索引语法创建索引\nCREATE [ UNIQUE | FULLTEXT ] INDEX index_name ON table_name (index_col_name,... ) ;\n\n查看索引\nSHOW INDEX FROM table_name ;\n\n 删除索引\nDROP INDEX index_name ON table_name ;\n\n 小案例：\ncreate table tb_user(\nid int primary key auto_increment comment '主键',\nname varchar(50) not null comment '用户名',\nphone varchar(11) not null comment '手机号',\nemail varchar(100) comment '邮箱',\nprofession varchar(11) comment '专业',\nage tinyint unsigned comment '年龄',\ngender char(1) comment '性别 , 1: 男, 2: 女',\nstatus char(1) comment '状态',\ncreatetime datetime comment '创建时间'\n) comment '系统用户表';\n\n-- name字段为姓名字段，该字段的值可能会重复，为该字段创建索引\ncreate index idx_user_name on tb_user(name);\n-- phone手机号是非空，且唯一，为该字段创建唯一索引\ncreate unique index idx_user_phone on tb_user(phone);\n--  为profession、age、status创建联合索引。\ncreate index idx_user_pro_age_sta on tb_user(profession,age,status);\n-- 为email建立合适的索引来提升查询效率\ncreate index idx_email on tb_user(email);\n\nSQL性能分析SQL执行频率\nMySQL 客户端连接成功后，通过 show [session|global] status 命令可以提供服务器状态信息。通过如下指令，可以查看当前数据库的INSERT、UPDATE、DELETE、SELECT的访问频次\n-- session 是查看当前会话 ;\n-- global 是查询全局数据 ;\nSHOW GLOBAL STATUS LIKE 'Com_______';\n-- Com_delete: 删除次数\n-- Com_select: 查询次数\n-- Com_update: 更新次数\n-- Com_insert: 插入次数\n\n通过上述指令，可以查看到当前数据库到底是以查询为主，还是以增删改为主，从而为数据库优化提供参考依据。 如果是以增删改为主，我们可以考虑不对其进行索引的优化。 如果是以查询为主，那么就要考虑对数据库的索引进行优化了。\n慢查询日志\n慢查询日志记录了所有执行时间超过指定参数（long_query_time，单位：秒，默认10秒）的所有SQL语句的日志\nMySQL的慢查询日志默认没有开启，可以查看一下系统变量 slow_query_log\n如果要开启慢查询日志，需要在MySQL的配置文件（&#x2F;etc&#x2F;my.cnf）中配置如下信息：\n# 开启MySQL慢日志查询开关\nslow_query_log=1\n# 设置慢日志的时间为2秒，SQL语句执行时间超过2秒，就会视为慢查询，记录慢查询日志\nlong_query_time=2\n\n\n\nprofile详情\nshow profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪里去了。通过have_profiling参数，能够看到当前MySQL是否支持profile操作：\nSELECT @@have_profiling ;\n-- 可以通过set语句在session/global级别开启profiling\nset profiling = 1;\n\n开关已经打开了，接下来所执行的SQL语句，都会被MySQL记录，并记录执行时间消耗到哪儿去了。 我们直接执行如下的SQL语句：\nselect * from tb_user;\nselect * from tb_user where id = 1;\nselect * from tb_user where name = '白起';\nselect count(*) from tb_sku;\n\n\n-- 查看每一条SQL的耗时基本情况\nshow profiles;\n\n-- 查看指定query_id的SQL语句各个阶段的耗时情况\nshow profile for query query_id;\n\n-- 查看指定query_id的SQL语句CPU的使用情况\nshow profile cpu for query query_id;\n\nexplain\nEXPLAIN 或者 DESC命令获取 MySQL 如何执行 SELECT 语句的信息，包括在 SELECT 语句执行过程中表如何连接和连接的顺序。\n语法:\n-- 直接在select语句之前加上关键字 explain / desc\nEXPLAIN SELECT 字段列表 FROM 表名 WHERE 条件 ;\n\nExplain 执行计划中各个字段的含义:\n\n\n\n字段\n含义\n\n\n\nid\nselect查询的序列号，表示查询中执行select子句或者是操作表的顺序(id相同，执行顺序从上到下；id不同，值越大，越先执行)\n\n\nselect_type\n表示 SELECT 的类型，常见的取值有 SIMPLE（简单表，即不使用表连接或者子查询）、PRIMARY（主查询，即外层的查询）、UNION（UNION 中的第二个或者后面的查询语句）、SUBQUERY（SELECT&#x2F;WHERE之后包含了子查询）等\n\n\ntype\n表示连接类型，性能由好到差的连接类型为NULL、system、const、eq_ref、ref、range、 index、all 。\n\n\npossible_key\n显示可能应用在这张表上的索引，一个或多个。\n\n\nkey\n实际使用的索引，如果为NULL，则没有使用索引。\n\n\nkey_len\n表示索引中使用的字节数， 该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下， 长度越短越好 。\n\n\nrows\nMySQL认为必须要执行查询的行数，在innodb引擎的表中，是一个估计值，可能并不总是准确的。\n\n\nfiltered\n表示返回结果的行数占需读取行数的百分比， filtered 的值越大越好。\n\n\n索引使用tb_user 表所创建的索引\n\n最左前缀法则\n如果索引了多列（联合索引），要遵守最左前缀法则。最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。如果跳跃某一列，索引将会部分失效(后面的字段索引失效)\n在 tb_user 表中，有一个联合索引，这个联合索引涉及到三个字段，顺序分别为：profession，age，status。对于最左前缀法则指的是，查询时，最左变的列，也就是profession必须存在，否则索引全部失效。而且中间不能跳过某一列，否则该列后面的字段索引将失效。 \nexplain select * from tb_user where profession = '软件工程' and age = 31 and status= '0';\n\n\nexplain select * from tb_user where profession = '软件工程' and age = 31;\n\n\nexplain select * from tb_user where profession = '软件工程';\n\n\n只要联合索引最左边的字段 profession存在，索引就生效，只不过索引的长度不同。 而且由以上三组测试，我们也可以推测出profession字段索引长度为47、age字段索引长度为2、status字段索引长度为5。\nexplain select * from tb_user where age = 31 and status = '0';\n\n\nexplain select * from tb_user where status = '0';\n\n\n上面的这两组测试，索引并没有生效，原因是因为不满足最左前缀法则，联合索引最左边的列profession不存在\nexplain select * from tb_user where profession = '软件工程' and status = '0';\n\n\n存在profession字段，最左边的列是存在的，索引满足最左前缀法则的基本条件。但是查询时，跳过了age这个列，所以后面的列索引是不会使用的，也就是索引部分生效，所以索引的长度就是47。\n-- 打乱顺序测试\nexplain select * from tb_user where age = 31 and status = '0' and profession = '软件工程'；\n\n\n是完全满足最左前缀法则的，索引长度54，联合索引是生效的。\n最左前缀法则中指的最左边的列，是指在查询时，联合索引的最左边的字段(即是第一个字段)必须存在，与我们编写SQL时，条件编写的先后顺序无关。\n范围查询\n联合索引中，出现范围查询(&gt;,&lt;)，范围查询右侧的列索引失效。\nexplain select * from tb_user where profession = '软件工程' and age > 30 and status = '0';\n\n\n范围查询使用&gt; 或 &lt; 时，走联合索引了，但是索引的长度为49，说明范围查询右边的status字段是没有走索引。\nexplain select * from tb_user where profession = '软件工程' and age >= 30 and status = '0';\n\n\n当范围查询使用&gt;&#x3D; 或 &lt;&#x3D; 时，走联合索引了，但是索引的长度为54，就说明所有的字段都是走索引的。\n所以，尽可能的使用类似于 &gt;&#x3D; 或 &lt;&#x3D; 这类的范围查询，而避免使用 &gt; 或 &lt;\n索引失效情况\n在索引列上进行运算操作， 索引将失效。\n在tb_user表中，还有一个phone字段的单列索引。\n当根据phone字段进行等值匹配查询时, 索引生效。\nexplain select * from tb_user where phone = '17799990015';\n\n\n 当根据phone字段进行函数运算操作之后，索引失效。·\nexplain select * from tb_user where substring(phone,10,2) = '15';\n\n\n字符串不加引号\n字符串类型字段使用时，不加引号，索引将失效。\nexplain select * from tb_user where profession = '软件工程' and age = 31 and status = '0';\nexplain select * from tb_user where profession = '软件工程' and age = 31 and status = 0;\n\n\nexplain select * from tb_user where phone = '17799990015';\nexplain select * from tb_user where phone = 17799990015;\n\n\n如果字符串不加单引号，对于查询结果，没什么影响，但是数据库存在隐式类型转换，索引将失效\n模糊查询\n仅仅是尾部模糊匹配，索引不会失效。如果是头部模糊匹配，索引失效\n根据profession字段查询，符合最左前缀法则，联合索引是可以生效的，我们主要看一下，模糊查询时，%加在关键字之前，和加在关键字之后的影响\nexplain select * from tb_user where profession like '软件%';\nexplain select * from tb_user where profession like '%工程';\nexplain select * from tb_user where profession like '%工%';\n\n\n在like模糊查询中，在关键字后面加%，索引可以生效。而如果在关键字前面加了%，索引将会失效\n or连接条件\n用or分割开的条件， 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。\nexplain select * from tb_user where id = 10 or age = 23;\nexplain select * from tb_user where phone = '17799990017' or age = 23;\n\n\n由于age没有索引，所以即使id、phone有索引，索引也会失效。所以需要针对于age也要建立索引。\n然后，我们可以对age字段建立索引。\ncreate index idx_user_age on tb_user(age);\n\n我们再次执行上述的SQL语句，看看前后执行计划的变化。\nexplain select * from tb_user where id = 10 or age = 23;\nexplain select * from tb_user where phone = '17799990017' or age = 23;\n\n\n当or连接的条件，左右两侧字段都有索引时，索引才会生效。\n 数据分布影响\n如果MySQL评估使用索引比全表更慢，则不使用索引。\nselect * from tb_user where phone >= '17799990005';\nselect * from tb_user where phone >= '17799990015';\n\n\n\n相同的SQL语句，只是传入的字段值不同，最终的执行计划也完全不一样。\n就是因为MySQL在查询时，会评估使用索引的效率与走全表扫描的效率，如果走全表扫描更快，则放弃索引，走全表扫描。 因为索引是用来索引少量数据的，如果通过索引查询返回大批量的数据，则还不如走全表扫描来的快，此时索引就会失效。\nis null 与 is not null 操作\nexplain select * from tb_user where profession is null;\nexplain select * from tb_user where profession is not null;\n\n\n将profession字段值全部更新为null，再次执行上述的两条SQL，查看SQL语句的执行计划。\nupdate tb_user set profession = null;\nexplain select * from tb_user where profession is null;\nexplain select * from tb_user where profession is not null;\n\n\n一模一样的SQL语句，先后执行了两次，结果查询计划是不一样的，这是和数据库的数据分布有关系。查询时MySQL会评估，走索引快，还是全表扫描快，如果全表扫描更快，则放弃索引走全表扫描。 因此，is null 、is not null是否走索引，得具体情况具体分析，并不是固定的。\nSQL提示目前tb_user表索引情况如下\n\nidx_user_age, idx_email 这两个索引直接删除\ndrop index idx_user_age on tb_user;\ndrop index idx_email on tb_user;\n\n执行SQL，查询走了联合索引\nexplain select * from tb_user where profession = '软件工程';\n\n\n创建profession的单列索引\ncreate index idx_user_pro on tb_user(profession);\n\n再次执行SQL语句，查看执行计划，看看到底走哪个索引\nexplain select * from tb_user where profession &#x3D; &#39;软件工程&#39;;\n\n\n我们可以看到，possible_keys中 idx_user_pro_age_sta，idx_user_pro 这两个索引都可能用到，最终MySQL选择了idx_user_pro_age_sta索引。这是MySQL自动选择的结果。\n此时就可以借助于MySQL的SQL提示来完成来指定使用哪个索引\n use index ： 建议MySQL使用哪一个索引完成此次查询（仅仅是建议，mysql内部还会再次进行评估）。\nexplain select * from tb_user use index(idx_user_pro) where profession &#x3D; &#39;软件工程&#39;;\n\n\n ignore index ： 忽略指定的索引。\nexplain select * from tb_user ignore index(idx_user_pro) where profession &#x3D; &#39;软件工程&#39;;\n\n\n force index ： 强制使用索引。\nexplain select * from tb_user force index(idx_user_pro_age_sta) where profession &#x3D;\n&#39;软件工程&#39;;\n\n\n覆盖索引\n覆盖索引是指查询使用了索引，并且需要返回的列，在该索引中已经全部能够找到 。\n尽量使用覆盖索引，减少select *\n查看一组SQL的执行计划\nexplain select id, profession from tb_user where profession = '软件工程' and age =\n31 and status = '0' ;\n\nexplain select id,profession,age, status from tb_user where profession = '软件工程'\nand age = 31 and status = '0' ;\n\nexplain select id,profession,age, status, name from tb_user where profession = '软\n件工程' and age = 31 and status = '0' ;\n\nexplain select * from tb_user where profession = '软件工程' and age = 31 and status\n= '0';\n\n执行结果为:\n\n这四条SQL语句的执行计划前面所有的指标都是一样的，看不出来差异。但是此时，我们主要关注的是后面的Extra，前面两天SQL的结果为 Using where; Using Index ; 而后面两条SQL的结果为: Using index condition 。\n\n\n\nExtra\n含义\n\n\n\nUsing where; Using Index\n查找使用了索引，但是需要的数据都在索引列中能找到，所以不需要回表查询数据\n\n\nUsing index condition\n查找使用了索引，但是需要回表查询数据\n\n\n因为，在tb_user表中有一个联合索引 idx_user_pro_age_sta，该索引关联了三个字段profession、age、status，而这个索引也是一个二级索引，所以叶子节点下面挂的是这一行的主键id。 所以当我们查询返回的数据在 id、profession、age、status 之中，则直接走二级索引直接返回数据了。 如果超出这个范围，就需要拿到主键id，再去扫描聚集索引，再获取额外的数据了，这个过程就是回表。 而我们如果一直使用select * 查询返回所有字段值，很容易就会造成回表查询（除非是根据主键查询，此时只会扫描聚集索引）\n什么是覆盖索引，什么是回表查询\n 表结构及索引示意图\n\nid是主键，是一个聚集索引。 name字段建立了普通索引，是一个二级索引（辅助索引）。\n执行SQL :\nselect * from tb_user where id &#x3D; 2;\n\n\n根据id查询，直接走聚集索引查询，一次索引扫描，直接返回数据，性能高。\n执行SQL\nselet id,name from tb_user where name &#x3D; &#39;Arm&#39;\n\n\n虽然是根据name字段查询，查询二级索引，但是由于查询返回在字段为 id，name，在name的二级索引中，这两个值都是可以直接获取到的，因为覆盖索引，所以不需要回表查询，性能高。\n执行SQL\nselet id,name,gender from tb_user where name &#x3D; &#39;Arm&#39;;\n\n\n由于在name的二级索引中，不包含gender，所以，需要两次索引扫描，也就是需要回表查询，性能相对较差一点\n前缀索引\n当字段类型为字符串（varchar，text，longtext等）时，有时候需要索引很长的字符串，这会让索引变得很大，查询时，浪费大量的磁盘IO， 影响查询效率。此时可以只将字符串的一部分前缀，建立索引，这样可以大大节约索引空间，从而提高索引效率。\n语法\ncreate index idx_xxxx on table_name(column(n)) ;\n\n为tb_user表的email字段，建立长度为5的前缀索引。\ncreate index idx_email_5 on tb_user(email(5));\n\n\n前缀长度\n可以根据索引的选择性来决定，而选择性是指不重复的索引值（基数）和数据表的记录总数的比值，索引选择性越高则查询效率越高， 唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。\nselect count(distinct email) &#x2F; count(*) from tb_user ;\nselect count(distinct substring(email,1,5)) &#x2F; count(*) from tb_user ;\n\n前缀索引的查询流程\n\n单列索引与联合索引\n单列索引：即一个索引只包含单个列。\n联合索引：即一个索引包含了多个列。\ntb_user 表中的索引情况\n\n执行一条SQL语句，看看其执行计划：\nexplain select id,phone,name from tb_stu where phone &#x3D; &#39;17799990010&#39; and name &#x3D; &#39;韩信&#39;;\n\n在and连接的两个字段 phone、name上都是有单列索引的，但是最终mysql只会选择一个索引，也就是说，只能走一个字段的索引，此时是会回表查询的。\n再创建一个phone和name字段的联合索引来查询一下执行计划\ncreate unique index idx_user_phone_name on tb_user(phone,name);\n\nexplain select id,phone,name from tb_stu where phone &#x3D; &#39;17799990010&#39; and name &#x3D; &#39;韩信&#39;;\n\n\n如果存在多个查询条件，考虑针对于查询字段建立索引时，建议建立联合索引，而非单列索引。\n联合索引具体的结构示意图如下：\n\n索引设计原则\n针对于数据量较大，且查询比较频繁的表建立索引。\n\n针对于常作为查询条件（where）、排序（order by）、分组（group by）操作的字段建立索引。\n\n尽量选择区分度高的列作为索引，尽量建立唯一索引，区分度越高，使用索引的效率越高。\n\n如果是字符串类型的字段，字段的长度较长，可以针对于字段的特点，建立前缀索引。\n\n尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以覆盖索引，节省存储空间，避免回表，提高查询效率。\n\n要控制索引的数量，索引并不是多多益善，索引越多，维护索引结构的代价也就越大，会影响增删改的效率。\n\n如果索引列不能存储NULL值，请在创建表时使用NOT NULL约束它。当优化器知道每列是否包含NULL值时，它可以更好地确定哪个索引最有效地用于查询。\n\n\n","slug":"MySQL-存储引擎-索引","date":"2023-05-18T02:31:14.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"c1f6d722cc4602038235f2e7924e8ff2","title":"MySQL事务","content":"事务事务 是一组操作的集合，它是一个不可分割的工作单位，事务会把所有的操作作为一个整体一起向系\n统提交或撤销操作请求，即这些操作要么同时成功，要么同时失败。\n默认MySQL的事务是自动提交的，也就是说，当执行完一条DML语句时，MySQL会立即隐\n式的提交事务。\n事务操作未控制事务\n测试正常情况\n-- 1. 查询张三余额\nselect * from account where name = '张三';\n-- 2. 张三的余额减少1000\nupdate account set money = money - 1000 where name = '张三';\n-- 3. 李四的余额增加1000\nupdate account set money = money + 1000 where name = '李四';\n\n可以看到数据操作前后是一致的。\n测试异常情况\n-- 1. 查询张三余额\nselect * from account where name = '张三';\n-- 2. 张三的余额减少1000\nupdate account set money = money - 1000 where name = '张三';\n出错了....\n-- 3. 李四的余额增加1000\nupdate account set money = money + 1000 where name = '李四';\n\n(出错了….) 这句话不符合SQL语法,执行就会报错，检查最终的数据情况, 发现数据在操作前后不一致了。\n控制事务\n查看&#x2F;设置事务提交方式\nselect @@autocommit;\n-- autocommit为1标识开启事务自动提交   为0表示手动提交事务\nset @@autocommit = 0;\n\n提交事务\ncommit;\n\n回滚事务\nrollback;\n\n开启事务\nstart transaction;\n-- 或者\nbegin;\n\n转账案例：\n-- 开启事务\nstart transaction\n-- 1. 查询张三余额\nselect * from account where name = '张三';\n-- 2. 张三的余额减少1000\nupdate account set money = money - 1000 where name = '张三';\n-- 3. 李四的余额增加1000\nupdate account set money = money + 1000 where name = '李四';\n-- 如果正常执行完毕, 则提交事务\ncommit;\n-- 如果执行过程中报错, 则回滚事务\n-- rollback;\n\n事务四大特性原子性（Atomicity）：事务是不可分割的最小操作单元，要么全部成功，要么全部失败。\n一致性（Consistency）：事务完成时，必须使所有的数据都保持一致状态。\n隔离性（Isolation）：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立环境下运行。\n持久性（Durability）：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的。\n上述就是事务的四大特性，简称ACID。\n并发事务问题\n 赃读：一个事务读到另外一个事务还没有提交的数据。\n\n比如B读取到了A未提交的数据。\n不可重复读：一个事务先后读取同一条记录，但两次读取的数据不同，称之为不可重复读\n\n 事务A两次读取同一条记录，但是读取到的数据却是不一样的。\n 幻读：一个事务按照条件查询数据时，没有对应的数据行，但是在插入数据时，又发现这行数据已经存在，好像出现了 “幻影”。\n\n事务隔离级别\n\n\n\n隔离级别\n脏读\n不可重复读\n幻读\n\n\n\nRead uncommitted\n存在\n存在\n存在\n\n\nRead committed\n不存在\n存在\n存在\n\n\nRepeatable Read(default)\n不存在\n不存在\n存在\n\n\nSerializable\n不存在\n不存在\n不存在\n\n\n 查看事务隔离级别\nselect @@TRANSACTION_IOSLATION;\n\n设置事务隔离级别\nSET [SESSION | GLOBAL] TRANSACTION IOSLATION LEVEL\n[READ COMMITTED | READ UNCOMMITTED | REPEATABLE READ | SERIALIZABLE]\n\n事务隔离级别越高，数据越安全，但是性能越低。\n","slug":"MySQL-事务","date":"2023-05-17T13:51:38.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"f293b2fcfae54b8509d50db2914ed7a0","title":"MySQL_约束_多表查询","content":"约束概念：约束是作用于表中字段上的规则，用于限制存储在表中的数据。\n目的：保证数据库中数据的正确、有效性和完整性。\n\n\n\n约束\n描述\n关键词\n\n\n\n非空约束\n限制该字段的数据不能为null\nNOT NULL\n\n\n唯一约束\n保证该字段的所有数据都是唯一、不重复的\nUNIQUE\n\n\n主键约束\n主键是一行数据的唯一标识，要求非空且唯一\nPRIMARY KEY\n\n\n默认约束\n保存数据时，如果未指定该字段的值，则采用默认值\nDEFAULT\n\n\n检查约束(8.0.16版本之后)\n保证字段值满足某一个条件\nCHECK\n\n\n外键约束\n用来让两张表的数据之间建立连接，保证数据的一致性和完整性\nFOREIGN KEY\n\n\n约束是作用于表中字段上的，可以在创建表&#x2F;修改表的时候添加约束。\ncreate table tb_user(\nid int primary key auto_increment comment 'ID唯一标识',\nname varchar(10) not null unique comment '姓名',\nage int check (age between 0 and 120) comment '年龄',\nstatus char(1) default 1 comment '状态',\ngender char(1) comment '性别'\n)comment '学生表';\n\n外键约束\n外键：用来让两张表的数据之间建立连接，从而保证数据的一致性和完整性。\ncreate table dept(\n\tid int auto_increment primary key comment 'ID',\n    name varchar(50) not null comment '部门名称',    \n)comment '部门表';\n\nCREATE TABLE `emp` (\n  `id` int DEFAULT NULL COMMENT '编号',\n  `workno` varchar(10) DEFAULT NULL,\n  `name` varchar(10) DEFAULT NULL COMMENT '姓名',\n  `gender` char(1) DEFAULT NULL COMMENT '性别',\n  `age` tinyint unsigned DEFAULT NULL COMMENT '年龄',\n  `idcard` char(18) DEFAULT NULL COMMENT '身份证号',\n  `workaddress` varchar(50) DEFAULT NULL COMMENT '工作地址',\n  `entrydate` date DEFAULT NULL COMMENT '入职时间',\n  `dept_id` int DEFAULT '1' COMMENT '部门id'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='员工表'\n\n为emp表的dept_id字段添加外键约束,关联dept表的主键id。\nalter table emp add constraint fk_emp_dept foreign key(dept_id) references dept(id);\n\n命令中的 “alter table emp” 表示将要修改的表是 emp，“add constraint” 表示添加一个约束，“fk_emp_dept_id” 是这个约束的名称，“foreign key (dept_id)” 表示这是一个外键约束，关联的列是 emp 表中的 dept_id 列，“references dept(id)” 表示外键引用了 dept 表的 id 列。这个约束的作用是确保 emp 表中的 dept_id 值必须在 dept 表的 id 列中存在，从而保证了参照完整性。\n添加了外键约束之后，我们再到dept表(父表)删除id为1的记录，然后看一下会发生什么现象。 此时将会报错，不能删除或更新父表记录，因为存在外键约束。\n1451 - Cannot delete or update a parent row: a foreign key constraint fails (hj.emp, CONSTRAINT fk_emp_dept FOREIGN KEY (dept_id) REFERENCES dept (id))\n 删除外键\nALTER TABLE 表名 DROP FOREIGN KEY 外键名称;\n\nalter table emp drop foreign key fk_emp_dept;\n\n删除&#x2F;更新行为\n添加了外键之后，再删除父表数据时产生的约束行为，我们就称为删除&#x2F;更新行为。\n具体的删除&#x2F;更新行为有以下几种:\n\n\n\n行为\n说明\n\n\n\nNO ACTION\n当在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键，如果有则不允许删除&#x2F;更新。 (与 RESTRICT 一致) 默认行为\n\n\nRESTRICT\n当在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键，如果有则不允许删除&#x2F;更新。 (与 NO ACTION 一致) 默认行为\n\n\nCASCADE\n当在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键，如果有，则也删除&#x2F;更新外键在子表中的记录。\n\n\nSET NULL\n当在父表中删除对应记录时，首先检查该记录是否有对应外键，如果有则设置子表中该外键值为null（这就要求该外键允许取null）。\n\n\nSET DEFAULT\n父表有变更时，子表将外键列设置成一个默认的值 (Innodb不支持)\n\n\n具体语法为:\nALTER TABLE 表名 ADD CONSTRAINT 外键名称 FOREIGN KEY (外键字段) REFERENCES\n主表名 (主表字段名) ON UPDATE 行为 ON DELETE 行为;\n\n小案例：CASCADE行为下  修改父表id为1的记录，将id修改为6\nalter table emp add constraint kf_emp_dept_id foreign key (dept_id) references dept(id)\non update cascade on delete cascade;\n\nupdate dept set id = 6 where id = 1;\n\ndelete form dept where id = 6;\n\n我们发现:\n原来在子表中dept_id值为1的记录，现在也变为6了，这就是cascade级联的效果。\n父表的数据删除成功了，但是子表中关联的记录也被级联删除了。\n SET NULL\nalter table emp add constraint fk_emp_dept_id foreign key (dept_id) references dept(id)\non update set null on delete set null;\n\ndelete from dept where id = 3;\n\nupdate dept set id = 7 where id = 5;\n\n父表的记录是可以正常的删除的，父表的数据删除&#x2F;跟新之后，再打开子表 emp，我们发现子表emp的dept_id字段，原来dept_id为1的数据，现在都被置为NULL了。\n多表查询多表关系一对多(多对一) \n如：部门 与 员工的关系，一个部门对应多个员工，一个员工对应一个部门\n实现: 在多的一方建立外键，指向一的一方的主键\ncreate table emp(\nid int auto_increment comment 'ID' primary key,\nname varchar(50) not null comment '姓名',\nage int comment '年龄',\njob varchar(20) comment '职位',\nsalary int comment '薪资',\nentrydate date comment '入职时间',\nmanagerid int comment '直属领导ID',\ndept_id int comment '部门ID',\nconstraint fk_emp_dept_id foreign key (dept_id) references dept(id)\non update cascade on delete cascade\n)comment '员工表';\n\ncreate table dept(\nid int auto_increment comment 'ID' primary key,\nname varchar(50) not null comment '部门名称'\n)comment '部门表';\n\n\n多对多\n如： 学生 与 课程的关系， 一个学生可以选修多门课程，一门课程也可以供多个学生选择\n实现: 建立第三张中间表，中间表至少包含两个外键，分别关联两方主键\nreate table course(\nid int auto_increment primary key comment '主键ID',\nname varchar(10) comment '课程名称'\n) comment '课程表';\n\n\ncreate table tb_user(\nid int auto_increment primary key comment '主键ID',\nname varchar(10) comment '姓名',\nage int comment '年龄',\ngender char(1) comment '1: 男 , 2: 女',\nphone char(11) comment '手机号'\n) comment '用户基本信息表';\n\n\ncreate table tb_user_edu(\nid int auto_increment primary key comment '主键ID',\ndegree varchar(20) comment '学历',\nmajor varchar(50) comment '专业',\nprimaryschool varchar(50) comment '小学',\nmiddleschool varchar(50) comment '中学',\nuniversity varchar(50) comment '大学',\nuserid int unique comment '用户ID',\nconstraint fk_userid foreign key (userid) references tb_user(id)\n) comment '用户教育信息表';\n\n\n一对一\n如：用户 与 用户详情的关系， 一对一关系，多用于单表拆分，将一张表的基础字段放在一张表中，其他详情字段放在另一张表中，以提升操作效率\n实现: 在任意一方加入外键，关联另外一方的主键，并且设置外键为唯一的(UNIQUE)\ncreate table tb_user(\nid int auto_increment primary key comment '主键ID',\nname varchar(10) comment '姓名',\nage int comment '年龄',\ngender char(1) comment '1: 男 , 2: 女',\nphone char(11) comment '手机号'\n) comment '用户基本信息表';\n\ncreate table tb_user_edu(\nid int auto_increment primary key comment '主键ID',\ndegree varchar(20) comment '学历',\nmajor varchar(50) comment '专业',\nprimaryschool varchar(50) comment '小学',\nmiddleschool varchar(50) comment '中学',\nuniversity varchar(50) comment '大学',\nuserid int unique comment '用户ID',\nconstraint fk_userid foreign key (userid) references tb_user(id)\n) comment '用户教育信息表';\n\n多表查询概述要执行多表查询，就只需要使用逗号分隔多张表即可，如： select * from emp , dept;\n 具体的执行结果如下:\n我们看到查询结果中包含了大量的结果集，总共102条记录，而这其实就是员工表emp所有的记录(17) 与 部门表dept所有记录(6) 的所有组合情况，这种现象称之为笛卡尔积。接下来，就来简单介绍下笛卡尔积。\n笛卡尔积: 笛卡尔乘积是指在数学中，两个集合A集合 和 B集合的所有组合情况。\n\n而在多表查询中，我们是需要消除无效的笛卡尔积的，只保留两张表关联部分的数据。\n\nselect * from emp , dept where emp.dept_id = dept.id;\n\n连接查询分类\n​\t内连接：相当于查询A、B交集部分数据 \n​\t外连接：\n​\t\t左外连接：查询左表所有数据，以及两张表交集部分数据\n​\t\t右外连接：查询右表所有数据，以及两张表交集部分数据\n​\t自连接：当前表与自身的连接查询，自连接必须使用表别名\n内连接\n内连接的语法分为两种: 隐式内连接、显式内连接。\n隐式内连接\nSELECT 字段列表 FROM 表1 , 表2 WHERE 条件 ... ;\n\n显式内连接\nSELECT 字段列表 FROM 表1 [ INNER ] JOIN 表2 ON 连接条件 ... ;\n\n小案例：查询每一个员工的姓名 , 及关联的部门的名称 (隐式内连接实现)\n -- 隐式内连接实现\nselect emp.name, dept.name from emp,dept where emp.dept_id = dept.id;\n\nselect emp.name, dept.name from emp join dept on emp.dept_id = dept.id;\n\n外连接\n外连接分为两种，分别是：左外连接 和 右外连接\n具体的语法结构为：\n左外连接 \n相当于查询表1(左表)的所有数据，当然也包含表1和表2交集部分的数据。\nSELECT 字段列表 FROM 表1 LEFT [ OUTER ] JOIN 表2 ON 条件 ... ;\n\n 小案例：查询emp表的所有数据, 和对应的部门信息\n分析：要查询emp的所有数据，所以是不能内连接查询的，需要考虑使用外连接查询。\nselect e.*, d.name from emp e left outer join dept d on e.dept_id = d.id;\n\nselect e.*, d.name from emp e left join dept d on e.dept_id = d.id;\n\n小案例： 查询dept表的所有数据, 和对应的员工信息(右外连接)\nselect dept.*, emp.* from emp right outer join dept on emp.dept_id = dept.id;\n\n左外连接和右外连接是可以相互替换的，只需要调整在连接查询时SQL中，表结构的先后顺序就可以了。而我们在日常开发使用时，更偏向于左外连接。\n自连接\n顾名思义，就是自己连接自己，也就是把一张表连接查询多次。\n查询语法\nSELECT 字段列表 FROM 表A 别名A JOIN 表A 别名B ON 条件 ... ;\n\n小案例：查询员工 及其 所属领导的名字，如果员工没有领导, 也需要查询出来\nselect a.name, b.name from emp a left join emp b on a.managerid = b.id;\n\n在自连接查询中，必须要为表起别名，要不然我们不清楚所指定的条件、返回的字段，到底是哪一张表的字段。\n联合查询\n对于union查询，就是把多次查询的结果合并起来，形成一个新的查询结果集。\n语法：\nSELECT 字段列表 FROM 表A ...\nUNION [ ALL ]\nSELECT 字段列表 FROM 表B ....;\n\n对于联合查询的多张表的列数必须保持一致，字段类型也需要保持一致。如果多条查询语句查询出来的结果，字段数量不一致，在进行union&#x2F;union all联合查询时，将会报错\nunion all 会将全部的数据直接合并在一起，union 会对合并之后的数据去重。\n子查询\nSQL语句中嵌套SELECT语句，称为嵌套查询，又称子查询\n语法\nSELECT * FROM t1 WHERE column1 = ( SELECT column1 FROM t2 );\n\n子查询外部的语句可以是INSERT &#x2F; UPDATE &#x2F; DELETE &#x2F; SELECT 的任何一个。\n根据子查询位置，分为：WHERE之后\t\t FROM之后\t\tSELECT之后\n标量子查询 ：子查询返回的结果是单个值\t\t常用的操作符：&#x3D;   &lt;&gt;   &gt;   &gt;&#x3D;   &lt;  &lt;&#x3D; \n案例：查询 “销售部” 的所有员工信息\nselect * from emp where dept_id = (select id from dept where name = '销售部');\n\n列子查询\n子查询返回的结果是一列（可以是多行），这种子查询称为列子查询。\n常用的操作符：IN 、NOT IN 、 ANY 、SOME 、 ALL\n\n\n\n操作符\n描述\n\n\n\nIN\n在指定的集合范围之内，多选一\n\n\nNOT IN\n不在指定的集合范围之内\n\n\nANY\n子查询返回列表中，有任意一个满足即可\n\n\nSOME\n与ANY等同，使用SOME的地方都可以使用ANY\n\n\nALL\n子查询返回列表的所有值都必须满足\n\n\n小案例：查询 “销售部” 和 “市场部” 的所有员工信息\nselect * from emp where id in\n(select id from dept where name = '销售部' or name = '市场部');\n\n 小案例：查询比 财务部 所有人工资都高的员工信息\nselect * from emp where salary > all ( select salary from emp where dept_id =\n(select id from dept where name = '财务部') );\n\n 小案例：查询比研发部其中任意一人工资高的员工信息\nselect * from emp where salary > any(select salary from emp where dept_id = \n(select id from dept where name = '研发部'));\n\n行子查询\n子查询返回的结果是一行（可以是多列），这种子查询称为行子查询。\n常用的操作符：&#x3D; 、&lt;&gt; 、IN 、NOT IN\n小案例：查询与 “张无忌” 的薪资及直属领导相同的员工信息 ;\nselect * from emp where (salary,managerid) = (select salary, managerid from emp where name ='张无忌');\n\n表子查询\n子查询返回的结果是多行多列，这种子查询称为表子查询。常用的操作符：IN\n小案例：查询与 “鹿杖客” , “宋远桥” 的职位和薪资相同的员工信息\nselect * from emp where (job,salary) in ( select job, salary from emp where name =\n'鹿杖客' or name = '宋远桥' );\n\n小案例：查询入职日期是 “2006-01-01” 之后的员工信息 , 及其部门信息\n````\n\n 小案例：查询员工的姓名、年龄、职位、部门信息 （隐式内连接）\n\n\n小案例： 查询年龄小于30岁的员工的姓名、年龄、职位、部门信息（显式内连接）\n\n\n小案例：查询拥有员工的部门ID、部门名称\n\n\n小案例：查询所有年龄大于40岁的员工, 及其归属的部门名称; 如果员工没有分配部门, 也需要展示出来(外连接)\n\n\n小案例：查询所有员工的工资等级\n\n\n小案例：查询 &quot;研发部&quot; 所有员工的信息及 工资等级\n\n\n小案例： 查询 &quot;研发部&quot; 员工的平均工资\n\n\n小案例： 查询工资比 &quot;灭绝&quot; 高的员工信息。\n\n\n小案例： 查询比平均薪资高的员工信息\n\n\n小案例：查询低于本部门平均工资的员工信息\n\n\n小案例：查询所有的部门信息, 并统计部门的员工人数\n\n\n小案例：查询所有学生的选课情况, 展示出学生名称, 学号, 课程名称\n\n\n\n","slug":"MySQL-约束-多表联查","date":"2023-05-16T13:40:42.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"8f9e88b96addebc57d8b075bdae2d985","title":"MySQL_DCL_函数","content":"DCLData Control Language(数据控制语言)，用来管理数据库用户、控制数据库的访问权限。\n管理用户查询用户\nselect * from mysql.user;\n\n\n其中 Host代表当前用户访问的主机, 如果为localhost, 仅代表只能够在当前本机访问，是不可以远程访问的。User代表的是访问该数据库的用户名。在MySQL中需要通过Host和User来唯一标识一个用户。\n创建用户\nCREATE USER '用户名'@'主机名' IDENTIFIED BY '密码';\n\n修改用户密码\nALTER USER '用户名'@'主机号' IDENTIFIED WITH mysql_native_password BY '新密码';\n\n删除用户\nDROP USER '用户名'@'主机名';\n\n在MySQL中需要通过用户名@主机名的方式，来唯一标识一个用户。\n主机名可以使用 % 通配。\n这类SQL开发人员操作的比较少，主要是DBA（ Database Administrator 数据库管理员）使用。\n权限控制\nMySQL中定义了很多种权限，但是常用的就以下几种\n\n\n\n权限\n说明\n\n\n\nALL,  ALL PRIVILEGES\n所有权限\n\n\nSELECT\n查询数据\n\n\nINSERT\n插入数据\n\n\nUPDATE\n修改数据\n\n\nDELETE\n删除数据\n\n\nALTER\n修改表\n\n\nDROP\n删除数据库&#x2F;表&#x2F;视图\n\n\nCREATE\n创建数据库&#x2F;表\n\n\n查询权限\nSHOW GRANTS FOR '用户名'@'主机名';\n\n授予权限\nGRANT 权限列表 ON 数据库名.表名 TO '用户名'@'主机名';\n\n撤销权限\nREVOKE 权限列表 ON 数据库名.表名 FROM '用户名'@'主机名';\n\n 多个权限之间，使用逗号分隔\n授权时， 数据库名和表名可以使用 * 进行通配，代表所有。\nSHOW GRANTS FOR 'hj'@'%';\n-- 运行结果：\n-- GRANT ALL PRIVILEGES ON `hj`.* TO `hj`@`%`\n-- 该命令为 MySQL 的授权语法，意思是授权用户 hj 在任何主机地址上（%）对数据库 hj 中所有表拥有所有权限（ALL PRIVILEGES）。可以执行如下 SQL 命令进行授权\n-- GRANT USAGE ON *.* TO `hj`@`%`\n-- 该命令为 MySQL 的授权语法，意思是授权用户 `hj` 在任何主机地址上（`%`）对所有的数据库、表、函数、存储过程等对象都拥有使用权限（`USAGE`）。\n\n函数MySQL中的函数主要分为以下四类： 字符串函数、数值函数、日期函数、流程函数。\n字符串函数\n\n\n\n函数\n功能\n\n\n\nCONCAT(S1,S2,…Sn)\n字符串拼接，将S1，S2，… Sn拼接成一个字符串\n\n\nLOWER(str)\n将字符串str全部转为小写\n\n\nUPPER(str)\n将字符串str全部转为大写\n\n\nLPAD(str,n,pad)\n左填充，用字符串pad对str的左边进行填充，达到n个字符串长度\n\n\nRPAD(str,n,pad)\n右填充，用字符串pad对str的右边进行填充，达到n个字符串长度\n\n\nTRIM(str)\n去掉字符串头部和尾部的空格\n\n\nSUBSTRING(str,start,len)\n返回从字符串str从start位置起的len个长度的字符串\n\n\n concat : 字符串拼接\nselect concat('Hello' , ' MySQL');\n\nlpad : 左填充\nselect lpad('01', 5, '-');\n\n rpad : 右填充\nselect rpad('01',5,'23456789');\n-- 01234\nselect rpad('01',5,'23');\n-- 01232\n\n trim : 去除空格\nselect trim(' Hello MySQL ');\n-- 'Hello MySQL'\n\n substring : 截取子字符串\nselect substring('Hello MySQL',1,5);\n-- Hello\n\n案例：由于业务需求变更，企业员工的工号，统一为5位数，目前不足5位数的全部在前面补0。比如： 1号员工的工号应该为00001。\nupdate workno = lpad(workno, 5, '0');\n\n数值函数\n\n\n\n函数\n功能\n\n\n\nCEIL(x)\n向上取整\n\n\nFLOOR(x)\n向下取整\n\n\nMOD(x,y)\n返回x&#x2F;y的模\n\n\nRAND()\n返回0~1内的随机数\n\n\nROUND(x,y)\n求参数x的四舍五入的值，保留y位小数\n\n\n ceil：向上取整\nselect ceil(1.1);\n\n floor：向下取整\nselect floor(1.9);\n\nmod：取模\nselect mod(7,4);\n\n rand：获取随机数\nselect rand();\n\nround：四舍五入\nselect round(2.344,2);\n\n小案例：通过数据库的函数，生成一个六位数的随机验证码。\nselect lpad('',6,substring(concat(rand(),''),3,9)) code;\n\n日期函数\n\n\n\n函数\n功能\n\n\n\nCURDATE()\n返回当前日期\n\n\nCURTIME()\n返回当前时间\n\n\nNOW()\n返回当前日期和时间\n\n\nYEAR(date)\n获取指定date的年份\n\n\nMONTH(date)\n获取指定date的月份\n\n\nDAY(date)\n获取指定date的日期\n\n\nDATE_ADD(date,INTERVAL expr type)\n返回一个日期&#x2F;时间值加上一个时间间隔expr后的时间值\n\n\nDATEDIFF(date1,date2)\n返回起始时间date1 和 结束时间date2之间的天数\n\n\n curdate：当前日期\nselect curdate();\n\n curtime：当前时间\nselect curtime();\n\n now：当前日期和时间\nselect now();\n\nYEAR , MONTH , DAY：当前年、月、日\nselect year(now());\nselect month(now());\nselect day(now());\n\ndate_add：增加指定的时间间隔\nselect date_add(now(),INTERVAL 70 YEAR);\nselect date_add('2023-05-16', INTERVAL 29 DAY);\n\n datediff：获取两个日期相差的天数\nselect datediff('2023-05-16','2023-06-14');\n\n小案例：查询所有员工的入职天数，并根据入职天数倒序排序。\nselect name , datediff(curdate(), entrydate) workday from emp order by workday desc;\n\n流程函数\n流程函数也是很常用的一类函数，可以在SQL语句中实现条件筛选，从而提高语句的效率。\n\n\n\n函数\n功能\n\n\n\nIF(value, t, f)\n如果value为true，则返回t，否则返回f\n\n\nIFNULL(value1, value2)\n如果value1不为空，返回value1，否则返回value2\n\n\nCASE WHEN [vall] THEN [res1] … ELSE [default] END\n如果val1为true，返回res1，… 否则返回default默认值\n\n\nCASE [expr] WHEN [val1] THEN [res1] …  ELSE [default] END\n如果expr的值等于val1，返回res1，… 否则返回default默认值\n\n\n if\nselect if(false, 'Ok', 'Error');\n-- 默认的false值还有 0, '', null\n\n ifnull\nselect ifnull('Ok','Default');\nselect ifnull('','Default');\nselect ifnull(null,'Default')\n-- 只有null会被识别为空\n\n case when then else end\n查询emp表的员工姓名和工作地址 (北京&#x2F;上海   替换成   一线城市 , 其他   替换成   二线城市)\nselect name, (case workaddress when '北京' then '一线城市' when '上海' then '一线城市' else '二线城市' end) '工作地点' from emp;\n\n案例：将学生的各个学科的成绩分等级展示，大于等于85 为优秀，(85，60]为及格，否则不及格\ncreate table score(\nid int comment 'ID',\nname varchar(20) comment '姓名',\nmath  tinyint unsigned comment '数学',\nenglish tinyint unsigned comment '英语',\nchinese tinyint unsigned comment '语文'\n) comment '学生成绩表';\ninsert into score(id, name, math, english, chinese) VALUES \n(1, 'Tom', 67, 88, 95),\n (2, 'Rose' , 23, 66, 90),\n (3, 'Jack', 56, 98, 76);\n \n select name , \n(case when math >= 85 then '优秀' when math >=60 then '及格' else '不及格' end ) math_score,\n(case when english >= 85 then '优秀' when english >=60 then '及格' else '不及格' end ) english_score,\n(case when chinese >= 85 then '优秀' when chinese >=60 then '及格' else '不及格' end ) chinese_score \n from score;\n\n","slug":"MySQL-DCL-函数","date":"2023-05-16T08:56:15.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"b886b366f3cb34006709f66f49ec6b62","title":"状态模式","content":"状态模式它将对象的行为与其内部状态分离开来，使得对象可以根据其内部状态的变化而改变其行为。状态模式通过将状态的切换封装在状态类中，使得状态转换具有可扩展性和灵活性，并避免了由于状态转换所带来的“if-else”嵌套过多的问题。\n状态模式的基本机制是将状态抽象为一个接口或者抽象类，每个具体状态类都实现该接口或者抽象类，并且在具体状态类中实现其对应的行为。在状态模式中，对象的状态切换是通过改变对象持有的状态实例来实现的。当对象需要状态转换时，它会委托当前状态实例处理状态转换，从而使得对象的行为和状态相互解耦，具有更好的可扩展性和灵活性。\n状态模式符合“单一职责原则”和“开闭原则”两个软件设计原则。其中，“单一职责原则”指一个类只负责一个职责，状态模式将状态抽象为一个独立的类，使得每个状态类只负责处理一个状态，从而使得每个类都具有单一的职责；“开闭原则”指对扩展开放，对修改关闭，状态模式通过增加新的状态类来扩展状态转换过程，而不需要修改原有的代码，从而符合“开闭原则”。\n状态模式存在三个角色：\n\n环境（Context）角色：它定义了客户端所感兴趣的接口，通常包含一个状态实例并维护一个指向具体状态对象的引用。这个上下文类可以处理请求并将其委托给当前状态对象。\n抽象状态（State）角色：它把所有具体状态类的共同点抽象出来，定义了这些共同点对应的接口或抽象类，并在其中定义了该状态下对象的行为。\n具体状态（Concrete State）角色：它实现了抽象状态的接口或抽象类，并且定义了在该状态下对象的行为。\n\n假设我们要设计一个电灯类，电灯可以处于3个不同的状态：\n关闭状态、开启状态和闪烁状态。电灯类的代码如下：\npublic class Light &#123;\n    private State state;\n\n    public Light() &#123;\n        this.state = new OffState();\n    &#125;\n\n    public void setState(State state) &#123;\n        this.state = state;\n    &#125;\n\n    public void turnOn() &#123;\n        state.turnOn(this);\n    &#125;\n\n    public void turnOff() &#123;\n        state.turnOff(this);\n    &#125;\n\n    public void blink() &#123;\n        state.blink(this);\n    &#125;\n&#125;\n\n在这里，我们将电灯的状态抽象成了接口State，然后定义了3个具体的状态类：OffState、OnState和BlinkState，这3个类实现了State接口。\npublic interface State &#123;\n    void turnOn(Light light);\n    void turnOff(Light light);\n    void blink(Light light);\n&#125;\n\npublic class OffState implements State &#123;\n    @Override\n    public void turnOn(Light light) &#123;\n        System.out.println(\"电灯开启\");\n        light.setState(new OnState());\n    &#125;\n\n    @Override\n    public void turnOff(Light light) &#123;\n        System.out.println(\"电灯已经关闭，不能再关闭\");\n    &#125;\n\n    @Override\n    public void blink(Light light) &#123;\n        System.out.println(\"电灯未开启，不能闪烁\");\n    &#125;\n&#125;\n\npublic class OnState implements State &#123;\n    @Override\n    public void turnOn(Light light) &#123;\n        System.out.println(\"电灯已经开启，不需要再开启\");\n    &#125;\n\n    @Override\n    public void turnOff(Light light) &#123;\n        System.out.println(\"电灯关闭\");\n        light.setState(new OffState());\n    &#125;\n\n    @Override\n    public void blink(Light light) &#123;\n        System.out.println(\"电灯开始闪烁\");\n        light.setState(new BlinkState());\n    &#125;\n&#125;\n\npublic class BlinkState implements State &#123;\n    @Override\n    public void turnOn(Light light) &#123;\n        System.out.println(\"电灯已经开启，不需要再开启\");\n    &#125;\n\n    @Override\n    public void turnOff(Light light) &#123;\n        System.out.println(\"电灯关闭\");\n        light.setState(new OffState());\n    &#125;\n\n    @Override\n    public void blink(Light light) &#123;\n        System.out.println(\"电灯停止闪烁\");\n        light.setState(new OnState());\n    &#125;\n&#125;\n\n在电灯类中，引入了一个State类型的变量state，表示当前电灯处于哪种状态。当电灯需要改变状态时，它会调用state的对应方法，然后将state设为新的状态。例如，当电灯需要开启时，它将调用OnState的turnOn方法，并将state设为OnState实例。\n下面是一个应用电灯类的例子：\npublic static void main(String[] args) &#123;\n    Light light = new Light();\n    light.turnOn();  // 电灯开启\n    light.turnOn();  // 电灯已经开启，不需要再开启\n    light.blink();   // 电灯开始闪烁\n    light.turnOff(); // 电灯关闭\n    light.blink();   // 电灯未开启，不能闪烁\n    light.turnOff(); // 电灯已经关闭，不能再关闭\n&#125;\n\n在这个例子中，我们首先创建了一个电灯实例light，然后通过调用它的turnOn、blink和turnOff方法，改变了它的状态，并输出了相应的信息。\n这个例子可以帮助我们理解状态模式的基本用法和原理，同时也能让我们感受到状态模式所提供的可扩展性和灵活性。当需要新增或修改状态时，我们只需要添加或修改相应的状态类，而不需要修改电灯类的代码，从而大大降低了代码的耦合度和维护成本。\n","slug":"状态模式","date":"2023-05-15T10:11:38.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"ab4a664e2e3638cd67bb057e6ee644aa","title":"迭代器模式","content":"迭代器模式它提供一种方便的方式来访问并遍历集合对象的元素，而无需暴露集合的内部表示。该模式将遍历集合的过程与集合的实现分离开来，使得可以在不影响客户端代码的情况下更改集合的内部实现。\n迭代器模式包括以下几个元素：\n1.迭代器接口（Iterator）：定义了用于访问和遍历集合元素的标准方法。\n2.具体迭代器（Concrete Iterator）：实现了Iterator接口，用于遍历集合中的元素。\n3.集合接口（Aggregate）：定义了一组用于集合管理和访问元素的方法。\n4.具体集合（Concrete Aggregate）：实现了Aggregate接口，用于管理和访问元素集合。\n通过使用迭代器模式，我们可以将遍历和集合的实现分离开来，从而使得可以方便地对集合进行修改而不会影响遍历的过程。另外，通过提供不同的迭代器实现，我们可以很容易地实现一些高级的遍历需求，比如倒序遍历、过滤遍历等。\n迭代器模式符合以下软件设计原则：\n1.单一职责原则：该模式将集合的遍历与集合本身的实现分离开来，保证每个对象只负责自己的单一职责。\n2.开闭原则：新增一种集合类型或修改现有集合类型的实现方式都不会影响到已有的迭代器实现，因此该模式对修改关闭，对扩展开放。\n3.依赖倒置原则：在迭代器模式中，客户端只依赖于迭代器接口，而不依赖于集合的内部表示，从而将高层次模块从底层模块中解耦出来。\n4.迪米特原则：该模式通过将遍历集合的访问方法委托给迭代器对象来遵循迪米特原则，即一个对象应该对其他对象有最少的了解。\n以下是一个简单的Java案例，演示了迭代器的使用方式：\n//迭代器接口\ninterface Iterator&lt;T> &#123;\n    boolean hasNext();\n    T next();\n&#125;\n\n//集合接口\ninterface Aggregate&lt;T> &#123;\n    void add(T element);\n    void remove(T element);\n    Iterator&lt;T> iterator();\n&#125;\n\n//具体集合\nclass MyList&lt;T> implements Aggregate&lt;T> &#123;\n    private T[] elements;\n    private int size;\n    private int capacity;\n\n    public MyList(int capacity) &#123;\n        this.elements = (T[]) new Object[capacity];\n        this.size = 0;\n        this.capacity = capacity;\n    &#125;\n\n    public void add(T element) &#123;\n        if(size &lt; capacity) &#123;\n            elements[size] = element;\n            size++;\n        &#125;\n    &#125;\n\n    public void remove(T element) &#123;\n        for(int i = 0; i &lt; size; i++) &#123;\n            if(elements[i].equals(element)) &#123;\n                for(int j = i; j &lt; size - 1; j++) &#123;\n                    elements[j] = elements[j+1];\n                &#125;\n                size--;\n                break;\n            &#125;\n        &#125;\n    &#125;\n\n    public Iterator&lt;T> iterator() &#123;\n        return new MyListIterator();\n    &#125;\n\n    //具体迭代器\n    private class MyListIterator implements Iterator&lt;T> &#123;\n        private int index = 0;\n\n        public boolean hasNext() &#123;\n            return index &lt; size;\n        &#125;\n\n        public T next() &#123;\n            T element = elements[index];\n            index++;\n            return element;\n        &#125;\n    &#125;\n&#125;\n\n//客户端代码\npublic class IteratorDemo &#123;\n    public static void main(String[] args) &#123;\n        // 创建一个列表\n        MyList&lt;String> list = new MyList&lt;>(5);\n        list.add(\"One\");\n        list.add(\"Two\");\n        list.add(\"Three\");\n        list.add(\"Four\");\n        list.add(\"Five\");\n\n        // 遍历列表中的元素\n            Iterator&lt;String> iterator = list.iterator();\n    while (iterator.hasNext()) &#123;\n        String element = iterator.next();\n        System.out.println(\"Element: \" + element);\n    &#125;\n&#125;\n\n 在上述示例中，我们定义了一个具体集合类MyList，该类实现了Aggregate接口，并包含了一个具体迭代器类MyListIterator。我们还定义了客户端代码用于测试集合的遍历，创建了一个列表并添加了元素，然后使用迭代器遍历列表中的每一个元素。执行该程序，会输出以下结果： \nElement: One\nElement: Two\nElement: Three\nElement: Four\nElement: Five\n\n在迭代器模式中，集合和迭代器是紧密相关联的。集合提供了快速访问迭代器的方法，而迭代器则提供了遍历整个集合的方法。这样，我们可以通过创建不同的迭代器对象来实现不同的遍历方式，而不会影响到集合的底层实现。因此，迭代器模式非常适合于需要访问和遍历集合元素的场景，可以提供一种清晰且灵活的解决方案。\n","slug":"迭代器模式","date":"2023-05-15T09:44:26.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"35b7e7d45f0bf660fac88351c60754e3","title":"命令模式","content":"命令模式它允许将请求封装为对象，从而使得请求的发送者和接收者解耦，同时可实现请求的队列化、撤销和恢复。\n该模式的关键点是将请求与实现解耦并引入命令对象，以便将请求发送给不同的对象，并可随时进行撤销和恢复。通常情况下，命令对象包含执行请求的接收者、请求数据以及实现该请求的方法。\n命令模式包括以下几个元素：\n1.命令接口（Command）：此接口定义了一个execute()方法，当Command对象被调用时，该方法会调用具体命令的execute()方法。\n2.具体命令类（Concrete Command）：此类实现了Command接口的execute()方法，它包含了一个Receiver对象，并包含了执行所需的所有参数。\n3.接收者（Receiver）：该组件包含了实现命令所需的代码和逻辑。\n4.调用者（Invoker）：该组件负责发送命令，通常会包含一个Command对象的引用。它还可维护一个命令历史记录，以便支持撤销和恢复操作。\n5.客户端（Client）：该组件创建具体命令对象，并将其传递给调用者对象以执行所需的操作。\n通过使用命令模式，我们可以将请求者与实现者彻底解耦，使得请求者无需知道具体执行者的存在和实现细节，并且可以轻松地对请求进行控制、评估、测试和管理。同时，命令模式还能实现请求的撤销和恢复，大大提升系统的可靠性和可维护性。\n以下是一个简单的Java案例，演示了命令模式的使用方式：\n//命令接口\npublic interface Command &#123;\n    void execute();\n&#125;\n\n//具体命令类: 打开文件命令\npublic class OpenCommand implements Command &#123;\n    private Receiver receiver;\n\n    public OpenCommand(Receiver receiver) &#123;\n        this.receiver = receiver;\n    &#125;\n\n    public void execute() &#123;\n        receiver.open();\n    &#125;\n&#125;\n\n//具体命令类: 关闭文件命令\npublic class CloseCommand implements Command &#123;\n    private Receiver receiver;\n\n    public CloseCommand(Receiver receiver) &#123;\n        this.receiver = receiver;\n    &#125;\n\n    public void execute() &#123;\n        receiver.close();\n    &#125;\n&#125;\n\n//接收者：文件编辑器\npublic class Receiver &#123;\n    public void open() &#123;\n        System.out.println(\"打开文件\");\n    &#125;\n\n    public void close() &#123;\n        System.out.println(\"关闭文件\");\n    &#125;\n&#125;\n\n//调用者：文本编辑器\npublic class Invoker &#123;\n    private Command openCommand;\n    private Command closeCommand;\n\n    public Invoker(Command openCommand, Command closeCommand) &#123;\n        this.openCommand = openCommand;\n        this.closeCommand = closeCommand;\n    &#125;\n\n    public void clickOpen() &#123;\n        openCommand.execute();\n    &#125;\n\n    public void clickClose() &#123;\n        closeCommand.execute();\n    &#125;\n&#125;\n\n//客户端代码\npublic class CommandDemo &#123;\n    public static void main(String[] args) &#123;\n        // 创建接收者\n        Receiver receiver = new Receiver();\n\n        // 创建具体命令对象\n        Command openCommand = new OpenCommand(receiver);\n        Command closeCommand = new CloseCommand(receiver);\n\n        // 创建调用者并将具体命令对象传给它\n        Invoker invoker = new Invoker(openCommand, closeCommand);\n\n        // 点击打开文件按钮\n        invoker.clickOpen();\n\n        // 点击关闭文件按钮\n        invoker.clickClose();\n    &#125;\n&#125;\n\n在上述示例中，我们定义了命令接口Command及两个具体命令类OpenCommand和CloseCommand，对应于文件的打开和关闭操作。我们还定义了一个接收者Receiver，表示文件编辑器，拥有打开和关闭文件的操作。Invoker负责调用具体命令对象执行对应的操作，即点击打开或关闭文件按钮。客户端代码创建具体命令对象和调用者，并传入。\n","slug":"命令模式","date":"2023-05-15T09:17:29.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"d75c20296e97428894e8c62bab0f606e","title":"访问者模式","content":"访问者模式它将算法封装到独立的对象中，使其可以在不修改现有对象结构的情况下增加新的操作。访问者模式的核心思想是在被访问的对象上定义一个接受访问者对象的接口，然后访问者对象通过该接口访问被访问对象。这样可以使得被访问对象保持稳定，而访问者对象可以根据需要进行扩展。\n访问者模式包含以下几个元素：\n1.访问者（Visitor）：该组件定义了处理对象结构中各个元素所要执行的方法，这些方法接受的参数可以是元素本身，也可以是元素的特定属性。\n2.具体访问者（Concrete Visitor）：该组件实现了访问者定义的方法，以便可以对对象结构中的不同元素执行特定的操作。\n3.元素（Element）：该组件定义了访问者能够访问的接口，一般它包含一个 accept() 方法，该方法接受一个访问者作为参数。\n4.具体元素（Concrete Element）：该组件实现了元素的接口，它可以被一个具体访问者实例访问并执行特定的操作。\n5.对象结构（Object Structure）：该组件是元素的集合，它定义了元素的集合以及提供可以接受访问者访问的接口。\n以下是一个简单的Java案例，演示了访问者模式的使用方式：\n假设我们有一个图形类层次结构，包括Rectangle、Circle和Triangle三个具体类型。我们可以定义一个Visitor接口，将不同的操作定义为Visitor的方法。然后，我们可以实现具体的Visitor类 RectangleVisitor、CircleVisitor和TriangleVisitor，并为每个具体类提供accept()方法，该方法接受一个Visitor对象并调用visit()方法。\n// Element接口\ninterface Shape &#123;\n    void accept(Visitor v);\n&#125;\n\n// 具体元素类：矩形\nclass Rectangle implements Shape &#123;\n    public void accept(Visitor v) &#123;\n        v.visit(this);\n    &#125;\n&#125;\n\n// 具体元素类：圆形\nclass Circle implements Shape &#123;\n    public void accept(Visitor v) &#123;\n        v.visit(this);\n    &#125;\n&#125;\n\n// 具体元素类：三角形\nclass Triangle implements Shape &#123;\n    public void accept(Visitor v) &#123;\n        v.visit(this);\n    &#125;\n&#125;\n\n// Visitor接口\ninterface Visitor &#123;\n    void visit(Rectangle r);\n    void visit(Circle c);\n    void visit(Triangle t);\n&#125;\n\n// 具体访问者类：用于计算图形面积\nclass AreaVisitor implements Visitor &#123;\n    public void visit(Rectangle r) &#123;\n        System.out.println(\"计算矩形的面积\");\n    &#125;\n    public void visit(Circle c) &#123;\n        System.out.println(\"计算圆形的面积\");\n    &#125;\n    public void visit(Triangle t) &#123;\n        System.out.println(\"计算三角形的面积\");\n    &#125;\n&#125;\n\n// 具体访问者类：用于计算图形周长\nclass PerimeterVisitor implements Visitor &#123;\n    public void visit(Rectangle r) &#123;\n        System.out.println(\"计算矩形的周长\");\n    &#125;\n    public void visit(Circle c) &#123;\n        System.out.println(\"计算圆形的周长\");\n    &#125;\n    public void visit(Triangle t) &#123;\n        System.out.println(\"计算三角形的周长\");\n    &#125;\n&#125;\n\n// 测试代码\npublic class VisitorDemo &#123;\n    public static void main(String[] args) &#123;\n        // 创建图形列表\n        Shape[] shapes = &#123; new Rectangle(), new Circle(), new Triangle() &#125;;\n\n        // 计算面积\n        Visitor areaVisitor = new AreaVisitor();\n        for (Shape shape : shapes) &#123;\n            shape.accept(areaVisitor);\n        &#125;\n\n        // 计算周长\n        Visitor perimeterVisitor = new PerimeterVisitor();\n        for (Shape shape : shapes) &#123;\n            shape.accept(perimeterVisitor);\n        &#125;\n    &#125;\n&#125;\n\n在上述示例中，我们定义了三个具体元素类：矩形、圆形和三角形，并定义了两个具体访问者类：用于计算面积和周长。在测试代码中，我们创建了一个图形列表，遍历该列表并分别使用AreaVisitor和PerimeterVisitor访问器计算每一个图形的面积和周长。执行该程序会输出如下结果：\n计算矩形的面积\n计算圆形的面积\n计算三角形的面积\n计算矩形的周长\n计算圆形的周长\n计算三角形的周长\n\n案例二\n下面是一个Java实现访问者模式的小案例。假设有一个电商平台，需要对用户购买记录进行分析，包括计算用户购买总金额、计算用户购买物品种类数等统计工作。首先定义被访问对象：购物车条目CartItem和用户购物车Cart：\npublic interface CartItem &#123;\n    void accept(Visitor visitor);\n&#125;\n\npublic class ProductCartItem implements CartItem &#123;\n    private String name;\n    private double price;\n\n    public ProductCartItem(String name, double price) &#123;\n        this.name = name;\n        this.price = price;\n    &#125;\n\n    public String getName() &#123;\n        return name;\n    &#125;\n\n    public double getPrice() &#123;\n        return price;\n    &#125;\n\n    @Override\n    public void accept(Visitor visitor) &#123;\n        visitor.visit(this);\n    &#125;\n&#125;\n\npublic class ServiceCartItem implements CartItem &#123;\n    private String name;\n    private double fee;\n\n    public ServiceCartItem(String name, double fee) &#123;\n        this.name = name;\n        this.fee = fee;\n    &#125;\n\n    public String getName() &#123;\n        return name;\n    &#125;\n\n    public double getFee() &#123;\n        return fee;\n    &#125;\n\n    @Override\n    public void accept(Visitor visitor) &#123;\n        visitor.visit(this);\n    &#125;\n&#125;\n\npublic class Cart &#123;\n    private List&lt;CartItem> cartItems = new ArrayList&lt;>();\n\n    public void addItem(CartItem item) &#123;\n        cartItems.add(item);\n    &#125;\n\n    public void removeItem(CartItem item) &#123;\n        cartItems.remove(item);\n    &#125;\n\n    public void accept(Visitor visitor) &#123;\n        for (CartItem item : cartItems) &#123;\n            item.accept(visitor);\n        &#125;\n        visitor.visit(this);\n    &#125;\n&#125;\n\n其中，Cart类为购物车类，可以添加和删除购物项，accept方法接受Visitor访问。\n然后定义访问者Visitor，用来计算购物车中的统计数据：\npublic interface Visitor &#123;\n    void visit(ProductCartItem item);\n    void visit(ServiceCartItem item);\n    void visit(Cart cart);\n&#125;\n\npublic class CartStatisticsVisitor implements Visitor &#123;\n    private double totalAmount;\n    private int distinctProductCount;\n\n    public double getTotalAmount() &#123;\n        return totalAmount;\n    &#125;\n\n    public int getDistinctProductCount() &#123;\n        return distinctProductCount;\n    &#125;\n\n    @Override\n    public void visit(ProductCartItem item) &#123;\n        totalAmount += item.getPrice();\n        distinctProductCount++;\n    &#125;\n\n    @Override\n    public void visit(ServiceCartItem item) &#123;\n        totalAmount += item.getFee();\n    &#125;\n\n    @Override\n    public void visit(Cart cart) &#123;\n        System.out.printf(\"Total amount: $%.2f, distinct product count: %d%n\", \n            totalAmount, distinctProductCount);\n    &#125;\n&#125;\n\nCartStatisticsVisitor类实现了Visitor接口，计算购物车中的数量和总金额。visit方法根据被访问对象进行相应的计算，并将结果记录在中间变量中。最后，visit(Cart cart)方法输出结果。\n使用访问者模式统计购物车上的总金额和不同商品数量：\npublic class Main &#123;\n    public static void main(String[] args) &#123;\n        Cart cart = new Cart();\n        cart.addItem(new ProductCartItem(\"Apple\", 3.99));\n        cart.addItem(new ProductCartItem(\"Banana\", 2.99));\n        cart.addItem(new ServiceCartItem(\"Delivery fee\", 5.99));\n\n        CartStatisticsVisitor visitor = new CartStatisticsVisitor();\n        cart.accept(visitor);\n    &#125;\n&#125;\n\n输出结果为：\nTotal amount: $12.97, distinct product count: 2\n\n这样，通过访问者模式可以方便地对购物车数据进行统计分析，而不会修改现有代码。\n","slug":"访问者模式","date":"2023-05-15T08:21:18.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"ba45023a93415880bd8ff889992600c8","title":"中介者模式","content":"中介者模式它的作用是减少对象之间的直接耦合关系，通过引入一个中介者对象来协调多个对象之间的交互行为，从而将系统中复杂的网状关系变为简单的星型结构。\n中介者模式的机制基于以下两个关键点：\n\n抽象中介者（Mediator）：为了把各同事类的耦合度降到最低，将对象间通信的控制交给一个中介者来协调调度，具体同事类都依赖中介者，由中介者负责消息的转发和协调。\n具体中介者（Concrete Mediator）：它从具体的同事类接收消息，并向具体同事类发送命令。它将各个同事对象之间的交互过程所需的信息进行了封装，使得各个同事类不再需要显式地引用其他同事类。\n\n中介者模式的优点有：\n\n中介者模式简化了对象之间的交互关系，将对象之间复杂的网状结构转变为简单的星型结构。\n中介者模式使得各对象之间的耦合度大大降低，可以独立地变化和复用。\n中介者模式易于扩展，增加新的同事类时不需要修改原有的代码，只需要按照同样的方式实现新的同事类即可。\n\n中介者模式的缺点有：\n\n中介者模式会增加系统中对象的个数，使得系统变得复杂。\n中介者模式中，中介者对象可能会变得过于复杂，难以维护与更新。\n\n下面是一个简单的中介者模式的示例，假设有三个对象之间需要进行通信：买方、卖方和中介者。买方想要购买物品，卖方想要出售物品，而中介者作为协调者来帮助买卖双方进行交易。\n// 定义买方类\npublic class Buyer &#123;\n    private Mediator mediator;\n\n    public void setMediator(Mediator mediator) &#123;\n        this.mediator = mediator;\n    &#125;\n\n    public void buyItem() &#123;\n        mediator.buyItem();\n    &#125;\n\n    public void receiveItem() &#123;\n        System.out.println(\"Received item from seller.\");\n    &#125;\n&#125;\n\n// 定义卖方类\npublic class Seller &#123;\n    private Mediator mediator;\n\n    public void setMediator(Mediator mediator) &#123;\n        this.mediator = mediator;\n    &#125;\n\n    public void sellItem() &#123;\n        mediator.sellItem();\n    &#125;\n\n    public void receivePayment() &#123;\n        System.out.println(\"Received payment from buyer.\");\n    &#125;\n&#125;\n\n// 定义中介者接口\npublic interface Mediator &#123;\n    void buyItem();\n    void sellItem();\n&#125;\n\n// 定义具体中介者类\npublic class TradeMediator implements Mediator &#123;\n    private Buyer buyer;\n    private Seller seller;\n\n    public void setBuyer(Buyer buyer) &#123;\n        this.buyer = buyer;\n        buyer.setMediator(this);\n    &#125;\n\n    public void setSeller(Seller seller) &#123;\n        this.seller = seller;\n        seller.setMediator(this);\n    &#125;\n\n    public void buyItem() &#123;\n        seller.receivePayment();\n        buyer.receiveItem();\n    &#125;\n\n    public void sellItem() &#123;\n        buyer.receivePayment();\n        seller.receiveItem();\n    &#125;\n&#125;\n\n// 测试类\npublic class MediatorTest &#123;\n    public static void main(String[] args) &#123;\n        TradeMediator mediator = new TradeMediator();\n        Buyer buyer = new Buyer();\n        Seller seller = new Seller();\n\n        mediator.setBuyer(buyer);\n        mediator.setSeller(seller);\n\n        buyer.buyItem();\n        seller.sellItem();\n    &#125;\n&#125;\n\n在这个示例中，各个类分别代表了买方、卖方和中介者。买方需要购买物品并接收物品，卖方需要出售物品并接收付款，而中介者则负责调节买卖双方之间的交易。通过中介者模式，买方和卖方可以通过中介者来进行通信，它们不需要直接相互了解，从而降低了耦合度。\n","slug":"中介者模式","date":"2023-05-15T07:45:06.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"4fe76f7b940933ddf5de5f39814faa1c","title":"MySQL_DQL","content":"DQLDQL英文全称是Data Query Language(数据查询语言)，数据查询语言，用来查询数据库中表的记录\n基本语法DQL 查询语句，语法结构如下：\nSELET\n\t字段列表\nFROM\n\t表名列表\nWHERE\n\t条件列表\nGROUP BY\n\t分组字段列表\nHAVING\n\t分组后条件列表\nORDER BY\n\t排序字段列表\nLIMIT\n\t分页参数\n\n\n基本查询（不带任何条件）\n\n条件查询（WHERE）\n\n聚合函数（count、max、min、avg、sum）\n\n分组查询（group by）\n\n排序查询（order by）\n\n分页查询（limit）\n\n\n基础查询 * 号代表查询所有字段，在实际开发中尽量少用（不直观、影响效率）\n 查询多个字段\nSELECT 字段1, 字段2, 字段3 ... FROM 表名 ;\n\nSELECT * FROM 表名 ;\n\n字段设置别名\nSELECT 字段1 [ AS 别名1 ] , 字段2 [ AS 别名2 ] ... FROM 表名; \n\nSELECT 字段1 [ 别名1 ] , 字段2 [ 别名2 ] ... FROM 表名;\n\n去除重复记录(DISTINCT)\nSELECT DISTINCT 字段列表 FROM 表名;\n\n查询所有员工的工作地址,起别名,不重复\nSELECT DISTINCT workaddress '工作地址' from  emp;\n\n条件查询语法\nSELECT 字段名 from 表名 WHERE 条件列表; \n\n\n\n\n运算符\n功能\n\n\n\n&gt; 、&gt;&#x3D;\n大于、大于等于\n\n\n&lt; 、&lt;&#x3D;\n小于、小于等于\n\n\n&#x3D;\n等于\n\n\n&lt;&gt; 、!&#x3D;\n不等于\n\n\nBETWEEN … AND …\n在某个范围之内(包含最小、最大值)\n\n\nIN(…)\n在in之后的列表中的值\n\n\nLIKE  占位符\n模糊匹配(_匹配单个字符, %匹配任意个字符)\n\n\nIS  NOT NULL\n不是NULL\n\n\nIS  NULL\n是NULL\n\n\nAND 、&amp;&amp;\n并且 (多个条件同时成立)\n\n\nOR 、||\n或者 (多个条件任意一个成立)\n\n\nNOT 、!\n非 , 不是\n\n\n 小案例：查询没有身份证号的员工信息\n不可以使用 &#x3D; null\nSELECT * FROM emp WHERE idcard IS NULL;\n\n聚合函数\n将一列数据作为一个整体，进行纵向计算\n常见的聚合函数\n\n\n\n函数\n描述\n\n\n\ncount\n统计数量\n\n\nmax\n最大值\n\n\nmin\n最小值\n\n\navg\n平均值\n\n\nsum\n求和\n\n\n语法\nSELECT 聚合函数(字段列表) FROM 表名 [条件查询] ...;\n\n注意 :NULL值是不参与所有聚合函数运算的\n对于count聚合函数，统计符合条件的总记录数，还可以通过 count(数字&#x2F;字符串)的形式进行统计查询\n小案例：统计该企业员工数量\nSELECT count(idcard) from emp;\t-- 字段值为NULL，是不参与所有聚合函数运算的\nSELECT count(*) from emp;\nSELECT count(1) from emp;\n\n分组查询\nSELECT 字段列表 FROM 表名 [ WHERE 条件 ] GROUP BY 分组字段名 [ HAVING 分组\n后过滤条件 ];\n\n where与having区别\n\n执行时机不同：where是分组之前进行过滤，不满足where条件，不参与分组；而having是分组之后对结果进行过滤\n\n判断条件不同：where不能对聚合函数进行判断，而having可以\n\n\n注意事项:\n\n分组之后，查询的字段一般为聚合函数和分组字段，查询其他字段无任何意义。\n\n执行顺序: where &gt; 聚合函数 &gt; having 。\n\n支持多字段分组, 具体语法为 : group by columnA,columnB\n\n\n小案例：查询年龄小于45的员工 , 并根据工作地址分组 , 获取员工数量大于等于3的工作地址\nselect workaddress,count(*) number from emp \nwhere age &lt; 45 \ngroup by workaddress\nhaving number >= 3;\n\n小案例： 统计各个工作地址上班的男性及女性员工的数量\nselect workaddress,gender,count(*) number from emp GROUP BY workaddress, gender;\n\n排序查询 语法\nSELECT 字段列表 FROM 表名 ORDER BY 字段1 排序方式1 , 字段2 排序方式2 ;\n\n 排序方式      ASC : 升序(默认值)      DESC: 降序\n如果是升序, 可以不指定排序方式ASC ;\n如果是多字段排序，当第一个字段值相同时，才会根据第二个字段进行排序 ;\n小案例：根据年龄对公司的员工进行升序排序 , 年龄相同 , 再按照入职时间进行降序排序\nselect * from emp order by age desc, entrydate desc;\n\n分页查询语法\n起始索引从0开始，起始索引 &#x3D; （查询页码 - 1）* 每页显示记录数。\n分页查询是数据库的方言，不同的数据库有不同的实现，MySQL中是LIMIT。\n如果查询的是第一页数据，起始索引可以省略，直接简写为 limit 10。\nSELECT 字段列表 FROM 表名 LIMIT 起始索引, 查询记录数 ;\n\n小案例：查询第2页员工数据, 每页展示10条记录 ——–&gt; (页码-1)*页展示记录数\nselect * from emp limit 10, 10;\n\n执行顺序\n验证  : 查询年龄大于15的员工姓名、年龄，并根据年龄进行升序排序\nselect name, age from emp where age > 15 order by age asc;\n\n在查询时，我们给emp表起一个别名 e，然后在select 及 where中使用该别名。\nselect e.name, e.age from emp e where e.age > 15 order by age asc;\n\n执行上述SQL语句后，我们看到依然可以正常的查询到结果，此时就说明： from 先执行, 然后where 和 select 执行。\n那 where 和 select 到底哪个先执行呢?\n此时，此时我们可以给select后面的字段起别名，然后在 where 中使用这个别名，然后看看是否可以执行成功。\nselect e.name, e.age eage from emp e where eage > 15 order by e.age asc;\n\n执行上述SQL报错了:1054 - Unknown column ‘eage’ in ‘where clause’\n说明是执行完from之后，到执行where。\n接下来，我们再执行如下SQL语句，查看执行效果：\nselect e.name ename , e.age eage from emp e where e.age > 15 order by eage asc;\n\n结果执行成功。 那么也就验证了: order by 是在select 语句之后执行的。\n综上所述，DQL语句的执行顺序为：\nfrom ... where ... group by ...having ... select ... order by ... limit ...\n\n","slug":"MySQL-DQL","date":"2023-05-14T06:22:31.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"676b4ad0d6495672245716dc73cc8c9e","title":"MySQL_DML","content":"DMLDML英文全称是Data Manipulation Language(数据操作语言)，用来对数据库中表的数据记录进\n行增、删、改操作。\n添加数据 给指定字段添加数据\nINSERT INTO 表名(字段1, 字段2, ...) VALUES(值1, 值2, ...);\n\n案例: 给employee表所有的字段添加数据\nINSERT INTO employee(id,workno,name,gender,age,idcard,entrydate)\nVALUES(1,'1','HJ','女',20,'450802200206141527','2023-05-14');\n\n给全部字段添加数据\nINSERT INTO employee \nvalues(2,'2','H','男',23,'45080220001121057X','2023-07-10');\n\n批量添加数据\nINSERT INTO 表名(字段1，字段2, ...) values\n(v1, v2, ...),(a1, a2, ...),(b1, b2, ...);\n/*\n或者\n*/\nINSERT INTO 表名\nVALUES (v1, v2, ...),(a1, a2, ...),(b1, b2, ...);\n\n执行DML语句，会检查插入的字段是否符合字段的类型，不符合会报错。\n 插入数据时，指定的字段顺序需要与值的顺序是一一对应的。\n 字符串和日期型数据应该包含在引号中。\n插入的数据大小，应该在字段的规定范围内。\n日期类型要连续，否则会报错\n修改数据\nUPDATE 表名 SET 字段1 &#x3D; V1, 字段2 &#x3D; V2, ... [WHERE 条件];\n\n删除数据\nDELEDE FROM 表名 [WHERE 条件];\n\n• DELETE 语句的条件可以有，也可以没有，如果没有条件，则会删除整张表的所有数据。\n• DELETE 语句不能删除某一个字段的值(可以使用UPDATE，将该字段值置为NULL即可)。\n\n\n\n类型\n大小\n有符号范围\n无符号范围\n描述\n\n\n\nTINYINT\n1byte\n(-2^7，2^7-1)\n(0，2^8-1)\n小整数值\n\n\nSMALLINT\n2bytes\n(-2^15，2^15-1)\n(0，2^16-1)\n大整数值\n\n\nMEDIUMINT\n3bytes\n(-2^23，2^23-1)\n(0，2^24-1)\n大整数值\n\n\nINT&#x2F;INTEGER\n4bytes\n(-2^31，2^31-1)\n(0，2^32-1)\n大整数值\n\n\nBIGINT\n8bytes\n(-2^63，2^63-1)\n(0，2^64-1)\n大整数值\n\n\nFLOAT\n4bytes\n(-3.402823466 E+38，3.402823466351 E+38)\n\n双精度浮点数值\n\n\nDOUBLE\n8bytes\n(-1.7976931348623157E+308，1.7976931348623157E+308)\n\n单精度浮点数值\n\n\nDECIMAL\n\n依赖于M(精度)和D(标度)\n依赖于M(精度)和D(标度)\n小数值(精确定点数)\n\n\n\n\n\n类型\n大小\n描述\n\n\n\nCHAR\n0-255 bytes\n定长字符串(需要指定长度)\n\n\nVARCHAR\n0-65535 bytes\n变长字符串(需要指定长度)\n\n\nTINYBLOB\n0-255 bytes\n不超过255个字符的二进制数据\n\n\nTINYTEXT\n0-255 bytes\n短文本字符串\n\n\nBLOB\n0-65 535 bytes\n二进制形式的长文本数据\n\n\nTEXT\n0-65 535 bytes\n长文本数据\n\n\nMEDIUMBLOB\n0-16 777 215 bytes\n二进制形式的中等长度文本数据\n\n\nMEDIUMTEXT\n0-16 777 215 bytes\n中等长度文本数据\n\n\nLONGBLOB\n0-4 294 967 295 bytes\n二进制形式的极大文本数据\n\n\nLONGTEXT\n0-4 294 967 295 bytes\n极大文本数据\n\n\n\n\n\n类型\n大小\n范围\n格式\n描述\n\n\n\nDATE\n3\n1000-01-01 至 9999-12-31\nYYYY-MM-DD\n日期值\n\n\nTIME\n3\n-838:59:59 至 838:59:59\nHH:MM:SS\n时间值或持续时间\n\n\nYEAR\n1\n1901 至 2155\nYYYY\n年份值\n\n\nDATETIME\n8\n1000-01-01 00:00:00 至9999-12-31 23:59:59\nYYYY-MM-DD HH:MM:SS\n混合日期和时间值\n\n\nTIMESTAMP\n4\n1970-01-01 00:00:01 至2038-01-19 03:14:07\nYYYY-MM-DD HH:MM:SS\n混合日期和时间值，时间戳\n\n\n","slug":"MySQL-DML","date":"2023-05-14T01:30:25.000Z","categories_index":"","tags_index":"MySQL","author_index":"大宝贝的程序员"},{"id":"56fbe19991b11dc0bb9030f36adcdf92","title":"MySQL_DDL_Linux","content":"启动MySQL服务启动mysql服务\nsystemctl start mysqld\n\n重启mysql服务\nsystemctl restart mysqld\n\n停止mysql服务\nsystemctl stop mysqld\n\n查询首次安装数据库自动生成的root密码grep 'temporary password' /var/log/mysqld.log\n\n命令行执行指令\nmysql [-h 127.0.0.1] -u root -p密码\n\n修改root用户密码登录到MySQL之后，需要将自动生成的不便记忆的密码修改了，修改成自己熟悉的便于记忆的密码\nALTER USER 'root'@'localhost' IDENTIDFIED BY 'hj0614';\n\n执行上述的SQL会报错，原因是因为设置的密码太简单，密码复杂度不够。\n设置密码的复杂度为简单类型\nset global validate_password.policy = 0;\n\n设置密码长度为6\nset global validate_password.length = 6;\n\n再次执行上述修改密码的指令!\n创建用户默认的root用户只能当前节点localhost访问，是无法远程访问的\n我们还需要创建一个root账户，用户远程访问\ncreate user 'root'@'%' IDENTIFIDE WITH mysql_native_password BY 'hj0614';\n\n给用户分配权限grant all on root.* to 'root'@'%';\n\n重新连接mysql\n查看权限show grants for  'root'@'%';\n\n撤销权限revoke all on root.* from 'root'@'%'\n\nSQL通用语法\nSQL语句可以单行或多行书写，以分号 ; 结尾。\nSQL语句可以使用空格 &#x2F; 缩进来增强语句的可读性。\nMySQL数据库的SQL语句不区分大小写，关键字建议使用大写。\n注释：\n单行注释：– 注释内容 或 # 注释内容\n多行注释：&#x2F;* 注释内容 *&#x2F;\n\nSQL分类SQL语句，根据其功能，主要分为四类：DDL、DML、DQL、DCL\n\n\n\nDDL：Data Definition Language\n数据定义语言，用来定义数据库对象(数据库，表，字段）\n\n\n\nDML：Data Manipulation  Language\n数据操作语言，用来对数据库表中的数据进行增删改\n\n\nDQL：Data Query Language\n数据查询语言，用来查询数据库中表的记录\n\n\nDCL：Data Control Language\n数据控制语言，用来创建数据库用户、控制数据库的访问权限\n\n\nDDL语句查询所有数据库\nshow databases;\n\n查询当前数据库\nselect database();\n\n创建数据库 [表示可选项]\ncreate database [if not exists] 数据库名 [default charset 字符集] [collate 排序规则];\n\n示例：创一个数据库使用默认的字符集，默认排列\ncreate database if not exists test; \n\n删除数据库     如果删除一个不存在的数据库，将会报错。此时，可以加上参数 if exists\ndrop database [if exists] 数据库名;\n\n切换数据库\nuse 数据库名;\n\n查询当前数据库所有表\nshow tables;\n\n查看指定表结构\n通过这条指令，我们可以查看到指定表的字段，字段的类型、是否可以为NULL，是否存在默认值等信息\ndesc 表名;\n\n查询指定表的建表语句\n通过这条指令，主要是用来查看建表语句的，而有部分参数我们在创建表的时候，并未指定也会查询\n到，因为这部分是数据库的默认值，如：存储引擎、字符集等。\nshow create table 表名;\n\n小案例：查看employee表的建表语句\nemployee\tCREATE TABLE `employee` (\n  `id` int DEFAULT NULL COMMENT '编号',\n  `workno` varchar(10) DEFAULT NULL,\n  `name` varchar(10) DEFAULT NULL COMMENT '姓名',\n  `gender` char(1) DEFAULT NULL COMMENT '性别',\n  `age` tinyint unsigned DEFAULT NULL COMMENT '年龄',\n  `idcard` char(18) DEFAULT NULL COMMENT '身份证号',\n  `workaddress` varchar(50) DEFAULT NULL COMMENT '工作地址',\n  `entrydate` date DEFAULT NULL COMMENT '入职时间'\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='员工表'\n\n创建表结构\t\n最后一个字段后面没有逗号\nCREATE TABLE 表名(\n字段1 字段1类型 [COMMENT 字段1注释],\n字段2 字段2类型 [COMMENT 字段2注释],\n...\n字段n 字段n类型 [COMMENT 字段n注释]\n) [COMMENT 表注释];\n\n小案例：设计一张员工信息表，要求如下：\n\n编号（纯数字）\n\n员工工号 (字符串类型，长度不超过10位)\n\n员工姓名（字符串类型，长度不超过10位）\n\n性别（男&#x2F;女，存储一个汉字）\n\n年龄（正常人年龄，不可能存储负数）\n\n身份证号（二代身份证号均为18位，身份证中有X这样的字符）\n\n入职时间（取值年月日即可）\n\n\ncreate table emp(\n id int comment '编号',\n workno varchar(10) comment '员工工号',\n name varchar(10) comment '员工姓名',\n sex char(1) comment '性别',\n age tinyint comment '年龄',\n idcard char(18) comment '身份证',\n entrydate date  comment '入职时间'\n) comment '员工信息表'\n\n添加表字段(ADD)\nALTER TABLE 表名 ADD 字段名 类型(长度) [comment 注释] [约束];\n\n小案例：为emp添加一个新的字段”昵称”为nickname，类型为varchar(20);\nALTER TABLE emp ADD nickname varchar(20) comment '昵称';\n\n修改数据类型(MODIFY)\nALTER TABLE 表名 MODIFY 字段名 新数据类型(长度);\n\n 修改字段名和字段类型(CHANGE)\nALTER TABLE 表名 CHANGE 旧字段名 新字段名 类型(长度) [comment 注释] [约束]\n\n小案例：将emp表的nickname字段修改为username，类型为varchar(30)\nALTER TABLE emp CHANGE nikename username varchar(30) comment '昵称';\n\n删除字段(DROP)\nALTER TABLE 表名 DROP 字段名;\n\n小案例：将emp表的字段username删除\nALTER TABLE emp DROP username;\n\n修改表名(RENAME TO)\nALTER TABLE 表名 RENAME TO 新表名;\n\n小案例：将emp表的表名修改为 employee\nALTER TABLE emp RANAME TO employee;\n\n 删除表\nDROP TABLE [IF EXISTS] 表名;\n\n小案例：如果tb_user表存在，则删除tb_user表\nDROP TABLE IF EXISTS tb_user;\n\n 删除指定表, 并重新创建表\nTRUNCATE TABLE 表名;\n\n","slug":"MySQL-DDL","date":"2023-05-13T10:36:19.000Z","categories_index":"","tags_index":"Linux,MySQL","author_index":"大宝贝的程序员"},{"id":"b847fc9b240aefdbf95225a3e7dc9026","title":"Spring Bean的生命周期","content":"Spring Bean的生命周期getBean方法都会调用doGetBean的逻辑\n    public &lt;T> T getBean(String name, @Nullable Class&lt;T> requiredType, @Nullable Object... args) throws BeansException &#123;\n        return this.doGetBean(name, requiredType, args, false);\n    &#125;\n\nprotected &lt;T> T doGetBean(String name, @Nullable Class&lt;T> requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException &#123;\n    ....\n    &#125;\n\nbean 的生命周期从调用 beanFactory 的 getBean 开始，到这个 bean 被销毁，可以总结为以下七个阶段：\n\n处理名称，检查缓存\n处理父子容器\n处理 dependsOn\n选择 scope 策略\n创建singleton\n创建prototype\n创建其他scope\n\n\n创建 bean\n创建bean实例\n依赖注入\n初始化\n登记可销毁的bean\n\n\n类型转换处理\n销毁 bean\n\n\n\n\n\n\n\n\n\n\n注意\n\n划分的阶段和名称并不重要，重要的是理解整个过程中做了哪些事情\n\n1. 处理名称，检查缓存\n\n这一步会处理别名，将别名解析为实际名称\n对 FactoryBean 也会特殊处理，如果以 &amp; 开头表示要获取 FactoryBean 本身，否则表示要获取其产品\n这里针对单例对象会检查一级、二级、三级缓存\nsingletonFactories 三级缓存，存放单例工厂对象\nearlySingletonObjects 二级缓存，存放单例工厂的产品对象\n如果发生循环依赖，产品是代理；无循环依赖，产品是原始对象\n\n\nsingletonObjects 一级缓存，存放单例成品对象\n\n\n\n2. 处理父子容器\n\n如果当前容器根据名字找不到这个 bean，此时若父容器存在，则执行父容器的 getBean 流程\n父子容器的 bean 名称可以重复\n\n3. 处理 dependsOn\n\n如果当前 bean 有通过 dependsOn 指定了非显式依赖的 bean，这一步会提前创建这些 dependsOn 的 bean \n所谓非显式依赖，就是指两个 bean 之间不存在直接依赖关系，但需要控制它们的创建先后顺序\n\n4. 选择 scope 策略\n\n对于 singleton scope，首先到单例池去获取 bean，如果有则直接返回，没有再进入创建流程\n对于 prototype scope，每次都会进入创建流程\n对于自定义 scope，例如 request，首先到 request 域获取 bean，如果有则直接返回，没有再进入创建流程\n\npublic class TestScope &#123;\n    public static void main(String[] args) &#123;\n        testRequestScope();\n    &#125;\n\n    // 单例 bean 从 refresh 被创建, 到 close 被销毁, BeanFactory 会记录哪些 bean 要调用销毁方法\n    private static void testSingletonScope() &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.registerBean(\"bean1\", Bean1.class);\n        context.registerBean(CommonAnnotationBeanPostProcessor.class);\n        context.refresh(); // getBean\n        context.close();\n    &#125;\n\n    // 多例 bean 从首次 getBean 被创建, 到调用 BeanFactory 的 destroyBean 被销毁\n    private static void testPrototypeScope() &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.registerBean(\"bean1\", Bean1.class, bd -> bd.setScope(\"prototype\"));\n        context.registerBean(CommonAnnotationBeanPostProcessor.class);\n        context.refresh();\n\n        Bean1 bean = context.getBean(Bean1.class);\n        // 没谁记录该 bean 要调用销毁方法, 需要我们自行调用\n        context.getDefaultListableBeanFactory().destroyBean(bean);\n\n        context.close();\n    &#125;\n\n    // request bean 从首次 getBean 被创建, 到 request 结束前被销毁\n    private static void testRequestScope() &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.getDefaultListableBeanFactory().registerScope(\"request\", new RequestScope());\n        context.registerBean(\"bean1\", Bean1.class, bd -> bd.setScope(\"request\"));\n        context.registerBean(CommonAnnotationBeanPostProcessor.class);\n        context.refresh();\n\n        for (int i = 0; i &lt; 2; i++) &#123;\n            new Thread(() -> &#123;\n                MockHttpServletRequest request = new MockHttpServletRequest();\n                // 每个 webRequest 对象会记录哪些 bean 要调用销毁方法\n                ServletWebRequest webRequest = new ServletWebRequest(request);\n                RequestContextHolder.setRequestAttributes(webRequest);\n\n                Bean1 bean = context.getBean(Bean1.class);\n                LoggerUtils.get().debug(\"&#123;&#125;\", bean);\n                LoggerUtils.get().debug(\"&#123;&#125;\", request.getAttribute(\"bean1\"));\n\n                // request 请求结束前调用这些销毁方法\n                webRequest.requestCompleted();\n            &#125;).start();\n        &#125;\n\n    &#125;\n\n    static class Bean1 &#123;\n        @PostConstruct\n        public void init() &#123;\n            LoggerUtils.get().debug(\"&#123;&#125; - init\", this);\n        &#125;\n\n        @PreDestroy\n        public void destroy() &#123;\n            LoggerUtils.get().debug(\"&#123;&#125; - destroy\", this);\n        &#125;\n    &#125;\n&#125;\n\n\n\n\n\n5.1 创建 bean - 创建 bean 实例   \t创建出空的实例\n\n\n\n要点\n总结\n\n\n\n有自定义 TargetSource 的情况\n由 AnnotationAwareAspectJAutoProxyCreator 创建代理返回\n\n\nSupplier 方式创建 bean 实例\n为 Spring 5.0 新增功能，方便编程方式创建  bean  实例\n\n\nFactoryMethod 方式  创建 bean  实例\n① 分成静态工厂与实例工厂；② 工厂方法若有参数，需要对工厂方法参数进行解析，利用  resolveDependency；③ 如果有多个工厂方法候选者，还要进一步按权重筛选\n\n\nAutowiredAnnotationBeanPostProcessor\n① 优先选择带  @Autowired  注解的构造；② 若有唯一的带参构造，也会入选\n\n\nmbd.getPreferredConstructors\n选择所有公共构造，这些构造之间按权重筛选\n\n\n采用默认构造\n如果上面的后处理器和 BeanDefiniation 都没找到构造，采用默认构造，即使是私有的\n\n\n5.2 创建 bean - 依赖注入\n\n\n\n要点\n总结\n\n\n\nAutowiredAnnotationBeanPostProcessor\n识别   @Autowired  及 @Value  标注的成员，封装为  InjectionMetadata 进行依赖注入\n\n\nCommonAnnotationBeanPostProcessor\n识别   @Resource  标注的成员，封装为  InjectionMetadata 进行依赖注入\n\n\nresolveDependency\n用来查找要装配的值，可以识别：① Optional；② ObjectFactory 及 ObjectProvider；③ @Lazy  注解；④ @Value  注解（${  }, #{ }, 类型转换）；⑤ 集合类型（Collection，Map，数组等）；⑥ 泛型和  @Qualifier（用来区分类型歧义）；⑦ primary  及名字匹配（用来区分类型歧义）\n\n\nAUTOWIRE_BY_NAME\n根据成员名字（set方法的名字）找 bean 对象，修改 mbd 的 propertyValues，不会考虑简单类型的成员\n\n\nAUTOWIRE_BY_TYPE\n根据成员类型执行 resolveDependency 找到依赖注入的值，修改  mbd 的 propertyValues\n\n\napplyPropertyValues\n根据 mbd 的 propertyValues 进行依赖注入（即xml中 &#96;&lt;property name ref\n\n\n依赖注入的优先级// 测试如果对同一属性进行的 @Autowired 注入、AUTOWIRE_BY_NAME、精确指定注入名称, 优先级是怎样的\npublic class TestInjection &#123;\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory());\n        context.registerBean(\"bean1\", Bean1.class, bd -> &#123;\n            // 优先级最高的：精确指定注入 bean 的名称 &lt;property name=\"bean3\" ref=\"bean2\"/> \t\t\t\t//property标签也是根据set方法进行注入的\n            bd.getPropertyValues().add(\"bean3\", new RuntimeBeanReference(\"bean2\"));\n            // 优先级次之的：通过 AUTOWIRE_BY_NAME 匹配\n            ((RootBeanDefinition) bd).setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_NAME);\n        &#125;);\n        context.registerBean(\"bean2\", Bean2.class);\n        context.registerBean(\"bean3\", Bean3.class);\n        context.registerBean(\"bean4\", Bean4.class);\n\n        context.refresh();\n    &#125;\n\n    static class Bean1 &#123;\n        MyInterface bean;\n\n        // 优先级最低的：@Autowired 匹配\n        @Autowired @Qualifier(\"bean4\")\n        public void setBean3(MyInterface bean) &#123;\n            System.out.println(bean);\n            this.bean = bean;\n        &#125;\n    &#125;\n\n    interface MyInterface &#123;\n    &#125;\n\n    static class Bean2 implements MyInterface &#123;\n    &#125;\n\n    static class Bean3 implements MyInterface &#123;\n    &#125;\n\n    static class Bean4 implements MyInterface &#123;\n    &#125;\n&#125;\n\n精确匹配的优先级最高，其次到按名字匹配AUTOWIRE_BY_NAME，最后是注解方式\n5.3 创建 bean - 初始化\n\n\n\n要点\n总结\n\n\n\n内置 Aware 接口的装配\n包括 BeanNameAware，BeanFactoryAware 等\n\n\n扩展 Aware 接口的装配\n由 ApplicationContextAwareProcessor 解析，执行时机在  postProcessBeforeInitialization\n\n\n@PostConstruct\n由 CommonAnnotationBeanPostProcessor 解析，执行时机在  postProcessBeforeInitialization\n\n\nInitializingBean\n通过接口回调执行初始化\n\n\ninitMethod\n根据 BeanDefinition 得到的初始化方法执行初始化，即 &lt;bean init-method&gt; 或 @Bean(initMethod)\n\n\n创建 aop 代理\n由 AnnotationAwareAspectJAutoProxyCreator 创建，执行时机在  postProcessAfterInitialization\n\n\n初始化方法的执行顺序public class TestInitialization &#123;\n\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.registerBean(CommonAnnotationBeanPostProcessor.class);\n        // &lt;bean init-method=\"initMethod\">\n        context.registerBean(\"bean1\", Bean1.class, bd -> bd.setInitMethodName(\"initMethod\"));\n        context.refresh();\n    &#125;\n\n    static class Bean1 implements InitializingBean, BeanFactoryAware &#123;\n\n        @Override\n        public void afterPropertiesSet() throws Exception &#123;\n            System.out.println(1);\n        &#125;\n\n        @PostConstruct\n        public void init() &#123;\n            System.out.println(2);\n        &#125;\n\n        public void initMethod() &#123;\n            System.out.println(3);\n        &#125;\n\n        @Override\n        public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123;\n            System.out.println(4);\n        &#125;\n    &#125;\n&#125;\n\n先执行内置 Aware 接口 -&gt;@PostConstruct -&gt;InitializingBean -&gt; initMethod\n4\n2\n1\n3\n\n5.4 创建 bean - 注册可销毁 bean\n在这一步判断并登记可销毁 bean\n\n判断依据\n如果实现了 DisposableBean 或 AutoCloseable 接口，则为可销毁 bean\n如果自定义了 destroyMethod，则为可销毁 bean\n如果采用 @Bean 没有指定 destroyMethod，则采用自动推断方式获取销毁方法名（close，shutdown）\n如果有 @PreDestroy 标注的方法\n\n\n存储位置\nsingleton scope 的可销毁 bean 会存储于 beanFactory 的成员当中\n自定义 scope 的可销毁 bean 会存储于对应的域对象当中\nprototype scope 不会存储，需要自己找到此对象销毁\n\n\n存储时都会封装为 DisposableBeanAdapter 类型对销毁方法的调用进行适配，体现了适配器模式\n\n6. 类型转换处理\n\n如果 getBean 的 requiredType 参数与实际得到的对象类型不同，会尝试进行类型转换\n\n7. 销毁 bean\n\n销毁时机\nsingleton bean 的销毁在 ApplicationContext.close 时，此时会找到所有 DisposableBean 的名字，逐一销毁\n自定义 scope bean 的销毁在作用域对象生命周期结束时\nprototype bean 的销毁可以通过自己手动调用 AutowireCapableBeanFactory.destroyBean 方法执行销毁\n\n\n同一 bean 中不同形式销毁方法的调用次序\n优先后处理器销毁，即 @PreDestroy\n其次 DisposableBean 接口销毁\n最后 destroyMethod 销毁（包括自定义名称，推断名称，AutoCloseable 接口 多选一）\n\n\n\n","slug":"Spring Bean的生命周期","date":"2023-05-13T04:32:54.000Z","categories_index":"","tags_index":"Java,Spring,面试题","author_index":"大宝贝的程序员"},{"id":"ec47f8662f4dbcb9ad26f0a7bcc5e407","title":"Spring refresh流程","content":"refresh 是 AbstractApplicationContext 中的一个方法，负责初始化 ApplicationContext 容器，容器必须调用 refresh 才能正常工作。它的内部主要会调用 12 个方法，我们把它们称为 refresh 的 12 个步骤：\n\nprepareRefresh –做好准备工作\n\nobtainFreshBeanFactory –创建或获取BeanFactory\n\nprepareBeanFactory –准备BeanFactory\n\npostProcessBeanFactory – 子类拓展BeanFactory\n\ninvokeBeanFactoryPostProcessors –后处理器拓展BeanFactory\n\nregisterBeanPostProcessors –准备Bean后处理器\n\ninitMessageSource –为ApplicationContext提供国际化功能\n\ninitApplicationEventMulticaster –为ApplicationContext提供事件发布器\n\nonRefresh – 留给子类拓展\n\nregisterListeners –为ApplicationContext准备监听器\n\nfinishBeanFactoryInitialization – 初始化单例Bean，执行Bean后处理器拓展\n\nfinishRefresh – 准备生命周期管理器，发布ContextRefreshed事件\n\n\n\n\n\n\n\n\n\n\n\n功能分类\n\n1 为准备环境\n\n2 3 4 5 6 为准备 BeanFactory\n\n7 8 9 10 12 为准备 ApplicationContext\n\n11 为初始化 BeanFactory 中非延迟单例 bean\n\n\n1. prepareRefresh\n\n这一步创建和准备了 Environment 对象，它作为 ApplicationContext 的一个成员变量\n\nEnvironment 对象的作用之一是为后续 @Value，值注入时提供键值\n\nEnvironment 分成三个主要部分\n\nsystemProperties - 保存 java 环境键值\nsystemEnvironment - 保存系统环境键值\n自定义 PropertySource - 保存自定义键值，例如来自于 *.properties 文件的键值\n\n\n\n\n示例// 如何获得和解析 @Value 内容\npublic class TestEnvironment &#123;\n    public static void main(String[] args) throws NoSuchFieldException, IOException &#123;\n        // 1) 获得 @Value 的值\n        System.out.println(\"=======================> 仅获取 @Value 值\");\n        QualifierAnnotationAutowireCandidateResolver resolver = new QualifierAnnotationAutowireCandidateResolver();\n        Object name = resolver.getSuggestedValue(new DependencyDescriptor(Bean1.class.getDeclaredField(\"name\"), false));\n        System.out.println(name);\n\n        // 2) 解析 @Value 的值\n        System.out.println(\"=======================> 获取 @Value 值, 并解析$&#123;&#125;\");\n        Object javaHome = resolver.getSuggestedValue(new DependencyDescriptor(Bean1.class.getDeclaredField(\"javaHome\"), false));\n        System.out.println(javaHome);\n        System.out.println(getEnvironment().resolvePlaceholders(javaHome.toString()));\n\n        // 3) 解析 SpEL 表达式\n        System.out.println(\"=======================> 获取 @Value 值, 并解析#&#123;&#125;\");\n        Object expression = resolver.getSuggestedValue(new DependencyDescriptor(Bean1.class.getDeclaredField(\"expression\"), false));\n        System.out.println(expression);\n        String v1 = getEnvironment().resolvePlaceholders(expression.toString());\n        System.out.println(v1);\n        //解析 #&#123;&#125;\n        System.out.println(new StandardBeanExpressionResolver().evaluate(v1, new BeanExpressionContext(new DefaultListableBeanFactory(),null)));\n    &#125;\n\n    private static Environment getEnvironment() throws IOException &#123;\n        //是Environment的重要实现，默认只能识别系统的键值，无法解析自定义的键值\n        //需要知道自定义键值的位置才能解析\n        StandardEnvironment env = new StandardEnvironment();\n        //添加自定文件的键值\n        env.getPropertySources().addLast(new ResourcePropertySource(\"jdbc\", new ClassPathResource(\"jdbc.properties\")));\n        return env;\n    &#125;\n\n    static class Bean1 &#123;\n        @Value(\"hello\")\n        private String name;\n\n        @Value(\"$&#123;jdbc.username&#125;\")\n        private String javaHome;\n\t\t\t\t//SpEL表达式\n        @Value(\"#&#123;'class version:' + '$&#123;java.class.version&#125;'&#125;\")\n        private String expression;\n    &#125;\n&#125;\n\n结果=======================> 仅获取 @Value 值\nhello\n=======================> 获取 @Value 值, 并解析$&#123;&#125;\n$&#123;jdbc.username&#125;\nroot\n=======================> 获取 @Value 值, 并解析#&#123;&#125;\n#&#123;'class version:' + '$&#123;java.class.version&#125;'&#125;\n#&#123;'class version:' + '61.0'&#125;\nclass version:61.0\n\n首先，创建了一个 QualifierAnnotationAutowireCandidateResolver 的实例，用来解析@Value的候处理器。然后创建了一个 new DependencyDescriptor(Bean1.class.getDeclaredField(“name”), false) 的实例用来描述 Bean1 类的 name 属性，这个实例中包含了该属性所在类的信息、属性的名称等详细信息。调用 QualifierAnnotationAutowireCandidateResolver 的 getSuggestedValue 方法，并传入 DependencyDescriptor 的实例来获取一个推荐的属性值。\n2. obtainFreshBeanFactory\n\n这一步获取（或创建） BeanFactory，它也是作为 ApplicationContext 的一个成员变量\nBeanFactory 的作用是负责 bean 的创建、依赖注入和初始化，bean 的各项特征由 BeanDefinition 定义\nBeanDefinition 作为 bean 的设计蓝图，规定了 bean 的特征，如单例多例、依赖关系、初始销毁方法等\nBeanDefinition 的来源有多种多样，可以是通过 xml 获得、配置类获得、组件扫描获得，也可以是编程添加\n\n\n所有的 BeanDefinition 会存入 BeanFactory 中的 beanDefinitionMap 集合\n\n\n// 演示各种 BeanDefinition 的来源\npublic class TestBeanDefinition &#123;\n    public static void main(String[] args) &#123;\n        System.out.println(\"========================> 一开始\");\n        DefaultListableBeanFactory beanFactory = new DefaultListableBeanFactory();\n        System.out.println(Arrays.toString(beanFactory.getBeanDefinitionNames()));\n\n        System.out.println(\"========================> 1) 从 xml 获取 \");\n        XmlBeanDefinitionReader reader1 = new XmlBeanDefinitionReader(beanFactory);\n        reader1.loadBeanDefinitions(new ClassPathResource(\"bd.xml\"));\n        System.out.println(Arrays.toString(beanFactory.getBeanDefinitionNames()));\n\n        System.out.println(\"========================> 2) 从配置类获取 \");\n        beanFactory.registerBeanDefinition(\"config1\", BeanDefinitionBuilder.genericBeanDefinition(Config1.class).getBeanDefinition());\n\n        ConfigurationClassPostProcessor postProcessor = new ConfigurationClassPostProcessor();\n        postProcessor.postProcessBeanDefinitionRegistry(beanFactory);\n        System.out.println(Arrays.toString(beanFactory.getBeanDefinitionNames()));\n\n        System.out.println(\"========================> 3) 扫描获取 \");\n        ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(beanFactory);\n        scanner.scan(\"day04.refresh.sub\");\n        System.out.println(Arrays.toString(beanFactory.getBeanDefinitionNames()));\n    &#125;\n\n    static class Bean1 &#123;\n\n    &#125;\n\n    static class Bean2 &#123;\n\n    &#125;\n\n    static class Config1 &#123;\n        @Bean\n        public Bean2 bean2() &#123;\n            return new Bean2();\n        &#125;\n    &#125;\n&#125;\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?>\n&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\">\n\n    &lt;bean id=\"bean1\" class=\"day04.refresh.TestBeanDefinition$Bean1\"/>\n\n&lt;/beans>\n\npackage day04.refresh.sub;\n\nimport org.springframework.stereotype.Component;\n\n@Component\npublic class Bean3 &#123;\n&#125;\n\n结果&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; 一开始\n[]\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; 1) 从 xml 获取 \n[bean1]\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; 2) 从配置类获取 \n[bean1, config1, bean2]\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; 3) 扫描获取 \n[bean1, config1, bean2, bean3, org.springframework.context.annotation.internalConfigurationAnnotationProcessor, org.springframework.context.annotation.internalAutowiredAnnotationProcessor, org.springframework.context.annotation.internalCommonAnnotationProcessor, org.springframework.context.event.internalEventListenerProcessor, org.springframework.context.event.internalEventListenerFactory]\n\n\n3. prepareBeanFactory\n\n这一步会进一步完善 BeanFactory，为它的各项成员变量赋值\nbeanExpressionResolver 用来解析 SpEL，常见实现为 StandardBeanExpressionResolver\npropertyEditorRegistrars 会注册类型转换器\n它在这里使用了 ResourceEditorRegistrar 实现类\n并应用 ApplicationContext 提供的 Environment 完成 ${ } 解析\n\n\nregisterResolvableDependency 来注册 beanFactory 以及 ApplicationContext，让它们也能用于依赖注入\nbeanPostProcessors 是 bean 后处理器集合，会工作在 bean 的生命周期各个阶段，此处会添加两个：\nApplicationContextAwareProcessor 用来解析 Aware 接口\nApplicationListenerDetector 用来识别容器中 ApplicationListener 类型的 bean\n\n\n\n\n4. postProcessBeanFactory\n\n这一步是空实现，留给子类扩展。\n一般 Web 环境的 App0licationContext 都要利用它注册新的 Scope，完善 Web 下的 BeanFactory\n\n\n这里体现的是模板方法设计模式\n\n5. invokeBeanFactoryPostProcessors\n\n这一步会调用 beanFactory 后处理器\nbeanFactory 后处理器，充当 beanFactory 的扩展点，可以用来补充或修改 BeanDefinition\n常见的 beanFactory 后处理器有\nConfigurationClassPostProcessor – 解析 @Configuration、@Bean、@Import、@PropertySource 等\nPropertySourcesPlaceHolderConfigurer – 替换 BeanDefinition 中的 ${ }\nMapperScannerConfigurer – 补充 Mapper 接口对应的 BeanDefinition\n\n\n\n\n6. registerBeanPostProcessors\n\n这一步是继续从 beanFactory 中找出 bean 后处理器，添加至 beanPostProcessors 集合中\nbean 后处理器，充当 bean 的扩展点，可以工作在 bean 的实例化、依赖注入、初始化阶段，常见的有：\nAutowiredAnnotationBeanPostProcessor 功能有：解析 @Autowired，@Value 注解\nCommonAnnotationBeanPostProcessor 功能有：解析 @Resource，@PostConstruct，@PreDestroy\nAnnotationAwareAspectJAutoProxyCreator 功能有：为符合切点的目标 bean 自动创建代理\n\n\n\n\n示例public class TestBeanPostProcessor &#123;\n\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        DefaultListableBeanFactory beanFactory = context.getDefaultListableBeanFactory();\n        beanFactory.registerBeanDefinition(\"bean1\", BeanDefinitionBuilder.genericBeanDefinition(Bean1.class).getBeanDefinition());\n        beanFactory.registerBeanDefinition(\"bean2\", BeanDefinitionBuilder.genericBeanDefinition(Bean2.class).getBeanDefinition());\n        beanFactory.registerBeanDefinition(\"bean3\", BeanDefinitionBuilder.genericBeanDefinition(Bean3.class).getBeanDefinition());\n        beanFactory.registerBeanDefinition(\"aspect1\", BeanDefinitionBuilder.genericBeanDefinition(Aspect1.class).getBeanDefinition());\n        beanFactory.registerBeanDefinition(\"processor1\",\n                BeanDefinitionBuilder.genericBeanDefinition(AutowiredAnnotationBeanPostProcessor.class).getBeanDefinition());\n        beanFactory.registerBeanDefinition(\"processor2\",\n                BeanDefinitionBuilder.genericBeanDefinition(CommonAnnotationBeanPostProcessor.class).getBeanDefinition());\n        beanFactory.registerBeanDefinition(\"processor3\",\n                BeanDefinitionBuilder.genericBeanDefinition(AnnotationAwareAspectJAutoProxyCreator.class).getBeanDefinition());\n\n        context.refresh();\n        beanFactory.getBean(Bean1.class).foo();\n    &#125;\n\n    static class Bean1 &#123;\n        Bean2 bean2;\n        Bean3 bean3;\n\n        @Autowired\n        public void setBean2(Bean2 bean2) &#123;\n            System.out.println(\"发生了依赖注入...\" + bean2);\n            this.bean2 = bean2;\n        &#125;\n\n        @Resource\n        public void setBean3(Bean3 bean3) &#123;\n            System.out.println(\"发生了依赖注入...\" + bean3);\n            this.bean3 = bean3;\n        &#125;\n\n        public void foo() &#123;\n            System.out.println(\"foo\");\n        &#125;\n    &#125;\n\n    static class Bean2 &#123;\n\n    &#125;\n\n    static class Bean3 &#123;\n\n    &#125;\n\n    @Aspect\n    static class Aspect1 &#123;\n        @Before(\"execution(* foo())\")\n        public void before() &#123;\n            System.out.println(\"before...\");\n        &#125;\n    &#125;\n&#125;\n\n结果发生了依赖注入...day04.refresh.TestBeanPostProcessor$Bean3@19b843ba\n发生了依赖注入...day04.refresh.TestBeanPostProcessor$Bean2@dc9876b\nbefore...\nfoo\n\n7. initMessageSource\n\n这一步是为 ApplicationContext 添加 messageSource 成员，实现国际化功能\n去 beanFactory 内找名为 messageSource 的 bean，如果没有，则提供空的 MessageSource 实现\n\n\n8. initApplicationContextEventMulticaster\n\n这一步为 ApplicationContext 添加事件广播器成员，即 applicationContextEventMulticaster\n它的作用是发布事件给监听器\n去 beanFactory 找名为 applicationEventMulticaster 的 bean 作为事件广播器，若没有，会创建默认的事件广播器\n之后就可以调用 ApplicationContext.publishEvent(事件对象) 来发布事件\n\n\n9. onRefresh\n\n这一步是空实现，留给子类扩展\nSpringBoot 中的子类在这里准备了 WebServer，即内嵌 web 容器\n\n\n体现的是模板方法设计模式\n\n10. registerListeners\n\n这一步会从多种途径找到事件监听器，并添加至 applicationEventMulticaster\n事件监听器顾名思义，用来接收事件广播器发布的事件，有如下来源\n事先编程添加的\n来自容器中的 bean\n来自于 @EventListener 的解析\n\n\n要实现事件监听器，只需要实现 ApplicationListener 接口，重写其中 onApplicationEvent(E e) 方法即可\n\n\n11. finishBeanFactoryInitialization\n\n这一步会将 beanFactory 的成员补充完毕，并初始化所有非延迟单例 bean\nconversionService 也是一套转换机制，作为对 PropertyEditor 的补充\nembeddedValueResolvers 即内嵌值解析器，用来解析 @Value 中的 ${ }，借用的是 Environment 的功能\nsingletonObjects 即单例池，缓存所有单例对象\n对象的创建都分三个阶段，每一阶段都有不同的 bean 后处理器参与进来，扩展功能\n\n\n\n\n12. finishRefresh\n\n这一步会为 ApplicationContext 添加 lifecycleProcessor 成员，用来控制容器内需要生命周期管理的 bean\n如果容器中有名称为 lifecycleProcessor 的 bean 就用它，否则创建默认的生命周期管理器\n准备好生命周期管理器，就可以实现\n调用 context 的 start，即可触发所有实现 LifeCycle 接口 bean 的 start\n调用 context 的 stop，即可触发所有实现 LifeCycle 接口 bean 的 stop\n\n\n发布 ContextRefreshed 事件，整个 refresh 执行完成\n\n\n","slug":"Spring-refresh流程","date":"2023-05-13T01:19:52.000Z","categories_index":"","tags_index":"Java,Spring,面试题","author_index":"大宝贝的程序员"},{"id":"2846594611fd32071ec8e1a72cd8f17d","title":"Spring如何解决循环依赖","content":"Spring如何解决循环依赖解决Set循环依赖注入Spring一级缓存singletonObjects\nsingletonObjects是一级缓存，用于存储单例Bean的实例对象。也就是说，当Spring容器创建一个单例Bean时，会将该Bean的实例对象放入一级缓存中，在后续使用该Bean时，直接从缓存中获取实例对象，避免了重复创建实例对象的过程。\n当没有循环依赖注入时，可以正常创建Bean\n\n存在问题：无法解决循环依赖\n首先调用A的getBean()到一级缓存看看A是否创建，如果返回为null表示没找到，就开始创建A，此时需要用到B进行依赖注入，又去一级缓存找有没有B，如果没有就开始创建B，此时B又需要用到A，又去一级缓存找A…..\n可见只有一级缓存是无法解决循环依赖注入的\n\n引入Spring的三级缓存singltonFactories\n解决循环依赖问题：\n先到一级缓存看看A是否创建，如果返回为null表示没找到，就开始创建A，此时创建的是一个半成品的A（工厂对象），把A放入三级缓存，此时需要用到B进行依赖注入，又去一级缓存找有没有B，如果没有就开始创建B，此时创建的也是一个半成品的B，此时B又需要用到A，又去一级缓存找A发现没有，又去三级缓存找，找到了A，并完成依赖注入，把B的成品放入singletonObjects，然后清除singletonFactories内的半成品B。这是A就可以拿到B，完成初始化。把A放到一级缓存，并清除三级缓存的A。\n\n问题又来了：Spring注入的对象大多是代理对象，那么能否完成注入呢\n先到一级缓存看看A是否创建，如果返回为null表示没找到，就开始创建A，此时创建的是一个半成品的A（工厂对象），把A放入三级缓存，此时需要用到B进行依赖注入，又去一级缓存找有没有B，如果没有就开始创建B，此时创建的也是一个半成品的B，此时B又需要用到A，又去一级缓存找A发现没有，又去三级缓存找，找到了A，并完成依赖注入（这时注入的并不是代理对象），初始化B并创建B的代理，把B的代理放入singletonObjects，然后清除singletonFactories内的半成品B。这是A就可以拿到B的代理，完成初始化。把A放到一级缓存，并清除三级缓存的A。可以看到，B注入的并不是代理，所有只有两个缓存是不可以解决Spring关于的代理对象的依赖注入。\n总的来说是因为存在循环依赖时，是依赖注入先发生，创建代理后发生\n\n引入Spring二级缓存earlySingletonObjects\n解决方法：提前创建代理\nSpring并不是对所有Bean的创建都提前创建代理，只有存在循环依赖时才提前创建代理\n\n解决Construct循环依赖注入A的构造依赖B，所有A无法创建工厂对象放入三级缓存，B的构造也依赖A，B也无法创建工厂对象放入缓存\n\nSpring的三级缓存无法解决构造器依赖注入。\n如何解决构造器的循环依赖呢？\n方法一\n可以给A注入一个B的代理对象（并不是真的B代理对象，而是创建了一个匿名内部类的TargetSource实现，当真正使用到B的方法时，会通过BeanFactory获取B，再调用），只要不妨碍A的创建以及初始化就行，当A成功创建之后，B也能成功创建，这时A想要用B的方法，只需要通过代理找到真正的B调用方法\n\n方法二\n可以给A注入一个工厂对象，只要不妨碍A的创建以及初始化就行，当A成功创建之后，B也能成功创建，这时A想要用B的方法，只需要通过工厂对象获取B，再调用方法\n\n以上两种都是通过延迟对象的创建来解析构造器循环依赖\n那么它们的延迟创建在Spring中如何体现呢？使用@Lazy\n在Spring中，可以在构造器循环依赖的其中一个对象的构造的参数上添加@Lazy来延迟对象的创建\n下面是一个在构造器循环依赖情况下使用 @Lazy 注解的示例代码：\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.context.annotation.AnnotationConfigUtils;\nimport org.springframework.context.annotation.Lazy;\nimport org.springframework.context.support.GenericApplicationContext;\n\nimport javax.annotation.PostConstruct;\n\npublic class TestConstructDelayCreate &#123;\n    static class A &#123;\n        private static final Logger log = LoggerFactory.getLogger(\"A\");\n        private B b;;\n        public A(@Lazy B b) &#123;\n            log.debug(\"A内的b>>>>&#123;&#125;\", b.getClass());\n            this.b = b;\n        &#125;\n        @PostConstruct\n        public void init() &#123;log.debug(\"init()\");&#125;\n    &#125;\n    static class B &#123;\n        private static final Logger log = LoggerFactory.getLogger(\"B\");\n        private A a;\n        public B(A a) &#123;\n            log.debug(\"B内的a>>>>&#123;&#125;\", a.getClass());\n            this.a = a;\n        &#125;\n        @PostConstruct\n        public void init() &#123;log.debug(\"init()\");&#125;\n    &#125;\n\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.registerBean(\"a\",A.class);\n        context.registerBean(\"b\",B.class);\n        AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory());\n        context.refresh();\n\n        System.out.println(context.getBean(A.class).b.getClass());\n        System.out.println(context.getBean(B.class).getClass());\n\n\n    &#125;\n&#125;\n\n运行结果:[DEBUG] 23:49:27.836 [main] - A内的b>>>>class day04.boot.TestConstructDelayCreate$B$$EnhancerBySpringCGLIB$$f09826e3 \n[DEBUG] 23:49:27.842 [main] - init() \n[DEBUG] 23:49:27.845 [main] - B内的a>>>>class day04.boot.TestConstructDelayCreate$A \n[DEBUG] 23:49:27.845 [main] - init() \nclass day04.boot.TestConstructDelayCreate$B$$EnhancerBySpringCGLIB$$f09826e3\nclass day04.boot.TestConstructDelayCreate$B\n\n跟踪@Lazy\nObject result = this.getAutowireCandidateResolver().getLazyResolutionProxyIfNecessary(descriptor, requestingBeanName);\n检查是否有@Lazy注解，是否需要创建代理\n\nreturn this.isLazy(descriptor) ? this.buildLazyResolutionProxy(descriptor, beanName) : null;\n是否需要创建代理\n\n看看创建代理的逻辑  buildLazyResolutionProxy\n创建了一个匿名内部类的TargetSource实现\n\n看看TargetSource的实现\n当调用getTarget()时，会通过BeanFactory获取B，再调用\n\n在上述代码中，可以给A注入一个B的代理对象，并不是真的B代理对象，而是创建了一个匿名内部类的TargetSource实现，内部关联了BeanFactory，当调用getTarget()时，会通过BeanFactory获取B，再调用。\n替换成ObjectFactory工厂对象\n下面是ObjectFactory示例代码：\npublic class TestConstructDelayCreate &#123;\n    static class A &#123;\n        private static final Logger log = LoggerFactory.getLogger(\"A\");\n        private ObjectFactory&lt;B> b;;\n        public A(ObjectFactory&lt;B> b) &#123;\n            log.debug(\"A内的b>>>>&#123;&#125;\", b.getClass());\n            this.b = b;\n        &#125;\n        @PostConstruct\n        public void init() &#123;log.debug(\"init()\");&#125;\n    &#125;\n    static class B &#123;\n        private static final Logger log = LoggerFactory.getLogger(\"B\");\n        private A a;\n        public B(A a) &#123;\n            log.debug(\"B内的a>>>>&#123;&#125;\", a.getClass());\n            this.a = a;\n        &#125;\n        @PostConstruct\n        public void init() &#123;log.debug(\"init()\");&#125;\n    &#125;\n\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.registerBean(\"a\",A.class);\n        context.registerBean(\"b\",B.class);\n        AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory());\n        context.refresh();\n\n        System.out.println(context.getBean(A.class).b.getObject());\n        System.out.println(context.getBean(B.class));\n    &#125;\n&#125;\n\n运行结果：\n[DEBUG] 16:33:27.224 [main] - A内的b>>>>class org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider \n[DEBUG] 16:33:27.230 [main] - init() \n[DEBUG] 16:33:27.233 [main] - B内的a>>>>class day04.boot.TestConstructDelayCreate$A \n[DEBUG] 16:33:27.234 [main] - init() \nday04.boot.TestConstructDelayCreate$B@7bd4937b\nday04.boot.TestConstructDelayCreate$B@7bd4937b\n\n优点：不会产生代理，减少内存开销\n替换成ObjectProvider对象（ObjectFactory的子类）\npublic class TestConstructDelayCreate &#123;\n    static class A &#123;\n        private static final Logger log &#x3D; LoggerFactory.getLogger(&quot;A&quot;);\n        private ObjectProvider&lt;B&gt; b;;\n        public A(ObjectProvider&lt;B&gt; b) &#123;\n            log.debug(&quot;A内的b&gt;&gt;&gt;&gt;&#123;&#125;&quot;, b.getClass());\n            this.b &#x3D; b;\n        &#125;\n        @PostConstruct\n        public void init() &#123;log.debug(&quot;init()&quot;);&#125;\n    &#125;\n    static class B &#123;\n        private static final Logger log &#x3D; LoggerFactory.getLogger(&quot;B&quot;);\n        private A a;\n        public B(A a) &#123;\n            log.debug(&quot;B内的a&gt;&gt;&gt;&gt;&#123;&#125;&quot;, a.getClass());\n            this.a &#x3D; a;\n        &#125;\n        @PostConstruct\n        public void init() &#123;log.debug(&quot;init()&quot;);&#125;\n    &#125;\n\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context &#x3D; new GenericApplicationContext();\n        context.registerBean(&quot;a&quot;,A.class);\n        context.registerBean(&quot;b&quot;,B.class);\n        AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory());\n        context.refresh();\n\n        System.out.println(context.getBean(A.class).b.getObject());\n        System.out.println(context.getBean(B.class));\n    &#125;\n&#125;\n\n运行结果\n[DEBUG] 16:36:43.644 [main] - A内的b>>>>class org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider \n[DEBUG] 16:36:43.649 [main] - init() \n[DEBUG] 16:36:43.652 [main] - B内的a>>>>class day04.boot.TestConstructDelayCreate$A \n[DEBUG] 16:36:43.653 [main] - init() \nday04.boot.TestConstructDelayCreate$B@21e360a\nday04.boot.TestConstructDelayCreate$B@21e360a\n\nObjectProvider、ObjectFactory都是Spring提供的工厂接口\nProvider：Java官方提供的一套工厂接口\n&lt;dependency>\n          &lt;groupId>javax.inject&lt;/groupId>\n          &lt;artifactId>javax.inject&lt;/artifactId>\n          &lt;version>1&lt;/version>\n      &lt;/dependency>\n\npublic class TestConstructDelayCreate &#123;\n    static class A &#123;\n        private static final Logger log = LoggerFactory.getLogger(\"A\");\n        private Provider&lt;B> b;;\n        public A(Provider&lt;B> b) &#123;\n            log.debug(\"A内的b>>>>&#123;&#125;\", b.getClass());\n            this.b = b;\n        &#125;\n        @PostConstruct\n        public void init() &#123;log.debug(\"init()\");&#125;\n    &#125;\n    static class B &#123;\n        private static final Logger log = LoggerFactory.getLogger(\"B\");\n        private A a;\n        public B(A a) &#123;\n            log.debug(\"B内的a>>>>&#123;&#125;\", a.getClass());\n            this.a = a;\n        &#125;\n        @PostConstruct\n        public void init() &#123;log.debug(\"init()\");&#125;\n    &#125;\n\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.registerBean(\"a\",A.class);\n        context.registerBean(\"b\",B.class);\n        AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory());\n        context.refresh();\n\n        System.out.println(context.getBean(A.class).b.get());\n        System.out.println(context.getBean(B.class));\n\n    &#125;\n&#125;\n\n运行结果：\n[DEBUG] 16:45:43.783 [main] - A内的b>>>>class org.springframework.beans.factory.support.DefaultListableBeanFactory$Jsr330Factory$Jsr330Provider \n[DEBUG] 16:45:43.788 [main] - init() \n[DEBUG] 16:45:43.792 [main] - B内的a>>>>class day04.boot.TestConstructDelayCreate$A \n[DEBUG] 16:45:43.792 [main] - init() \nday04.boot.TestConstructDelayCreate$B@43dac38f\nday04.boot.TestConstructDelayCreate$B@43dac38f\n\n分析：\npublic Object resolveDependency(DependencyDescriptor descriptor, @Nullable String requestingBeanName, @Nullable Set&lt;String> autowiredBeanNames, @Nullable TypeConverter typeConverter) throws BeansException &#123;\n       descriptor.initParameterNameDiscovery(this.getParameterNameDiscoverer());\n       if (Optional.class == descriptor.getDependencyType()) &#123;\n           return this.createOptionalDependency(descriptor, requestingBeanName);\n       &#125; else if (ObjectFactory.class != descriptor.getDependencyType() &amp;&amp; ObjectProvider.class != descriptor.getDependencyType()) &#123;\n           \n           //创建Provider\n           if (javaxInjectProviderClass == descriptor.getDependencyType()) &#123;\n               return (new DefaultListableBeanFactory.Jsr330Factory()).createDependencyProvider(descriptor, requestingBeanName);\n               \n               \n           &#125;\n\n//跟入 createDependencyProvider方法 是Jsr330Factory工厂的方法\nprivate class Jsr330Factory implements Serializable &#123;\n        private Jsr330Factory() &#123;\n        &#125;\n\n        public Object createDependencyProvider(DependencyDescriptor descriptor, @Nullable String beanName) &#123;\n            return new DefaultListableBeanFactory.Jsr330Factory.Jsr330Provider(descriptor, beanName);\n        &#125;\n\n        private class Jsr330Provider extends DefaultListableBeanFactory.DependencyObjectProvider implements Provider&lt;Object> &#123;\n            public Jsr330Provider(DependencyDescriptor descriptor, @Nullable String beanName) &#123;\n                super(descriptor, beanName);\n            &#125;\n\n            @Nullable\n            public Object get() throws BeansException &#123;\n                return this.getValue();\n            &#125;\n        &#125;\n    &#125;\n\n//看看get方法的getvalue()的实现   \n@Nullable\n        protected Object getValue() throws BeansException &#123;\n            return this.optional ? DefaultListableBeanFactory.this.createOptionalDependency(this.descriptor, this.beanName) : DefaultListableBeanFactory.this.doResolveDependency(this.descriptor, this.beanName, (Set)null, (TypeConverter)null);\n        &#125;\n\n当b.get()被调用时，会进入DefaultListableBeanFactory.this.doResolveDependency(this.descriptor,  this.beanName, (Set)null, (TypeConverter)null)；使用已经创建好的Bean\n使用Scope注解，也会创建代理解决构造器循环依赖 \n在类上添加@Scope(ProxyMode &#x3D; ScopedProxyMode.TARGET.CLASS)\n不推荐使用，因为会产生额外的beanDefination，也会产生额外的单例bean\n","slug":"Spring如何解决循环依赖","date":"2023-05-11T13:26:38.000Z","categories_index":"","tags_index":"Java,Spring,面试题","author_index":"大宝贝的程序员"},{"id":"a685b627e4865e5f328e43bd436e0d9e","title":"代理的创建时机","content":"代理的创建时机代理的创建时机\n创建 -&gt; ( * ) 依赖注入 -&gt; 初始化 ( * )\n\n初始化之后 (无循环依赖时)\n\n实例创建后, 依赖注入前 (有循环依赖时), 并暂存于二级缓存\n\n\npublic class A17_1 &#123;\n\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.registerBean(ConfigurationClassPostProcessor.class);\n        context.registerBean(Config.class);\n        context.refresh();\n        context.close();\n        // 创建 -> (*) 依赖注入 -> 初始化 (*)\n        /*\n            学到了什么\n                a. 代理的创建时机\n                    1. 初始化之后 (无循环依赖时)\n                    2. 实例创建后, 依赖注入前 (有循环依赖时), 并暂存于二级缓存\n                b. 依赖注入与初始化不应该被增强, 仍应被施加于原始对象\n         */\n    &#125;\n\n    @Configuration\n    static class Config &#123;\n        @Bean // 解析 @Aspect、产生代理\n        public AnnotationAwareAspectJAutoProxyCreator annotationAwareAspectJAutoProxyCreator() &#123;\n            return new AnnotationAwareAspectJAutoProxyCreator();\n        &#125;\n\n        @Bean // 解析 @Autowired\n        public AutowiredAnnotationBeanPostProcessor autowiredAnnotationBeanPostProcessor() &#123;\n            return new AutowiredAnnotationBeanPostProcessor();\n        &#125;\n\n        @Bean // 解析 @PostConstruct\n        public CommonAnnotationBeanPostProcessor commonAnnotationBeanPostProcessor() &#123;\n            return new CommonAnnotationBeanPostProcessor();\n        &#125;\n\n        @Bean\n        public Advisor advisor(MethodInterceptor advice) &#123;\n            AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut();\n            pointcut.setExpression(\"execution(* foo())\");\n            return new DefaultPointcutAdvisor(pointcut, advice);\n        &#125;\n\n        @Bean\n        public MethodInterceptor advice() &#123;\n            return (MethodInvocation invocation) -> &#123;\n                System.out.println(\"before...\");\n                return invocation.proceed();\n            &#125;;\n        &#125;\n\n        @Bean\n        public Bean1 bean1() &#123;\n            return new Bean1();\n        &#125;\n\n        @Bean\n        public Bean2 bean2() &#123;\n            return new Bean2();\n        &#125;\n    &#125;\n\n    static class Bean1 &#123;\n        public void foo() &#123;\n\n        &#125;\n        public Bean1() &#123;\n            System.out.println(\"Bean1()\");\n        &#125;\n        /*\n        @Autowired \n        public void setBean2(Bean2 bean2) &#123;\n            System.out.println(\"Bean1 setBean2(bean2) class is: \" + bean2.getClass());\n        &#125;*/\n        @PostConstruct \n        public void init() &#123;\n            System.out.println(\"Bean1 init()\");\n        &#125;\n    &#125;\n\n    static class Bean2 &#123;\n        public Bean2() &#123;\n            System.out.println(\"Bean2()\");\n        &#125;\n        @Autowired \n        public void setBean1(Bean1 bean1) &#123;\n            System.out.println(\"Bean2 setBean1(bean1) class is: \" + bean1.getClass());\n        &#125;\n        @PostConstruct \n        public void init() &#123;\n            System.out.println(\"Bean2 init()\");\n        &#125;\n    &#125;\n&#125;\n\n以上例子是一个单向的依赖，Bean2的创建依赖Bean1。代理的创建时机为类的初始化之后\n输出结果&gt;&gt;&gt;&gt;创建Bean1\nBean1()\n&gt;&gt;&gt;&gt;完成初始化\nBean1 init()\n&gt;&gt;&gt;&gt;为bean1创建代理\n[TRACE] 21:09:48.069 [main] o.s.a.a.a.AnnotationAwareAspectJAutoProxyCreator - Creating implicit proxy for bean &#39;bean1&#39; with 0 common interceptors and 2 specific interceptors \n&gt;&gt;&gt;&gt;创建Bean2\nBean2()\n&gt;&gt;&gt;&gt;完成Bean1属性的的依赖注入，注入的是代理\nBean2 setBean1(bean1) class is: class \norg.springframework.aop.framework.autoproxy.A17_1$Bean1$$EnhancerBySpringCGLIB$$2c2cbac1\n&gt;&gt;&gt;&gt;完成初始化\nBean2 init()\n\n修改Bean1的代码，让Bean1，Bean2存在循环依赖\nstatic class Bean1 &#123;\n        public void foo() &#123;\n\n        &#125;\n        public Bean1() &#123;\n            System.out.println(\"Bean1()\");\n        &#125;\n        @Autowired \n        public void setBean2(Bean2 bean2) &#123;\n            System.out.println(\"Bean1 setBean2(bean2) class is: \" + bean2.getClass());\n        &#125;\n        @PostConstruct \n        public void init() &#123;\n            System.out.println(\"Bean1 init()\");\n        &#125;\n    &#125;\n\n运行结果如下：>>>创建Bean1，Spring的放入三级缓存，发现需要依赖Bean2，查找缓存没有Bean2，就创建Bean2\nBean1()\n>>>创建Bean2，Spring的放入三级缓存,需要Bean1，查找缓存有Bean1\nBean2()\n>>>创建Bean1的代理\n[TRACE] 21:16:39.601 [main] o.s.a.a.a.AnnotationAwareAspectJAutoProxyCreator - Creating implicit proxy for bean 'bean1' with 0 common interceptors and 2 specific interceptors \n>>>>注入代理对象\nBean2 setBean1(bean1) class is: class org.springframework.aop.framework.autoproxy.A17_1$Bean1$$EnhancerBySpringCGLIB$$c459ff85\n>>>>完成初始化\nBean2 init()\n>>>>注入Bean2的代理\nBean1 setBean2(bean2) class is: class org.springframework.aop.framework.autoproxy.A17_1$Bean2\n>>>>完成初始化\nBean1 init()\n\n循环依赖时，代理的创建时机被提前到依赖注入之前\n依赖注入（set方法）与初始化（初始化方法）不应该被增强, 仍用原始对象的set和初始化方法\n\n\n","slug":"代理的创建时机","date":"2023-05-11T12:55:58.000Z","categories_index":"","tags_index":"Java,Spring,Proxy","author_index":"大宝贝的程序员"},{"id":"a95318a81b0b3097ed11bab255c7390f","title":"从@Aspect到Advisor","content":"@Aspect—&gt;&gt;AdvisorAnnotationAwareAspectJAutoProxyCreator\nAnnotationAwareAspectJAutoProxyCreator的作用是将高级切面转换成低级切面，使其能够被Spring框架所识别和使用。具体来说，它会读取应用中所有的@Aspect注解，并将这些注解解析成切面的定义。然后，它会对切面定义进行解析，并通过AspectJ编译器将切面转换成可执行的代码块和增强器，并将其绑定到目标对象的代理上。\nfindEligibleAdvisors \n从容器中获取所有的Advisor列表，然后通过匹配切点和目标对象，筛选出适用于该目标对象的Advisor列表。具体包括以下步骤：\n\n获取Spring容器中的所有Advisor。Spring容器中的Advisor代表着切面中定义的增强器，它们用于在目标对象的方法执行前后进行拦截并执行相应的增强逻辑。在这一步中，findEligibleAdvisors方法会从容器中获取所有Advisor对象列表。\n筛选使用AspectJ注解标注的Aspect对象。在这一步中，findEligibleAdvisors方法会遍历所有的Advisor对象，并检查是否是由使用AspectJ注解标注的Aspect对象创建的。如果是，则表示该Advisor属于切面定义的增强逻辑，需要参与目标对象的代理。\n\nwrapIfNecessary\n这个方法的作用是检查目标对象是否需要进行代理，如果需要，则创建代理对象。而目标对象是否需要进行代理，则取决于以下几个因素：\n\n是否启用代理。即是否使用&lt;aop:aspectj-autoproxy&gt;标签启用了自动代理的功能，或者在Java配置中使用@EnableAspectJAutoProxy注解启用了自动代理的功能。\n是否满足代理条件。在Spring框架中，只有当目标对象的类型是非final类或者实现了一个或多个接口时，才能够创建代理对象。否则，Spring框架无法通过动态代理实现对目标对象的增强。\n是否存在增强器。在这个步骤中，Spring框架会调用findAdvisorsThatCanApply方法，查找适合目标对象的增强器列表。如果找到了适合目标对象的增强器，则表示需要为该对象创建代理。\n\n如果目标对象需要创建代理，wrapIfNecessary方法就会根据目标对象的类型，采用不同的代理方式来创建代理对象。如果目标对象实现了一个或多个接口，则使用JDK动态代理的方式进行代理；否则，使用CGLIB代理的方式进行代理。\npackage org.springframework.aop.framework.autoproxy;\n\nimport org.aopalliance.intercept.MethodInterceptor;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Before;\nimport org.springframework.aop.Advisor;\n\nimport org.springframework.aop.aspectj.AspectJExpressionPointcut;\nimport org.springframework.aop.aspectj.annotation.AnnotationAwareAspectJAutoProxyCreator;\nimport org.springframework.aop.support.DefaultPointcutAdvisor;\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.ConfigurationClassPostProcessor;\nimport org.springframework.context.support.GenericApplicationContext;\nimport org.springframework.core.annotation.Order;\n\nimport java.util.List;\n\npublic class TestAspect &#123;\n    public static void main(String[] args) &#123;\n        GenericApplicationContext context = new GenericApplicationContext();\n        context.registerBean(\"aspect1\", Aspect1.class);\n        context.registerBean(\"config\", Config.class);\n        context.registerBean(ConfigurationClassPostProcessor.class);\n        context.registerBean(AnnotationAwareAspectJAutoProxyCreator.class);\n        context.refresh();\n//        for (String name : context.getBeanDefinitionNames()) &#123;\n//            System.out.println(name);\n//        &#125;\n\n        /*\n            第一个重要方法 findEligibleAdvisors 找到有【资格】的 Advisors\n                a. 有【资格】的 Advisor 一部分是低级的, 可以由自己编写, 如下例中的 advisor3\n                b. 有【资格】的 Advisor 另一部分是高级的, 解析 @Aspect 后获得\n         */\n        AnnotationAwareAspectJAutoProxyCreator creator = context.getBean(AnnotationAwareAspectJAutoProxyCreator.class);\n        List&lt;Advisor> advisors = creator.findEligibleAdvisors(Target1.class, \"target1\");\n        for (Advisor advisor : advisors) &#123;\n            System.out.println(advisor);\n        &#125;\n\n        /*\n            第二个重要方法 wrapIfNecessary\n                a. 它内部调用 findEligibleAdvisors, 只要返回集合不空, 则表示需要创建代理\n         */\n        Object o1 = creator.wrapIfNecessary(new Target1(), \"target1\", \"target1\");\n        System.out.println(o1.getClass());\n        Object o2 = creator.wrapIfNecessary(new Target2(), \"target2\", \"target2\");\n        System.out.println(o2.getClass());\n\n        ((Target1) o1).foo();\n        /*\n            学到了什么\n                a. 自动代理后处理器 AnnotationAwareAspectJAutoProxyCreator 会帮我们创建代理\n                b. 通常代理创建的活在原始对象初始化后执行, 但碰到循环依赖会提前至依赖注入之前执行\n                c. 高级的 @Aspect 切面会转换为低级的 Advisor 切面, 理解原理, 大道至简\n         */\n    &#125;\n\n    static class Target1 &#123;\n        public void foo() &#123;\n            System.out.println(\"target1 foo\");\n        &#125;\n    &#125;\n\n    static class Target2 &#123;\n        public void bar() &#123;\n            System.out.println(\"target2 bar\");\n        &#125;\n    &#125;\n\n    @Aspect // 高级切面类\n//    @Order(1)\n    static class Aspect1 &#123;\n        @Before(\"execution(* foo())\")\n        public void before1() &#123;\n            System.out.println(\"aspect1 before1...\");\n        &#125;\n\n        @Before(\"execution(* foo())\")\n        public void before2() &#123;\n            System.out.println(\"aspect1 before2...\");\n        &#125;\n    &#125;\n\n    @Configuration\n    static class Config &#123;\n        @Bean // 低级切面\n        public Advisor advisor3(MethodInterceptor advice3) &#123;\n            AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut();\n            pointcut.setExpression(\"execution(* foo())\");\n            DefaultPointcutAdvisor advisor = new DefaultPointcutAdvisor(pointcut, advice3);\n            return advisor;\n        &#125;\n        @Bean\n        public MethodInterceptor advice3() &#123;\n            return invocation -> &#123;\n                System.out.println(\"advice3 before...\");\n                Object result = invocation.proceed();\n                System.out.println(\"advice3 after...\");\n                return result;\n            &#125;;\n        &#125;\n    &#125;\n\n&#125;\n\n\n输出结果org.springframework.aop.interceptor.ExposeInvocationInterceptor.ADVISOR\n>>>>>>>>>>>>>>>>>>>>>>>>>>>低级切面\norg.springframework.aop.support.DefaultPointcutAdvisor: pointcut [AspectJExpressionPointcut: () execution(* foo())]; advice [org.springframework.aop.framework.autoproxy.A17$Config$$Lambda$117/0x0000000800d64950@d41f816]\n>>>>>>>>>>>>>>>>>>>>>>>>>>>被解析成低级切面的高级切面\nInstantiationModelAwarePointcutAdvisor: expression [execution(* foo())]; advice method [public void org.springframework.aop.framework.autoproxy.A17$Aspect1.before1()]; perClauseKind=SINGLETON\nInstantiationModelAwarePointcutAdvisor: expression [execution(* foo())]; advice method [public void org.springframework.aop.framework.autoproxy.A17$Aspect1.before2()]; perClauseKind=SINGLETON\n>>>>>>>>>>>>>>>>>>>>>>>>Target1匹配切面，会创建代理\nclass org.springframework.aop.framework.autoproxy.A17$Target1$$EnhancerBySpringCGLIB$$7efd35eb\n>>>>>>>>>>>>>>>>>>>>>>>>Target2不匹配切面，不会创建代理\nclass org.springframework.aop.framework.autoproxy.A17$Target2\nadvice3 before...\naspect1 before1...\naspect1 before2...\ntarget1 foo\nadvice3 after...\n\n  \n\n","slug":"从@Aspect到Advisor","date":"2023-05-11T12:04:08.000Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"d3cf2f31cfa6170a433ba0c99bf2326a","title":"切点匹配规则","content":"切点匹配规则AspectJ使用切点指示器（Pointcut Designator）来描述需要匹配的切点，切点指示器定义了切点的名称、参数、返回类型、标注等信息。切点指示器可以根据需要使用通配符、逻辑运算符、正则表达式等方式来描述切点。\nAspectJ中的切点指示符主要分为以下几种：\n\nexecution：匹配方法执行的连接点，以方法的访问修饰符、返回类型、类名、方法名和参数决定。\nwithin：匹配指定类型内的方法执行。\nthis：匹配当前AOP代理对象类型的执行方法。\ntarget：匹配当前目标对象类型的执行方法。\nargs：匹配当前执行的方法传入参数为指定类型的执行方法。\nannotation：匹配当前执行方法持有指定注解的方法执行。\nbean：匹配指定名称的bean中的方法执行。\ncflow：匹配满足条件的方法的任何调用。\nif：用于组合其他切点指示器。\n\n使用切点指示器可以非常方便的定义Spring中的切点表达式，如：@Pointcut(&quot;execution(* com.example.service.*.*(..))&quot;)\npublic void servicePointcut() &#123;&#125;\n\n@Pointcut(&quot;execution(* com.example.dao.*.*(..))&quot;)\npublic void daoPointcut() &#123;&#125;\n\n@Pointcut(&quot;within(com.example.*)&quot;)\npublic void withinPointcut() &#123;&#125;\n\n@Pointcut(&quot;@annotation(com.example.annotation.Loggable)&quot;)\npublic void annotationPointcut() &#123;&#125;\n\n上面的代码定义了四个切点表达式，分别对应于拦截service包、dao包、com.example包下的所有方法和所有标有@Loggable注解的方法的拦截。在实际使用时，可以将这些切点表达式和Advice组合成Advisor，通过使用AOP来实现对目标方法的拦截。\naspectj 切点的局限性\n实际的 @Transactional 切点实现，无法匹配实现了接口添加了@Transactional注解的类的方法\nimport org.springframework.aop.aspectj.AspectJExpressionPointcut;\nimport org.springframework.aop.support.StaticMethodMatcherPointcut;\nimport org.springframework.core.annotation.MergedAnnotations;\nimport org.springframework.transaction.annotation.Transactional;\n\nimport java.lang.reflect.Method;\n\npublic class A16 &#123;\n    public static void main(String[] args) throws NoSuchMethodException &#123;\n//        AspectJExpressionPointcut无法处理加载类上的信息\n//        检查是否匹配成功  execution\n        AspectJExpressionPointcut pt1 = new AspectJExpressionPointcut();\n        pt1.setExpression(\"execution(* bar())\");\n        System.out.println(pt1.matches(T1.class.getMethod(\"foo\"), T1.class));//false\n        System.out.println(pt1.matches(T1.class.getMethod(\"bar\"), T1.class));//true\n//        根据方法注解进行匹配  @annotation表达式\n        AspectJExpressionPointcut pt2 = new AspectJExpressionPointcut();\n        pt2.setExpression(\"@annotation(org.springframework.transaction.annotation.Transactional)\");\n        System.out.println(pt2.matches(T1.class.getMethod(\"foo\"), T1.class));//true\n        System.out.println(pt2.matches(T1.class.getMethod(\"bar\"), T3.class));//false\n       \n    &#125;\n    static class T1 &#123;\n        @Transactional\n        public void foo() &#123;\n        &#125;\n        public void bar() &#123;\n        &#125;\n    &#125;\n\n    @Transactional\n    static class T2 &#123;\n        public void foo() &#123;\n        &#125;\n    &#125;\n\n    @Transactional\n    interface I3 &#123;\n        void foo();\n    &#125;\n    static class T3 implements I3 &#123;\n        public void foo() &#123;\n        &#125;\n    &#125;\n&#125;\n\nSpring如何解决\nStaticMethodMatcherPointcut是Spring框架中用于匹配静态方法的切点对象。在AOP编程中，切点用于定义在哪些方法执行时会被拦截并执行增强逻辑。StaticMethodMatcherPointcut基于指定的规则匹配静态方法并确定在这些方法执行时是否要进行拦截。\nStaticMethodMatcherPointcut的匹配规则根据传入的Class和方法Method对象进行判断，只有当方法符合预定义的规则时，切点才会拦截方法执行，否则会被忽略。通常，可以通过实现matches方法来自定义匹配规则，只拦截符合条件的目标方法。\n//        StaticMethodMatcherPointcut抽象类可以处理类上的信息，实现match方法\n        StaticMethodMatcherPointcut pt3 = new StaticMethodMatcherPointcut() &#123;\n            @Override               //方法                    类\n            public boolean matches(Method method, Class&lt;?> targetClass) &#123;\n                // 读取方法信息，检查方法上是否加了 Transactional 注解\n                MergedAnnotations annotations = MergedAnnotations.from(method);\n                if (annotations.isPresent(Transactional.class)) &#123;\n                    return true;\n                &#125;\n    \t\t\t// 读取方法信息类信息，查看类上是否加了 Transactional 注解\n                //默认只查看本类删是否含有相应信息，修改成SearchStrategy.TYPE_HIERARCHY\n                annotations = MergedAnnotations.from(targetClass, MergedAnnotations.SearchStrategy.TYPE_HIERARCHY);\n                if (annotations.isPresent(Transactional.class)) &#123;\n                    return true;\n                &#125;\n                return false;\n            &#125;\n        &#125;;\n\n        System.out.println(pt3.matches(T1.class.getMethod(\"foo\"), T1.class));//true\n        System.out.println(pt3.matches(T1.class.getMethod(\"bar\"), T1.class));//false\n        System.out.println(pt3.matches(T2.class.getMethod(\"foo\"), T2.class));//true\n        System.out.println(pt3.matches(T3.class.getMethod(\"foo\"), T3.class));//true\n\n\n\n\n底层切点实现是如何匹配的: 调用了 aspectj 的匹配方法\n\n比较关键的是它实现了 MethodMatcher 接口, 用来执行方法的匹配\n\n\n","slug":"切点匹配规则","date":"2023-05-11T09:35:15.000Z","categories_index":"","tags_index":"Java,Spring","author_index":"大宝贝的程序员"},{"id":"42403d33a4a316854760bf766fee144b","title":"JDK和CGlib在Spring中的统一","content":"JDK和CGlib在Spring中的统一Spring 中对切点、通知、切面的抽象如下\n\n切点：接口 Pointcut，典型实现 AspectJExpressionPointcut\n通知：典型接口为 MethodInterceptor 代表环绕通知\n切面：Advisor，包含一个 Advice 通知，PointcutAdvisor 包含一个 Advice 通知和一个 Pointcut\n\nclassDiagram\n\nclass Advice\nclass MethodInterceptor\nclass Advisor\nclass PointcutAdvisor\n\nPointcut &lt;|-- AspectJExpressionPointcut\nAdvice &lt;|-- MethodInterceptor\nAdvisor &lt;|-- PointcutAdvisor\nPointcutAdvisor o-- \"一\" Pointcut\nPointcutAdvisor o-- \"一\" Advice\n\n&lt;&lt;interface>> Advice\n&lt;&lt;interface>> MethodInterceptor\n&lt;&lt;interface>> Pointcut\n&lt;&lt;interface>> Advisor\n&lt;&lt;interface>> PointcutAdvisor\n\n代理相关类图\n\nAopProxyFactory 根据 proxyTargetClass 等设置选择 AopProxy 实现\nAopProxy 通过 getProxy 创建代理对象\n图中 Proxy 都实现了 Advised 接口，能够获得关联的切面集合与目标（其实是从 ProxyFactory 取得）\n调用代理方法时，会借助 ProxyFactory 将通知统一转为环绕通知：MethodInterceptor\n\nclassDiagram\n\nAdvised &lt;|-- ProxyFactory\nProxyFactory o-- Target\nProxyFactory o-- \"多\" Advisor\n\nProxyFactory --> AopProxyFactory : 使用\nAopProxyFactory --> AopProxy\nAdvised &lt;|-- 基于CGLIB的Proxy\n基于CGLIB的Proxy &lt;-- ObjenesisCglibAopProxy : 创建\nAopProxy &lt;|-- ObjenesisCglibAopProxy\nAopProxy &lt;|-- JdkDynamicAopProxy\n基于JDK的Proxy &lt;-- JdkDynamicAopProxy : 创建\nAdvised &lt;|-- 基于JDK的Proxy\n\nclass AopProxy &#123;\n   +getProxy() Object\n&#125;\n\nclass ProxyFactory &#123;\n\tproxyTargetClass : boolean\n&#125;\n\nclass ObjenesisCglibAopProxy &#123;\n\tadvised : ProxyFactory\n&#125;\n\nclass JdkDynamicAopProxy &#123;\n\tadvised : ProxyFactory\n&#125;\n\n&lt;&lt;interface>> Advised\n&lt;&lt;interface>> AopProxyFactory\n&lt;&lt;interface>> AopProxy\n\n\n\n在 Spring 中，JDK动态代理和CGLIB动态代理会被自动地统一使用。Spring框架会自动选择合适的代理方式，以确保代理对象的正确性和高效性。\n如果我们希望代理的对象实现了接口，那么我们可以选择使用 JDK 动态代理。相比 CGLIB 动态代理，JDK 动态代理的代理对象更加轻量级，因为直接实现了目标对象的接口。\n如果我们希望代理对象没有实现接口，那么我们只能使用 CGLIB 动态代理。CGLIB 动态代理会在运行时生成代理对象的子类，并重写目标对象的方法。所以 CGLIB 动态代理的代理对象比 JDK 动态代理的代理对象更加强大和灵活，但同时也更加重量级。\n总之，在使用 Spring 框架中的 AOP 时，我们只需要关注代理对象是否实现了接口，Spring 会根据情况自动选择使用 JDK 动态代理还是 CGLIB 动态代理。如果我们希望选择特定的代理方式，可以通过在 Spring 配置文件中进行配置来实现。\n模拟Spring创建代理的使用jdk或者cglib&#x2F;&#x2F;要注意导包：MethodInterceptor是与 cglib的 MethodInterceptor 同名的\nimport org.aopalliance.intercept.MethodInterceptor;\nimport org.springframework.aop.aspectj.AspectJExpressionPointcut;\nimport org.springframework.aop.framework.ProxyFactory;\nimport org.springframework.aop.support.DefaultPointcutAdvisor;\n\npublic class TestSpringAop &#123;\n    public static void main(String[] args) &#123;\n        &#x2F;*\n            两个切面概念\n            aspect &#x3D;\n                通知1(advice) +  切点1(pointcut)\n                通知2(advice) +  切点2(pointcut)\n                通知3(advice) +  切点3(pointcut)\n                ...\n            advisor &#x3D; 更细粒度的切面，包含一个通知和切点\n         *&#x2F;\n\n        &#x2F;&#x2F; 1. 准备好切点\n        AspectJExpressionPointcut pointcut &#x3D; new AspectJExpressionPointcut();\n        pointcut.setExpression(&quot;execution(* foo())&quot;);\n        &#x2F;&#x2F; 2. 准备好通知  MethodInterceptor与cglib的MethodInterceptor不一样\n        &#x2F;&#x2F;本质上是环绕通知\n        MethodInterceptor advice &#x3D; new MethodInterceptor() &#123;\n            @Override\n            public Object invoke(MethodInvocation invocation) throws Throwable &#123;\n                System.out.println(&quot;before...&quot;);\n                Object result &#x3D; invocation.proceed(); &#x2F;&#x2F; 调用目标\n                System.out.println(&quot;after...&quot;);\n                return result;\n            &#125;\n        &#125;;\n        &#x2F;&#x2F; 3. 备好切面\n        DefaultPointcutAdvisor advisor &#x3D; new DefaultPointcutAdvisor(pointcut, advice);\n        &#x2F;&#x2F; 4. 创建代理\n        Target2 target &#x3D; new Target2();\n        ProxyFactory factory &#x3D; new ProxyFactory();\n        factory.setTarget(target);\n        factory.addAdvisor(advisor);\n        &#x2F;&#x2F;获取接口类型 factory无法判断是否实现了接口，需要手动设置\n        factory.setInterfaces(target.getClass().getInterfaces());\n        factory.setProxyTargetClass(false);\n        Target2 proxy &#x3D; (Target2) factory.getProxy();\n        &#x2F;&#x2F;cglib代理\n        System.out.println(proxy.getClass());\n        proxy.foo();\n        proxy.bar();\n        &#x2F;*\n            学到了什么\n                a. Spring 的代理选择规则\n                b. 底层的切点实现\n                c. 底层的通知实现\n                d. ProxyFactory 是用来创建代理的核心实现, 用 AopProxyFactory 选择具体代理实现\n                    - JdkDynamicAopProxy\n                    - ObjenesisCglibAopProxy\n         *&#x2F;\n    &#125;\n\n    interface I1 &#123;\n        void foo();\n\n        void bar();\n    &#125;\n\n    static class Target1 implements I1 &#123;\n        public void foo() &#123;\n            System.out.println(&quot;target1 foo&quot;);\n        &#125;\n\n        public void bar() &#123;\n            System.out.println(&quot;target1 bar&quot;);\n        &#125;\n    &#125;\n\n    static class Target2 &#123;\n        public void foo() &#123;\n            System.out.println(&quot;target2 foo&quot;);\n        &#125;\n\n        public void bar() &#123;\n            System.out.println(&quot;target2 bar&quot;);\n        &#125;\n    &#125;\n&#125;\n\nSpring 的代理选择规：\n\nproxyTargetClass &#x3D; false, 目标实现了接口, 用 jdk 实现\nproxyTargetClass &#x3D; false,  目标没有实现接口, 用 cglib 实现\nproxyTargetClass &#x3D; true, 总是使用 cglib 实现\n\n","slug":"JDK和CGlib在Spring中的统一","date":"2023-05-11T05:01:11.000Z","categories_index":"","tags_index":"Java,Spring,Proxy","author_index":"大宝贝的程序员"},{"id":"f73a8e23e6f6f669cf99c7dba8fa0722","title":"","content":"title: AOP实现之proxydate: 2023-05-10 16:38:34tags:\n\nAOP\nSpring\nJava\n\nAOP实现之proxyjdk动态代理\n动态代理是通过反射机制实现的，可以动态地生成代理类和代理对象，在运行时将需要增强的代码织入到目标对象的方法中。\n实现动态代理，需要用到以下类和接口：\n\njava.lang.reflect.Proxy：提供了用于创建动态代理的方法newProxyInstance()。\njava.lang.reflect.InvocationHandler：定义了代理对象的调用处理器，负责实现代理对象调用的逻辑以及需要增强的代码。\n\n代理对象调用的流程如下：\n\n当代理对象的方法被调用时，会被转发到InvocationHandler的invoke()方法。\n在invoke()方法中，根据调用的方法名和参数，判断需要执行什么样的业务逻辑。\n如果需要增强方法，将增强逻辑插入到调用方法前或后执行，否则直接调用目标对象的方法。\n\n优点\n使用jdk动态代理的好处是可以避免手动编写代理类，提高代码的复用度。同时，由于jdk动态代理是运行时动态生成代理对象，因此不需要针对每个被代理的类手动创建代理类，提高了开发效率。\n缺点\n\n代理对象必须实现接口：由于jdk动态代理是基于接口进行代理的，因此只能够为接口类型的类创建代理对象。如果需要对非接口类型的类进行代理，可以使用CGLib库。\n\n补充说明\nJDK 只能针对接口代理，代理对象和目标对象之间是平级兄弟关系，也就是说代理对象并不是目标对象的子类，而是实现了相同接口的新类型。因此，在理论上，代理对象和目标对象的类型可以是相同的，甚至目标对象可以被 final 修饰。\n示例import java.lang.reflect.InvocationHandler;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\n\ninterface Hello &#123;\n    void sayHello();\n&#125;\n\nclass HelloImpl implements Hello &#123;\n    public void sayHello() &#123;\n        System.out.println(\"Hello world!\");\n    &#125;\n&#125;\n\nclass MyInvocationHandler implements InvocationHandler &#123;\n    private Object target;// 目标对象\n\n    public MyInvocationHandler(Object target) &#123;\n        this.target = target;\n    &#125;\n\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;\n        System.out.println(\"Before Method Invoke\");\n        // 方法.invoke(目标, 参数);\n        Object result = method.invoke(target, args);\n        System.out.println(\"After Method Invoke\");\n        return result;\n    &#125;\n&#125;\n\npublic class ProxyTest &#123;\n    public static void main(String[] args) &#123;\n        HelloImpl hello = new HelloImpl();\n        MyInvocationHandler handler = new MyInvocationHandler(hello);\n        //参数（目标类的类加载器用于运行时动态生成字节码，接口类型，InvocationHandler的具体实现）\n        Hello proxyHello = Proxy.newProxyInstance(hello.getClass().getClassLoader(),\n                hello.getClass().getInterfaces(), handler);\n        proxyHello.sayHello();\n    &#125;\n&#125;\n\n\n\nCGlib代理CGlib是一个开源的Java字节码增强库，可以在运行时动态地生成一个目标类的子类，通过这个子类来实现对目标类的代理（代理的具体实现方式视情况而定）。与JDK动态代理相比，CGlib代理无需目标对象实现接口，能够代理目标对象的方法，包括private、protected修饰的方法。\nCGlib代理的实现中，需要使用到ASM库来生成代理类的字节码。具体而言，ASM是一个轻量级Java字节码操作和生成库，能够在运行时动态生成类的字节码，以达到动态修改类的目的。\n使用CGlib代理时，我们一般需要实现一个MethodInterceptor接口，用来对目标类的方法进行拦截和增强。\nCGLib实现代理的原理可以分为以下几个步骤：\n\n定义一个类，继承被代理类；\n在代理类中定义一个变量，用于持有被代理类的引用；\n在代理类中重写被代理类中的所有需要代理的方法；\n在重写的方法中加入额外的代理逻辑（例如记录方法调用的日志、进行权限验证等）。\n\nCGLib实现代理的步骤：\n\n设置CGLib的Enhancer对象的SuperClass和Callback属性，这样Enhancer就知道要代理哪个类以及代理的具体实现方式；\n使用Enhancer对象的create()方法来生成代理对象。\n\n通过实例了解一下cglib的工作大概流程\n示例1import net.sf.cglib.proxy.Enhancer;\nimport net.sf.cglib.proxy.MethodInterceptor;\nimport net.sf.cglib.proxy.MethodProxy;\n\nimport java.lang.reflect.Method;\n\npublic class CGLibProxyDemo &#123;\n\n    public static void main(String[] args) &#123;\n        RealSubject realSubject = new RealSubject();\n        RealSubject proxy = (RealSubject) Enhancer.create(RealSubject.class, new MyMethodInterceptor(realSubject));\n        proxy.request();\n    &#125;\n\n    static class RealSubject &#123;\n        public void request() &#123;\n            System.out.println(\"RealSubject.request()\");\n        &#125;\n    &#125;\n\t//实现一个MethodInterceptor接口\n    static class MyMethodInterceptor implements MethodInterceptor &#123;\n        private Object target;//目标\n\n        public MyMethodInterceptor(Object target) &#123;\n            this.target = target;\n        &#125;\n\n        @Override\n        public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123;\n            System.out.println(\"Before method: \" + method);\n            Object result = method.invoke(target, objects);\n            System.out.println(\"After method: \" + method);\n            return result;\n        &#125;\n    &#125;\n&#125;\n\n在这个例子中，MyMethodInterceptor类是实现了CGLib的MethodInterceptor接口，它的作用是在代理类的方法执行时进行拦截和增强。\n通过调用Enhancer.create()方法，我们得到了一个代理对象，它拥有RealSubject类的全部方法，但是在执行request方法时会经过CGLib生成的MyMethodInterceptor拦截器，我们可以在这个拦截器中进行自己的逻辑处理。\n示例2import org.springframework.cglib.proxy.Enhancer;\nimport org.springframework.cglib.proxy.MethodInterceptor;\nimport org.springframework.cglib.proxy.MethodProxy;\n\nimport java.lang.reflect.Method;\nimport java.util.Arrays;\n\npublic class CglibProxyDemo &#123;\n\n    static class Target &#123;\n        public void foo() &#123;\n            System.out.println(\"target foo\");\n        &#125;\n    &#125;\n\n    // 代理是子类型, 目标是父类型\n    public static void main(String[] param) &#123;\n//        Target target = new Target();\n\n        Target proxy = (Target) Enhancer.create(Target.class, new MethodInterceptor() &#123;\n            @Override\n            public Object intercept(Object p, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123;\n                System.out.println(\"before...\");\n//  Object result = method.invoke(target, args); // 用方法反射调用目标\n// methodProxy 它可以避免反射调用\n//  Object result = methodProxy.invoke(target, args); // 内部没有用反射, 需要目标 （spring）\n    Object result = methodProxy.invokeSuper(p, args); // 内部没有用反射, 需要代理\n                System.out.println(\"after...\");\n                return result;\n            &#125;\n        &#125;);\n\n        proxy.foo();\n\n    &#125;\n&#125;\n\n有三种调用方式：method.invoke(target, args)  用反射调用目标，性能较低，需要目标对象\n​\t\t\t\t\t\t\t\tmethodProxy.invoke(target, args) 内部没有使用反射，需要目标对象  spring使用的方式\n​\t\t\t\t\t\t\t\tmethodProxy.invokeSuper(p, args) 内部没有使用反射，不需要目标对象\n代理是子类型, 目标是父类型：\n\n当目标使用final修饰，无法代理会报错\n\n当代理方法使用final修饰，不会把错，方法无法得到增强\n\n\n模拟jdk动态代理源码示例public class Test &#123;\n    public static void main(String[] param) &#123;\n        // ⬇️1. 创建代理，这时传入 InvocationHandler\n        Foo proxy = new $Proxy0(new InvocationHandler() &#123;    \n            // ⬇️5. 进入 InvocationHandler\n            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable&#123;\n                // ⬇️6. 功能增强\n                System.out.println(\"before...\");\n                // ⬇️7. 反射调用目标方法\n                return method.invoke(new Target(), args);\n            &#125;\n        &#125;);\n        // ⬇️2. 调用代理方法\n        proxy.foo();\n        proxy.bar();\n    &#125;\n&#125;\n\npublic interface Foo &#123;\n        void foo();\n        int bar();\n    &#125;\n\n\nstatic class Target implements Foo &#123;\n        public void foo() &#123;\n            System.out.println(\"target foo\");\n        &#125;\n\n        public int bar() &#123;\n            System.out.println(\"target bar\");\n            return 100;\n        &#125;\n    &#125;\n\n\n// ⬇️这就是 jdk 代理类的源码, 秘密都在里面\npublic class $Proxy0 extends Proxy implements Foo &#123;\n\n    public $Proxy0(InvocationHandler h) &#123;\n        super(h);\n    &#125;\n    // ⬇️3. 进入代理方法\n    public void foo() &#123;\n        try &#123;\n            // ⬇️4. 回调 InvocationHandler\n            h.invoke(this, foo, new Object[0]);\n        &#125; catch (RuntimeException | Error e) &#123;\n            throw e;\n        &#125; catch (Throwable e) &#123;\n            throw new UndeclaredThrowableException(e);\n        &#125;\n    &#125;\n\n    @Override\n    public int bar() &#123;\n        //用try-catch处理，把catch到的异常抛出，让外界知道代理有没有执行错误\n        try &#123;\n            Object result = h.invoke(this, bar, new Object[0]);\n            return (int) result;\n        &#125; catch (RuntimeException | Error e) &#123;//运行异常直接抛出\n            throw e;\n        &#125; catch (Throwable e) &#123;\n            throw new UndeclaredThrowableException(e);//检查异常要转换成运行异常再抛出\n        &#125;\n    &#125;\n\n    static Method foo;\n    static Method bar;\n    static &#123;\n        try &#123;\n            foo = A12.Foo.class.getMethod(\"foo\");\n            bar = A12.Foo.class.getMethod(\"bar\");\n        &#125; catch (NoSuchMethodException e) &#123;\n            throw new NoSuchMethodError(e.getMessage());\n        &#125;\n    &#125;\n&#125;\n\n以上是模拟jdk动态代理对象的源码，通过接口回调将【增强逻辑】置于代理类之外\n但是在真实的场景中，代理对象是看不到的，是程序在运行期间通过asm技术动态生成代理对象的ASMfiled\n\n把ASMfiled导出成一个类（可以通过流读取字节数组，生成代理对象，查看里面的内容）\n\n在Java代理中，ASM框架通常是作为动态代理机制的底层实现，用来生成字节码并创建代理类。具体来说，使用ASM框架创建代理类的过程大致如下：\n\n定义一个ClassWriter对象作为ASM框架生成字节码的输出流；\n通过ClassWriter定义类名和父类名称等相关信息，创建类的定义；\n定义类的字段、构造函数和代理方法等元素，这些元素将会被编码成字节码；\n利用ASM的MethodVisitor类访问器生成方法的字节码实现。MethodVisitor是定义在ASM框架中的一个访问类，用于可以随时提供关于方法的信息；\n生成字节码并将其写入输出流；\n\n示例转载自黑马package com.itheima;\n\nimport org.springframework.asm.*;\n\npublic class $Proxy0Dump implements Opcodes &#123;\n\n    public static byte[] dump() throws Exception &#123;\n\n        ClassWriter cw = new ClassWriter(0);\n        FieldVisitor fv;\n        MethodVisitor mv;\n        AnnotationVisitor av0;\n\n        cw.visit(52, ACC_PUBLIC + ACC_SUPER, \"com/itheima/$Proxy0\", null, \"java/lang/reflect/Proxy\", new String[]&#123;\"com/itheima/Foo\"&#125;);\n\n        cw.visitSource(\"$Proxy0.java\", null);\n\n        &#123;\n            fv = cw.visitField(ACC_STATIC, \"foo\", \"Ljava/lang/reflect/Method;\", null, null);\n            fv.visitEnd();\n        &#125;\n        &#123;\n            mv = cw.visitMethod(ACC_PUBLIC, \"&lt;init>\", \"(Ljava/lang/reflect/InvocationHandler;)V\", null, null);\n            mv.visitCode();\n            Label l0 = new Label();\n            mv.visitLabel(l0);\n            mv.visitLineNumber(11, l0);\n            mv.visitVarInsn(ALOAD, 0);\n            mv.visitVarInsn(ALOAD, 1);\n            mv.visitMethodInsn(INVOKESPECIAL, \"java/lang/reflect/Proxy\", \"&lt;init>\", \"(Ljava/lang/reflect/InvocationHandler;)V\", false);\n            Label l1 = new Label();\n            mv.visitLabel(l1);\n            mv.visitLineNumber(12, l1);\n            mv.visitInsn(RETURN);\n            Label l2 = new Label();\n            mv.visitLabel(l2);\n            mv.visitLocalVariable(\"this\", \"Lcom/itheima/$Proxy0;\", null, l0, l2, 0);\n            mv.visitLocalVariable(\"h\", \"Ljava/lang/reflect/InvocationHandler;\", null, l0, l2, 1);\n            mv.visitMaxs(2, 2);\n            mv.visitEnd();\n        &#125;\n        &#123;\n            mv = cw.visitMethod(ACC_PUBLIC, \"foo\", \"()V\", null, null);\n            mv.visitCode();\n            Label l0 = new Label();\n            Label l1 = new Label();\n            Label l2 = new Label();\n            mv.visitTryCatchBlock(l0, l1, l2, \"java/lang/Throwable\");\n            mv.visitLabel(l0);\n            mv.visitLineNumber(17, l0);\n            mv.visitVarInsn(ALOAD, 0);\n            mv.visitFieldInsn(GETFIELD, \"com/itheima/$Proxy0\", \"h\", \"Ljava/lang/reflect/InvocationHandler;\");\n            mv.visitVarInsn(ALOAD, 0);\n            mv.visitFieldInsn(GETSTATIC, \"com/itheima/$Proxy0\", \"foo\", \"Ljava/lang/reflect/Method;\");\n            mv.visitInsn(ACONST_NULL);\n            mv.visitMethodInsn(INVOKEINTERFACE, \"java/lang/reflect/InvocationHandler\", \"invoke\", \"(Ljava/lang/Object;Ljava/lang/reflect/Method;[Ljava/lang/Object;)Ljava/lang/Object;\", true);\n            mv.visitInsn(POP);\n            mv.visitLabel(l1);\n            mv.visitLineNumber(20, l1);\n            Label l3 = new Label();\n            mv.visitJumpInsn(GOTO, l3);\n            mv.visitLabel(l2);\n            mv.visitLineNumber(18, l2);\n            mv.visitFrame(Opcodes.F_SAME1, 0, null, 1, new Object[]&#123;\"java/lang/Throwable\"&#125;);\n            mv.visitVarInsn(ASTORE, 1);\n            Label l4 = new Label();\n            mv.visitLabel(l4);\n            mv.visitLineNumber(19, l4);\n            mv.visitTypeInsn(NEW, \"java/lang/reflect/UndeclaredThrowableException\");\n            mv.visitInsn(DUP);\n            mv.visitVarInsn(ALOAD, 1);\n            mv.visitMethodInsn(INVOKESPECIAL, \"java/lang/reflect/UndeclaredThrowableException\", \"&lt;init>\", \"(Ljava/lang/Throwable;)V\", false);\n            mv.visitInsn(ATHROW);\n            mv.visitLabel(l3);\n            mv.visitLineNumber(21, l3);\n            mv.visitFrame(Opcodes.F_SAME, 0, null, 0, null);\n            mv.visitInsn(RETURN);\n            Label l5 = new Label();\n            mv.visitLabel(l5);\n            mv.visitLocalVariable(\"e\", \"Ljava/lang/Throwable;\", null, l4, l3, 1);\n            mv.visitLocalVariable(\"this\", \"Lcom/itheima/$Proxy0;\", null, l0, l5, 0);\n            mv.visitMaxs(4, 2);\n            mv.visitEnd();\n        &#125;\n        &#123;\n            mv = cw.visitMethod(ACC_STATIC, \"&lt;clinit>\", \"()V\", null, null);\n            mv.visitCode();\n            Label l0 = new Label();\n            Label l1 = new Label();\n            Label l2 = new Label();\n            mv.visitTryCatchBlock(l0, l1, l2, \"java/lang/NoSuchMethodException\");\n            mv.visitLabel(l0);\n            mv.visitLineNumber(26, l0);\n            mv.visitLdcInsn(Type.getType(\"Lcom/itheima/Foo;\"));\n            mv.visitLdcInsn(\"foo\");\n            mv.visitInsn(ICONST_0);\n            mv.visitTypeInsn(ANEWARRAY, \"java/lang/Class\");\n            mv.visitMethodInsn(INVOKEVIRTUAL, \"java/lang/Class\", \"getMethod\", \"(Ljava/lang/String;[Ljava/lang/Class;)Ljava/lang/reflect/Method;\", false);\n            mv.visitFieldInsn(PUTSTATIC, \"com/itheima/$Proxy0\", \"foo\", \"Ljava/lang/reflect/Method;\");\n            mv.visitLabel(l1);\n            mv.visitLineNumber(29, l1);\n            Label l3 = new Label();\n            mv.visitJumpInsn(GOTO, l3);\n            mv.visitLabel(l2);\n            mv.visitLineNumber(27, l2);\n            mv.visitFrame(Opcodes.F_SAME1, 0, null, 1, new Object[]&#123;\"java/lang/NoSuchMethodException\"&#125;);\n            mv.visitVarInsn(ASTORE, 0);\n            Label l4 = new Label();\n            mv.visitLabel(l4);\n            mv.visitLineNumber(28, l4);\n            mv.visitTypeInsn(NEW, \"java/lang/NoSuchMethodError\");\n            mv.visitInsn(DUP);\n            mv.visitVarInsn(ALOAD, 0);\n            mv.visitMethodInsn(INVOKEVIRTUAL, \"java/lang/NoSuchMethodException\", \"getMessage\", \"()Ljava/lang/String;\", false);\n            mv.visitMethodInsn(INVOKESPECIAL, \"java/lang/NoSuchMethodError\", \"&lt;init>\", \"(Ljava/lang/String;)V\", false);\n            mv.visitInsn(ATHROW);\n            mv.visitLabel(l3);\n            mv.visitLineNumber(30, l3);\n            mv.visitFrame(Opcodes.F_SAME, 0, null, 0, null);\n            mv.visitInsn(RETURN);\n            mv.visitLocalVariable(\"e\", \"Ljava/lang/NoSuchMethodException;\", null, l4, l3, 0);\n            mv.visitMaxs(3, 1);\n            mv.visitEnd();\n        &#125;\n        cw.visitEnd();\n\n        return cw.toByteArray();\n    &#125;\n&#125;\n\n通过调用 代理类的$Proxy0Dump.dump()获取字节数组，用来的加载器加载字节数组\npublic class TestProxy &#123;\n    public static void main(String[] args) throws Exception &#123;\n        byte[] dump = $Proxy0Dump.dump();\n\n        /*FileOutputStream os = new FileOutputStream(\"$Proxy0.class\");\n        os.write(dump, 0, dump.length);\n        os.close();*/\n\n        ClassLoader loader = new ClassLoader() &#123;\n            @Override\n            protected Class&lt;?> findClass(String name) throws ClassNotFoundException &#123;\n                return super.defineClass(name, dump, 0, dump.length);\n            &#125;\n        &#125;;\n        Class&lt;?> proxyClass = loader.loadClass(\"com.itheima.$Proxy0\");\n\n        Constructor&lt;?> constructor = proxyClass.getConstructor(InvocationHandler.class);\n        Foo proxy = (Foo) constructor.newInstance(new InvocationHandler() &#123;\n            @Override\n            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;\n                System.out.println(\"before...\");\n                System.out.println(\"调用目标\");\n                return null;\n            &#125;\n        &#125;);\n\n        proxy.foo();\n    &#125;\n&#125;\n\n\njdk反射方法的优化import java.lang.reflect.Field;\nimport java.lang.reflect.Method;\n\n// 运行时请添加 --add-opens java.base/java.lang.reflect=ALL-UNNAMED --add-opens java.base/jdk.internal.reflect=ALL-UNNAMED\npublic class TestMethodInvoke &#123;\n    public static void main(String[] args) throws Exception &#123;\n        Method foo = TestMethodInvoke.class.getMethod(\"foo\", int.class);\n        for (int i = 1; i &lt;= 17; i++) &#123;\n            show(i, foo);\n            foo.invoke(null, i);\n        &#125;\n        System.in.read();\n    &#125;\n\n    // 方法反射调用时, 底层 MethodAccessor 的实现类\n    private static void show(int i, Method foo) throws Exception &#123;\n        Method getMethodAccessor = Method.class.getDeclaredMethod(\"getMethodAccessor\");\n        getMethodAccessor.setAccessible(true);\n        Object invoke = getMethodAccessor.invoke(foo);\n        if (invoke == null) &#123;\n            System.out.println(i + \":\" + null);\n            return;\n        &#125;\n        Field delegate = Class.forName(\"jdk.internal.reflect.DelegatingMethodAccessorImpl\").getDeclaredField(\"delegate\");\n        delegate.setAccessible(true);\n        System.out.println(i + \":\" + delegate.get(invoke));\n    &#125;\n\n    public static void foo(int i) &#123;\n        System.out.println(i + \":\" + \"foo\");\n    &#125;\n&#125;\n\n\n\n前 16 次反射性能较低，是基于Java的MethodAccessor调用的\n\n第 17 次调用会生成代理类，优化为非反射调用\n\n\n模拟 CGlib代理和 jdk 动态代理原理查不多\n\n回调的接口换了一下，InvocationHandler 改成了 MethodInterceptor\n\n调用目标时有所改进，见下面代码片段\n\nmethod.invoke 是反射调用，必须调用到足够次数才会进行优化\n\nmethodProxy.invoke 是不反射调用，它会正常（间接）调用目标对象的方法（Spring 采用）\n\nmethodProxy.invokeSuper 也是不反射调用，它会正常（间接）调用代理对象的方法，可以省略目标对象\n\n\n\n\n//代理对象\npublic class Proxy extends Target &#123;\n    private MethodInterceptor methodInterceptor;\n\n    public void setMethodInterceptor(MethodInterceptor methodInterceptor) &#123;\n        this.methodInterceptor = methodInterceptor;\n    &#125;\n\n    static Method save0;\n    static Method save1;\n    static Method save2;\n    static MethodProxy save0Proxy;\n    static MethodProxy save1Proxy;\n    static MethodProxy save2Proxy;\n    static &#123;\n        try &#123;\n            save0 = Target.class.getMethod(\"save\");\n            save1 = Target.class.getMethod(\"save\", int.class);\n            save2 = Target.class.getMethod(\"save\", long.class);\n            save0Proxy = MethodProxy.create(Target.class, Proxy.class, \"()V\", \"save\", \"saveSuper\");\n            save1Proxy = MethodProxy.create(Target.class, Proxy.class, \"(I)V\", \"save\", \"saveSuper\");\n            save2Proxy = MethodProxy.create(Target.class, Proxy.class, \"(J)V\", \"save\", \"saveSuper\");\n        &#125; catch (NoSuchMethodException e) &#123;\n            throw new NoSuchMethodError(e.getMessage());\n        &#125;\n    &#125;\n\n    // >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 带原始功能的方法\n    public void saveSuper() &#123;\n        super.save();\n    &#125;\n    public void saveSuper(int i) &#123;\n        super.save(i);\n    &#125;\n    public void saveSuper(long j) &#123;\n        super.save(j);\n    &#125;\n    // >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 带增强功能的方法\n    @Override\n    public void save() &#123;\n        try &#123;\n            methodInterceptor.intercept(this, save0, new Object[0], save0Proxy);\n        &#125; catch (Throwable e) &#123;\n            throw new UndeclaredThrowableException(e);\n        &#125;\n    &#125;\n\n    @Override\n    public void save(int i) &#123;\n        try &#123;\n            methodInterceptor.intercept(this, save1, new Object[]&#123;i&#125;, save1Proxy);\n        &#125; catch (Throwable e) &#123;\n            throw new UndeclaredThrowableException(e);\n        &#125;\n    &#125;\n\n    @Override\n    public void save(long j) &#123;\n        try &#123;\n            methodInterceptor.intercept(this, save2, new Object[]&#123;j&#125;, save2Proxy);\n        &#125; catch (Throwable e) &#123;\n            throw new UndeclaredThrowableException(e);\n        &#125;\n    &#125;\n&#125;\n\n//目标对象\npublic class Target &#123;\n    public void save() &#123;\n        System.out.println(\"save()\");\n    &#125;\n\n    public void save(int i) &#123;\n        System.out.println(\"save(int)\");\n    &#125;\n\n    public void save(long j) &#123;\n        System.out.println(\"save(long)\");\n    &#125;\n&#125;\n\npublic class A13 &#123;\n\n    public static void main(String[] args) &#123;\n        Proxy proxy = new Proxy();\n        Target target = new Target();\n        proxy.setMethodInterceptor(new MethodInterceptor() &#123;\n            @Override\n            public Object intercept(Object p, Method method, Object[] args,\n                                    MethodProxy methodProxy) throws Throwable &#123;\n                System.out.println(\"before...\");\n//                return method.invoke(target, args); // 反射调用\n                // FastClass\n//                return methodProxy.invoke(target, args); // 内部无反射, 结合目标用\n                return methodProxy.invokeSuper(p, args); // 内部无反射, 结合代理用\n            &#125;\n        &#125;);\n\n        proxy.save();\n        proxy.save(1);\n        proxy.save(2L);\n    &#125;\n&#125;\n\nMethodProxy 的 invoke 或 invokeSuper 方法时如何避免反射的调用\nimport org.springframework.cglib.core.Signature;\n\npublic class ProxyFastClass &#123;\n    static Signature s0 = new Signature(\"saveSuper\", \"()V\");\n    static Signature s1 = new Signature(\"saveSuper\", \"(I)V\");\n    static Signature s2 = new Signature(\"saveSuper\", \"(J)V\");\n\n    // 获取代理方法的编号\n    /*\n        Proxy\n            saveSuper()              0\n            saveSuper(int)           1\n            saveSuper(long)          2\n        signature 包括方法名字、参数返回值\n     */\n    public int getIndex(Signature signature) &#123;\n        if (s0.equals(signature)) &#123;\n            return 0;\n        &#125; else if (s1.equals(signature)) &#123;\n            return 1;\n        &#125; else if (s2.equals(signature)) &#123;\n            return 2;\n        &#125;\n        return -1;\n    &#125;\n\n    // 根据方法编号, 正常调用目标对象方法\n    public Object invoke(int index, Object proxy, Object[] args) &#123;\n        if (index == 0) &#123;\n            ((Proxy) proxy).saveSuper();\n            return null;\n        &#125; else if (index == 1) &#123;\n            ((Proxy) proxy).saveSuper((int) args[0]);\n            return null;\n        &#125; else if (index == 2) &#123;\n            ((Proxy) proxy).saveSuper((long) args[0]);\n            return null;\n        &#125; else &#123;\n            throw new RuntimeException(\"无此方法\");\n        &#125;\n    &#125;\n\n    public static void main(String[] args) &#123;\n        ProxyFastClass fastClass = new ProxyFastClass();\n        int index = fastClass.getIndex(new Signature(\"saveSuper\", \"()V\"));\n        System.out.println(index);\n\n        fastClass.invoke(index, new Proxy(), new Object[0]);\n    &#125;\n&#125;\n\n\nimport org.springframework.cglib.core.Signature;\n\npublic class TargetFastClass &#123;\n    static Signature s0 = new Signature(\"save\", \"()V\");\n    static Signature s1 = new Signature(\"save\", \"(I)V\");\n    static Signature s2 = new Signature(\"save\", \"(J)V\");\n\n    // 获取目标方法的编号\n    /*\n        Target\n            save()              0\n            save(int)           1\n            save(long)          2\n        signature 包括方法名字、参数返回值\n     */\n    public int getIndex(Signature signature) &#123;\n        if (s0.equals(signature)) &#123;\n            return 0;\n        &#125; else if (s1.equals(signature)) &#123;\n            return 1;\n        &#125; else if (s2.equals(signature)) &#123;\n            return 2;\n        &#125;\n        return -1;\n    &#125;\n\n    // 根据方法编号, 正常调用目标对象方法\n    public Object invoke(int index, Object target, Object[] args) &#123;\n        if (index == 0) &#123;\n            ((Target) target).save();\n            return null;\n        &#125; else if (index == 1) &#123;\n            ((Target) target).save((int) args[0]);\n            return null;\n        &#125; else if (index == 2) &#123;\n            ((Target) target).save((long) args[0]);\n            return null;\n        &#125; else &#123;\n            throw new RuntimeException(\"无此方法\");\n        &#125;\n    &#125;\n\n    public static void main(String[] args) &#123;\n        TargetFastClass fastClass = new TargetFastClass();\n        int index = fastClass.getIndex(new Signature(\"save\", \"(I)V\"));\n        System.out.println(index);\n        fastClass.invoke(index, new Target(), new Object[]&#123;100&#125;);\n    &#125;\n&#125;\n\n\n当调用 MethodProxy 的 invoke 或 invokeSuper 方法时, 会动态生成两个类\nProxyFastClass 配合代理对象一起使用, 避免反射\nTargetFastClass 配合目标对象一起使用, 避免反射 (Spring 用的这种)\n\n\nTargetFastClass 记录了 Target 中方法与编号的对应关系\nsave(long) 编号 2\nsave(int) 编号 1\nsave() 编号 0\n首先根据方法名和参数个数、类型, 用 switch 和 if 找到这些方法编号\n然后再根据编号去调用目标方法, 又用了一大堆 switch 和 if, 但避免了反射\n\n\nProxyFastClass 记录了 Proxy 中方法与编号的对应关系，不过 Proxy 额外提供了下面几个方法\nsaveSuper(long) 编号 2，不增强，仅是调用 super.save(long)\nsaveSuper(int) 编号 1，不增强, 仅是调用 super.save(int)\nsaveSuper() 编号 0，不增强, 仅是调用 super.save()\n查找方式与 TargetFastClass 类似\n\n\n为什么有这么麻烦的一套东西呢？\n避免反射, 提高性能, 代价是一个代理类配两个 FastClass 类, 代理类中还得增加仅调用 super 的一堆方法\n用编号处理方法对应关系比较省内存, 另外, 最初获得方法顺序是不确定的, 这个过程没法固定死\n\n\n\njdk动态代理和cglib的对比\n\njdk不是一上来就优化，先要调用16次，第17次才会针对一个方法产生一个代理类 ，后面的调用都无需反射\n\ncglib是一开始就产生代理，一个代理类对应两个fastclass,一个配合代理对象使用，另一个配合目标本身，每个fastclass匹配到多个方法，所有产生的代理类的数目相对jdk代理要少\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;jdk 和 cglib 在 Spring 中的统一&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n","slug":"AOP实现之proxy","date":"2023-05-10T08:38:34.603Z","categories_index":"","tags_index":"","author_index":"大宝贝的程序员"},{"id":"d7e2802c58918040c31b14740192f666","title":"AOP实现之agent类加载","content":"AOP实现之agent类加载","slug":"AOP实现之agent类加载","date":"2023-05-10T08:32:18.000Z","categories_index":"","tags_index":"Java,Spring,AOP","author_index":"大宝贝的程序员"},{"id":"594a23198fd2599edc1d35598ede3148","title":"模板方法的设计模式","content":"模板方法的设计模式它定义了一个算法的骨架，允许子类在不改变算法结构的情况下重新定义算法的某些步骤。\n模板方法设计模式的优点\n\n将算法的实现细节和算法本身分离开，使得算法的变化不会影响到算法的客户端，只需要修改算法的具体实现即可。\n通过把通用方法提取到抽象类中，避免了重复代码的出现，提高了代码的可重用性。\n提高了代码的可扩展性，可以在不修改算法骨架结构的情况下替换部分内容。\n使得算法的实现更加灵活，允许不同子类实现算法骨架的不同部分。\n\n模板方法设计模式的缺点\n\n由于将算法细节分离开，代码的难度可能会增加，这会导致代码的维护成本增加。\n子类对父类的依赖性较高，使得继承的滥用可能会导致代码的复杂性和不可读性增加。\n如果算法骨架的修改较多，可能会导致大量的类都需要进行调整，这会带来一定的开销。\n\n示例首先，创建一个抽象模板类，它定义了一个算法的骨架，并包含一些抽象方法，这些方法将在子类中实现。\npublic abstract class AlgorithmTemplate &#123;\n    public void executeAlgorithm() &#123;\n        initialize();\n        process();\n        finalize();\n    &#125;\n\n    protected abstract void initialize();\n\n    protected abstract void process();\n\n    protected abstract void finalize();\n&#125;\n\n然后，创建几个继承自抽象模板类的具体子类，并实现抽象方法。\npublic class ConcreteAlgorithmA extends AlgorithmTemplate &#123;\n    protected void initialize() &#123;\n        System.out.println(&quot;ConcreteAlgorithmA: Initializing...&quot;);\n    &#125;\n\n    protected void process() &#123;\n        System.out.println(&quot;ConcreteAlgorithmA: Processing...&quot;);\n    &#125;\n\n    protected void finalize() &#123;\n        System.out.println(&quot;ConcreteAlgorithmA: Finalizing...&quot;);\n    &#125;\n&#125;\n\npublic class ConcreteAlgorithmB extends AlgorithmTemplate &#123;\n    protected void initialize() &#123;\n        System.out.println(&quot;ConcreteAlgorithmB: Initializing...&quot;);\n    &#125;\n\n    protected void process() &#123;\n        System.out.println(&quot;ConcreteAlgorithmB: Processing...&quot;);\n    &#125;\n\n    protected void finalize() &#123;\n        System.out.println(&quot;ConcreteAlgorithmB: Finalizing...&quot;);\n    &#125;\n&#125;\n\n最后，在主程序中使用这些子类来演示模板方法设计模式。\npublic class Main &#123;\n    public static void main(String[] args) &#123;\n        AlgorithmTemplate algorithmA = new ConcreteAlgorithmA();\n        AlgorithmTemplate algorithmB = new ConcreteAlgorithmB();\n\n        algorithmA.executeAlgorithm();\n        algorithmB.executeAlgorithm();\n    &#125;\n&#125;\n\n当执行该程序时，输出如下：\nConcreteAlgorithmA: Initializing...\nConcreteAlgorithmA: Processing...\nConcreteAlgorithmA: Finalizing...\nConcreteAlgorithmB: Initializing...\nConcreteAlgorithmB: Processing...\nConcreteAlgorithmB: Finalizing...\n\n可以看出，多个具体子类都通过继承抽象模板类来实现相同的算法骨架。这使得更改算法骨架变得更加容易，并且可以避免重复编写相似的算法代码。\n","slug":"模板方法模式","date":"2023-05-09T13:53:44.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"8ced7c3b7dad33d06ee59bde7028cd4a","title":"策略模式","content":"策略模式它允许在运行时根据不同的情况选择算法的行为方式。\n在策略模式中，有多个算法可以完成同一项任务。在使用策略模式时，我们将每个算法都封装在一个独立的类中，这些类都实现了一个共同的接口。然后，在运行时，我们可以根据需要选择合适的算法来完成任务。\n优点\n\n封装了一系列算法：将一系列算法封装在不同的策略类中，使得这些算法可以互相替换而不影响客户端使用。\n可以动态切换算法：客户端可以在运行时动态选择使用哪个算法，实现了算法的动态切换。\n减少了复杂的条件语句：在不使用策略模式时，常常需要使用大量的条件语句来实现不同的算法，这样会使代码变得复杂而难以维护，策略模式可以避免这种情况的发生。\n提高了代码的可复用性：不同的策略类可以被多个客户端使用，提高了代码的重用性和可扩展性。\n\n缺点\n\n增加了类的数量：每个算法都需要一个对应的策略类，这样就会增加类的数量，使代码更加复杂，因此需要适当考虑其使用情况。\n客户端必须了解不同的策略类：客户端必须知道所有可用的策略类，并且自己决定哪一个策略类最适合解决当前的问题，这将增加客户端的困难。\n策略模式无法完全解决复杂的问题：虽然策略模式可以很好地解决简单的问题，但在某些情况下，可能需要多个算法之间的协调和处理，这时使用策略模式就很难实现。\n\n示例//创建一个接口，实现所有算法类都要实现的方法\ninterface Operation &#123;\n    double calculate(double a, double b);\n&#125;\n\n//创建算法类和实现Operation接口\nclass Addition implements Operation &#123;\n    public double calculate(double a, double b) &#123;\n        return a + b;\n    &#125;\n&#125;\n\nclass Subtraction implements Operation &#123;\n    public double calculate(double a, double b) &#123;\n        return a - b;\n    &#125;\n&#125;\n\nclass Multiplication implements Operation &#123;\n    public double calculate(double a, double b) &#123;\n        return a * b;\n    &#125;\n&#125;\n\n//创建一个Context类，用来设置实际的策略\nclass Calculator &#123;\n    private Operation operation;\n\n    public void setOperation(Operation operation) &#123;\n        this.operation = operation;\n    &#125;\n\n    public double calculate(double a, double b) &#123;\n        return operation.calculate(a, b);\n    &#125;\n&#125;\n\n//使用示例\npublic class StrategyExample &#123;\n    public static void main(String[] args) &#123;\n        double a = 1.5;\n        double b = 2.0;\n\n        Calculator calculator = new Calculator();\n\n        //设置加法运算策略\n        calculator.setOperation(new Addition());\n        System.out.println(calculator.calculate(a, b)); //3.5\n\n        //设置减法运算策略\n        calculator.setOperation(new Subtraction());\n        System.out.println(calculator.calculate(a, b)); //-0.5\n\n        //设置乘法运算策略\n        calculator.setOperation(new Multiplication());\n        System.out.println(calculator.calculate(a, b)); //3.0\n    &#125;\n&#125;\n\n","slug":"策略模式","date":"2023-05-09T13:20:59.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"cd55e9d94b94ab2ca447027ce4beeca6","title":"观察者模式","content":"观察者模式它允许一个对象（称为被观察者或主题）维护一组依赖于它的对象（称为观察者），当被观察者发生变化时，它会通知所有观察者以便更新它们自己的状态。\n优点：\n\n在观察者模式中，被观察者和观察者之间是松耦合的关系，使得它们之间的交互变得简单而灵活。\n观察者模式支持广播通信，当一个对象发生改变时，多个观察者会同时得到通知，可以在不同的处理逻辑对其进行响应，提高了系统的可扩展性。\n观察者模式符合面向对象设计原则，将业务分离，使得代码更易于维护和扩展。\n\n缺点：\n\n观察者模式可能会导致系统中的观察者对象过多，造成性能上的问题。\n观察者模式需要考虑到开发效率与运行效率的平衡，在一些特定的场景下，使用观察者模式并不是最佳实践。\n\n应用场景：\n\n一个对象的改变需要同时改变其他对象的时候，可以考虑使用观察者模式。\n当系统中多个对象之间存在着一对多关系：一个对象的改变会影响到其他对象的时候，可以使用观察者模式。\n在分层架构中，可以使用观察者模式来解耦各层之间的关系。\n当需要将一个对象的状态同步到其他对象中，而又不希望耦合太多代码的时候，可以使用观察者模式。\n\n下面是一个Java代码示例首先，我们定义一个主题接口（Subject），定义主题必须实现的方法：\npublic interface Subject &#123;\n    void attach(Observer observer);\n    void detach(Observer observer);\n    void notifyObservers(String msg);\n&#125;\n\n其中，attach(Observer)方法和detach(Observer)方法用于注册和注销观察者，notifyObservers(String)方法用于通知所有注册的观察者主题发生了变化。\n然后，我们定义一个观察者接口（Observer），观察者必须实现的方法：\npublic interface Observer &#123;\n    void update(String msg);\n&#125;\n\n其中，update(String)方法用于接收主题发生变化的通知并进行相应的处理。\n接下来，我们实现Subject接口：\npublic class ConcreteSubject implements Subject &#123;\n    private List&lt;Observer> observers = new ArrayList&lt;>();\n    private String state;\n\n    public void attach(Observer observer) &#123;\n        observers.add(observer);\n    &#125;\n\n    public void detach(Observer observer) &#123;\n        observers.remove(observer);\n    &#125;\n\n    public void notifyObservers(String msg) &#123;\n        for (Observer observer : observers) &#123;\n            observer.update(msg);\n        &#125;\n    &#125;\n\n    public void setState(String state) &#123;\n        this.state = state;\n        notifyObservers(\"State changed to \" + state);\n    &#125;\n&#125;\n\n其中，observers是用于存储所有观察者对象的列表，state是主题的状态。\nattach(Observer)方法和detach(Observer)方法用于添加和删除观察者对象，notifyObservers(String)方法用于通知所有观察者主题发生了变化。在setState(String)方法中，每次设置主题状态时都会调用notifyObservers(String)方法通知所有观察者。\n最后，我们实现Observer接口：\npublic class ConcreteObserver implements Observer &#123;\n    private String name;\n\n    public ConcreteObserver(String name) &#123;\n        this.name = name;\n    &#125;\n\n    public void update(String msg) &#123;\n        System.out.println(name + \" received message: \" + msg);\n    &#125;\n&#125;\n\n其中，update()方法用于接收主题发生变化的通知并进行相应的处理。\n下面是一个使用观察者模式的示例：\npublic static void main(String[] args) &#123;\n    ConcreteSubject subject = new ConcreteSubject();\n\n    ConcreteObserver observer1 = new ConcreteObserver(\"Observer 1\");\n    ConcreteObserver observer2 = new ConcreteObserver(\"Observer 2\");\n\n    subject.attach(observer1);\n    subject.attach(observer2);\n\n    subject.setState(\"New state\");\n\n    subject.detach(observer1);\n\n    subject.setState(\"Another new state\");\n&#125;\n\n在这个示例中，我们首先创建了一个ConcreteSubject对象，并创建了两个ConcreteObserver对象。然后，我们将两个观察者对象注册到主题对象中，并设置主题状态为“New state”，所有观察者都会收到通知并进行相应的处理。接着，我们将其中一个观察者对象从主题对象中注销，设置主题状态为“Another new state”，只有一个观察者对象会收到通知并进行相应的处理。\n","slug":"观察者模式","date":"2023-05-09T12:47:06.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"2b09bf65e737f820cf301fb7434c8c8e","title":"责任链模式","content":"责任链模式责任链模式是一种行为设计模式，用于将请求从一个处理程序传递到另一个处理程序，直到找到能够处理请求的处理程序。每个处理程序都将请求传递给下一个处理程序，直到请求被处理为止。\n责任链模式的特点\n\n请求发送者不必知道请求在何时、何处以及如何被处理。\n可以动态增加或修改请求的处理流程，增强了系统的灵活性、可维护性、可扩展性。\n处理程序之间解耦，互相独立，易于单元测试和调试。\n可以避免请求的发送者和接收者的耦合关系，提高系统的灵活性。\n\n责任链模式的优点\n\n单一职责原则：每个处理程序只负责处理自己专业领域内的请求。\n开闭原则：可以很方便地增加或删除处理程序，同时不会影响到其他处理程序。\n易于扩展：可以根据需要动态地增加或修改请求的处理流程。具有很好的灵活性和可扩展性。\n代码复用性高：能够避免大量重复代码的产生，减少了系统的维护成本。\n\n责任链模式的缺点\n\n无法保证请求一定被处理：如果链中没有任何一个处理程序能够处理请求，那么请求可能会被忽略或者丢失。\n可能导致系统性能下降：由于处理程序是动态添加的，可能会导致系统的处理过程比较缓慢、效率较低。\n可能会产生很多细粒度的对象：如果责任链比较长，那么可能会产生很多细粒度的对象，导致系统资源的浪费。\n\n示例public abstract class Handler &#123;\n \n    protected Handler successor;\n \n    public void setSuccessor(Handler successor) &#123;\n        this.successor = successor;\n    &#125;\n \n    public abstract void handleRequest(Request request);\n&#125;\n\npublic class ConcreteHandler1 extends Handler &#123;\n \n    public void handleRequest(Request request) &#123;\n        if (request.getType() == RequestType.TYPE1) &#123;\n            System.out.println(request.getName() + \" is handled by ConcreteHandler1\");\n        &#125; else if (successor != null) &#123;\n            successor.handleRequest(request);\n        &#125;\n    &#125;\n&#125;\n\npublic class ConcreteHandler2 extends Handler &#123;\n \n    public void handleRequest(Request request) &#123;\n        if (request.getType() == RequestType.TYPE2) &#123;\n            System.out.println(request.getName() + \" is handled by ConcreteHandler2\");\n        &#125; else if (successor != null) &#123;\n            successor.handleRequest(request);\n        &#125;\n    &#125;\n&#125;\n\npublic class Request &#123;\n     \n    private RequestType type;\n    private String name;\n \n    public Request(RequestType type, String name) &#123;\n        this.type = type;\n        this.name = name;\n    &#125;\n \n    public RequestType getType() &#123;\n        return type;\n    &#125;\n \n    public String getName() &#123;\n        return name;\n    &#125;\n&#125;\n\npublic enum RequestType &#123;\n    TYPE1, TYPE2\n&#125;\n\npublic class Main &#123;\n \n    public static void main(String[] args) &#123;\n        Handler handler1 = new ConcreteHandler1();\n        Handler handler2 = new ConcreteHandler2();\n         \n        handler1.setSuccessor(handler2);\n \n        Request request1 = new Request(RequestType.TYPE1, \"Request 1\");\n        Request request2 = new Request(RequestType.TYPE2, \"Request 2\");\n \n        handler1.handleRequest(request1);\n        handler1.handleRequest(request2);\n    &#125;\n&#125;\n\n在上面的示例中，我们定义了一个抽象处理程序类Handler，它包含一个指向下一个处理程序的引用。然后我们创建了两个具体的处理程序类ConcreteHandler1和ConcreteHandler2，它们都继承了Handler类并实现了handleRequest()方法。\n我们还创建了一个Request类和一个枚举类型RequestType用于模拟请求对象。最后，我们在Main类中创建了两个请求对象并将它们传递给链中的第一个处理程序ConcreteHandler1。\n在上面的示例中，如果请求类型是TYPE1，则它由ConcreteHandler1处理，否则它将传递给下一个处理程序。如果请求类型是TYPE2，则它由ConcreteHandler2处理，否则它将传递给下一个处理程序，直到找到能够处理请求的处理程序。\n","slug":"责任链模式","date":"2023-05-09T12:16:17.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"424f199f6c88af138e76cfdf85fb4fa5","title":"代理模式","content":"代理模式它为其他对象提供一种代理以控制对这个对象的访问。代理对象在客户端和目标对象之间起到中介作用，可以进行一些附加的工作，例如访问控制、远程访问、缓存等。\n以下是一个简单的代理模式的Java例子：interface Image &#123;\n    void display();\n&#125;\n\nclass RealImage implements Image &#123;\n    private String filename;\n\n    public RealImage(String filename) &#123;\n        this.filename = filename;\n        loadFromDisk();\n    &#125;\n\n    private void loadFromDisk() &#123;\n        System.out.println(\"Loading \" + filename);\n    &#125;\n\n    public void display() &#123;\n        System.out.println(\"Displaying \" + filename);\n    &#125;\n&#125;\n\nclass ImageProxy implements Image &#123;\n    private String filename;\n    private RealImage image;\n\n    public ImageProxy(String filename) &#123;\n        this.filename = filename;\n    &#125;\n\n    public void display() &#123;\n        if (image == null)\n            image = new RealImage(filename);\n        image.display();\n    &#125;\n&#125;\n\npublic class ProxyDemo &#123;\n    public static void main(String[] args) &#123;\n        Image image = new ImageProxy(\"test.jpg\");\n        image.display();\n    &#125;\n&#125;\n\n在上面的例子中，我们定义了一个Image接口，其中RealImage是实现此接口的具体对象，它代表了一个真实的图片文件。ImageProxy类也实现了Image接口，但是它并不是真正的图片，而是一个代理。它可以延迟加载RealImage对象，并且在需要时，通过代理实现对RealImage对象的访问。这样，我们就可以控制对RealImage对象的访问并且可以做一些附加的工作。当客户端调用Image的display()方法时，ImageProxy会判断是否已经加载了RealImage对象，如果没有，则创建RealImage对象并调用它的display()方法。\n","slug":"代理模式","date":"2023-05-09T11:52:50.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"af957b16e3777023cafc3b1e9179b2cd","title":"装饰器模式","content":"装饰器模式装饰者模式（Decorator Pattern）\n​\t它允许你向现有对象添加新的功能，同时又不改变其结构。装饰者模式的核心思想是将功能进行分离，让各个类只专注于自己的职责。通过这种方式，我们可以以非常灵活的方式扩展系统的功能，而不必修改原有代码。\n​\t装饰器模式的主要目的是为对象动态地添加额外的功能，不需要修改原始对象的结构。装饰器接收一个原始对象，并在其上添加一些额外的装饰操作，从而增强了原始对象的功能。装饰器模式避免了使用子类继承的方式进行功能扩展，因为这种方式可能导致类层次结构过于复杂，而且无法动态修改对象的行为。\n下面是一个简单的示例：\n首先，我们需要定义一个具有基本功能的接口Component和该接口的一个实现类ConcreteComponent：\npublic interface Component &#123;\n    void operation();\n&#125;\n\npublic class ConcreteComponent implements Component &#123;\n    @Override\n    public void operation() &#123;\n        System.out.println(\"This is a Concrete Component\");\n    &#125;\n&#125;\n\n然后，我们需要创建装饰器Decorator，它持有一个Component实例，并重新实现operation方法：\npublic class Decorator implements Component &#123;\n    private Component component;\n\n    public Decorator(Component component) &#123;\n        this.component = component;\n    &#125;\n\n    @Override\n    public void operation() &#123;\n        component.operation();\n    &#125;\n&#125;\n\n最后，我们创建一个具体的装饰器ConcreteDecorator，它添加了额外的功能：\npublic class ConcreteDecorator extends Decorator &#123;\n    public ConcreteDecorator(Component component) &#123;\n        super(component);\n    &#125;\n\n    @Override\n    public void operation() &#123;\n        super.operation();\n        addedFunction();\n    &#125;\n\n    private void addedFunction() &#123;\n        System.out.println(\"This is an added function\");\n    &#125;\n&#125;\n\n我们可以通过以下方式使用装饰器模式：\nComponent c1 = new ConcreteComponent();\nc1.operation();\n\nComponent c2 = new ConcreteDecorator(new ConcreteComponent());\nc2.operation();\n\nComponent c3 = new ConcreteDecorator(new ConcreteDecorator(new ConcreteComponent()));\nc3.operation();\n\n输出结果：\nThis is a Concrete Component\nThis is a Concrete Component\nThis is an added function\nThis is a Concrete Component\nThis is an added function\nThis is an added function\n\n从输出结果可以看出，通过装饰器模式，我们可以动态地添加额外的功能，而不需要修改原有代码。\n总结一下，装饰器模式是一种非常有用的设计模式，它可以让我们以一种灵活的方式扩展系统的功能。同时，装饰器模式让各个类的职责更加清晰，可以更加方便地进行维护。但是，在使用装饰器模式时，我们需要注意不要过度使用，避免造成代码过于复杂和混乱。\n","slug":"装饰器模式","date":"2023-05-09T11:12:05.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"890198f3e29403fa2d961081f54ed8cd","title":"组合模式","content":"组合模式组合模式 \n主要通过将对象组合成树形结构来表示“整体-部分”的关系，让客户端能够以一致的方式对待单个对象和对象合。\n下面是一个简单的例子import java.util.ArrayList;\nimport java.util.List;\n\npublic interface Employee &#123;\n    void showDetails();\n&#125;\n\nclass Leaf implements Employee &#123;\n    private String name;\n    private String position;\n\n    Leaf(String name, String position) &#123;\n        this.name = name;\n        this.position = position;\n    &#125;\n\n    @Override\n    public void showDetails() &#123;\n        System.out.println(name + \" is a \" + position);\n    &#125;\n&#125;\n\nclass Composite implements Employee &#123;\n    private List&lt;Employee> employees = new ArrayList&lt;Employee>();\n\n    @Override\n    public void showDetails() &#123;\n        for (Employee employee : employees) &#123;\n            employee.showDetails();\n        &#125;\n    &#125;\n\n    public void addEmployee(Employee employee) &#123;\n        employees.add(employee);\n    &#125;\n\n    public void removeEmployee(Employee employee) &#123;\n        employees.remove(employee);\n    &#125;\n&#125;\n\npublic class Main &#123;\n    public static void main(String[] args) &#123;\n        Composite organization = new Composite();\n        organization.addEmployee(new Leaf(\"John Doe\", \"Manager\"));\n        Composite department = new Composite();\n        department.addEmployee(new Leaf(\"Jane Smith\", \"Team Lead\"));\n        department.addEmployee(new Leaf(\"Bob Johnson\", \"Engineer\"));\n        Composite subDepartment = new Composite();\n        subDepartment.addEmployee(new Leaf(\"Tina Turner\", \"Engineer\"));\n        subDepartment.addEmployee(new Leaf(\"Steve Rogers\", \"Engineer\"));\n        department.addEmployee(subDepartment);\n        organization.addEmployee(department);\n        organization.showDetails();\n    &#125;\n&#125;\n\n在上面的示例中，Employee是一个接口，有两个实现类：Leaf和Composite。Leaf代表的是单个员工，而Composite代表的是员工组合。composite 对象的作用是，将分散的调用集中起来，统一调用入口，它的特征是，与具体干活的实现实现同一个接口，当调用 composite 对象的接口方法时，其实是委托具体干活的实现来完成\n使用组合模式，我们可以创建一个包含多个员工和部门的组织结构，并可以方便地对整个组织结构进行操作。在示例中，我们创建了一个根节点organization，它包含一个员工John Doe和一个部门department。部门department包含一个团队领导Jane Smith和一个工程师Bob Johnson，以及一个子部门subDepartment，子部门subDepartment包含两个工程师Tina Turner和Steve Rogers。最后，我们通过调用organization.showDetails()方法来打印组织结构中每个员工的详细信息。\n","slug":"组合模式","date":"2023-05-09T10:42:25.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"a8b2eb4f810dce05fcecb16e8edfd126","title":"适配器模式","content":"适配器模式适配器模式（Adapter Pattern）是一种结构型设计模式，它用于将一个类的接口转换成客户端所期望的另一种接口，从而使原本不兼容的接口能够协同工作。在适配器模式中，适配器充当了两个不兼容接口之间的桥梁，它负责允许这些接口间能够相互协作。\n在Java中，适配器模式常用于将不兼容的接口进行转换，通常包含以下三个角色：\n\nTarget（目标抽象类）：客户端所期望的接口，定义客户端所需的操作。\nAdapter（适配器类）：适配器，将Adaptee转换成Target所期望的接口。它维护了一个指向Adaptee对象的引用，并实现Target接口，使得客户端能够通过Adapter来访问Adaptee对象。\nAdaptee（原本的类）：需要被适配的类，包含原本的方法或接口。\n\n示例我们模拟了一个旧版的Android手机（OldAndroidPhone）和一个新版的iOS手机（NewiPhone），它们分别有不同的音乐播放器接口，而我们需要使用一个适配器（MusicPlayerAdapter）来兼容它们的操作：\n//原有音乐播放接口 \ninterface MusicPlayer &#123;\n    void playMP3(String fileName);\n&#125;\n\n//旧版Android手机\nclass OldAndroidPhone implements MusicPlayer &#123;\n    public void playMP3(String fileName) &#123;\n        System.out.println(\"Old Android phone is playing MP3 file: \" + fileName);\n    &#125;\n&#125;\n\n//新版iPhone手机 \ninterface NewiPhonePlayer &#123;\n    void playAAC(String fileName);\n&#125;\n\nclass NewiPhone implements NewiPhonePlayer &#123;\n    public void playAAC(String fileName) &#123;\n        System.out.println(\"New iPhone is playing AAC file: \" + fileName);\n    &#125;\n&#125;\n\n//适配器，将旧版手机的操作适配成新版手机可以使用的形式\nclass MusicPlayerAdapter implements NewiPhonePlayer &#123;\n    private MusicPlayer player;\n    \n    public MusicPlayerAdapter(MusicPlayer player)&#123;\n        this.player = player;\n    &#125;\n    \n    //适配器将AAC文件转化为MP3文件，并调用原有的播放方法\n    public void playAAC(String fileName) &#123;\n        String mp3File = convertAACtoMP3(fileName);\n        player.playMP3(mp3File);\n    &#125;\n    \n    private String convertAACtoMP3(String fileName)&#123;\n        System.out.println(\"Converting AAC to MP3: \" + fileName);\n        return fileName.replace(\".aac\", \".mp3\");\n    &#125;\n&#125;\n\n//客户端使用例子\npublic class Client &#123;\n    public static void main(String[] args) &#123;\n        //旧版Android手机播放MP3\n        MusicPlayer oldPhone = new OldAndroidPhone();\n        oldPhone.playMP3(\"old_phone_music.mp3\");\n        \n        //新版iPhone手机播放AAC，使用适配器兼容播放MP3文件\n        NewiPhonePlayer newPhone = new NewiPhone();\n        MusicPlayerAdapter adapter = new MusicPlayerAdapter(oldPhone);\n        newPhone.playAAC(\"new_phone_music.aac\");\n        adapter.playAAC(\"new_phone_music.aac\");\n    &#125;\n&#125;\n\n在上面的代码中，我们首先定义了两个已有的音乐播放器接口 MusicPlayer 和 NewiPhonePlayer，它们分别被 OldAndroidPhone 和 NewiPhone 实现。然后，我们使用 MusicPlayerAdapter 类将 OldAndroidPhone 类的 playMP3 方法适配成 NewiPhonePlayer 接口的 playAAC 方法，通过这个适配器，新版 iPhone 手机可以兼容旧版 Android 手机的音乐播放功能。在客户端使用时，我们分别调用了 OldAndroidPhone 的 playMP3 方法（旧版 Android 手机）和 NewiPhonePlayer 的 playAAC 方法（新版 iPhone 手机，使用 MusicPlayerAdapter 适配器）。\n我们可以看出适配器模式的优点：\n\n可以让原有的接口和新接口之间不发生冲突地协同工作，从而让代码更好地拓展和维护。\n\n","slug":"适配器模式","date":"2023-05-09T08:09:59.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"3dbdc722104d94fc07273118e255f7fd","title":"工厂模式","content":"工厂模式简单⼯⼚模式指由⼀个⼯⼚对象来创建实例，客户端不需要关注创建逻辑，只需提供传⼊⼯⼚的参数\n简单工厂UML类图\n\n适⽤于⼯⼚类负责创建对象较少的情况，缺点是如果要增加新产品，就需\n要修改⼯⼚类的判断逻辑，违背开闭原则，且产品多的话会使⼯⼚类⽐较\n复杂。\n简单工厂的例子:\nCalendar类的getInstance方法使用了一种简单工厂的方式来创建不同地区的日历对象。\n具体来说，Calendar类本身是一个抽象类，它定义了一些方法来处理日期和时间。由于不同地区有不同的日历，因此Calendar类并没有直接实现具体的日历，而是通过getInstance方法来获取指定地区的Calendar实例。getInstance方法接受一个Locale类型的参数，它根据不同的Locale参数调用createCalendar方法创建具体的日历实例。createCalendar方法是一个protected方法，由Calendar类的子类来实现。在具体的子类中，根据传入的Locale参数创建相应的日历对象，然后返回。这里，createCalendar方法就扮演了一个简单工厂模式中的工厂的角色，而Calendar类则相当于一个创建工厂，负责整个日历对象的生成过程的调度。\n工厂方法模式在工厂方法模式中，我们不再提供一个统一的工厂类来创建所有的对象，而是提供一个工厂接口，由不同的子类来实现工厂接口中的方法来创建不同的对象。这样做的好处是增加了系统的扩展性和灵活性，可以根据实际需求来增加相应的子类和工厂实现。\n工厂方法模式通常由四部分组成：抽象产品、具体产品、抽象工厂、具体工厂。其中，抽象产品是需要创建的对象的通用接口，具体产品是实现抽象产品接口的具体类，抽象工厂是创建产品的接口，具体工厂是实现抽象工厂接口的具体类。\n工厂方法UML类图：\n\n使用工厂方法模式的好处是：\n\n可扩展性更好。使用工厂方法模式可以更加容易地扩展和添加新的产品类，而不会影响原有的代码结构。\n\n易于维护。工厂方法模式将创建对象的代码集中在一个地方，易于维护。当需要修改时，只需要修改对应的工厂类即可，而不需要修改客户端代码。\n\n降低耦合度。使用工厂方法模式可以将客户端代码和具体产品的实现解耦，使得客户端代码不需要了解每个具体产品类的细节。\n\n\n抽象工厂方法模式抽象工厂模式是工厂方法模式的扩展，旨在提供一个工厂接口来创建一系列相关的产品，而不是单一的产品类。\n抽象工厂模式中会存在多个产品族，每个产品族包含多个产品等级结构。产品等级结构是指具有相同功能的产品组成的集合，例如大众汽车、奥迪汽车和奔驰汽车组成了一个产品族，而每种汽车都具有车轮、车身和发动机等等组成的产品等级结构。\n在抽象工厂模式中，定义一个抽象工厂接口，它包含了一组用于创建产品族中每个产品等级结构的方法。每个具体的工厂类实现这个接口，负责实现自己的产品族生产过程。而具体产品则由具体工厂类中的具体方法实现来创建。\n通过抽象工厂模式，我们可以创造一个家族的对象，这个家族可以看做是一个产品族，而每个家族成员可以看做是这个产品族中的某个产品等级结构。抽象工厂模式保证了各种产品之间的兼容性，即一个工厂创建的产品都是该工厂创建的其他产品的兼容组合。\n抽象工厂UML类图：\n\n示例代码:首先是汽车工厂的抽象类（Abstract Factory）：\n// 定义汽车工厂的抽象类\nabstract class CarFactory &#123;\n    abstract Wheel createWheel();\n    abstract Body createBody();\n    abstract Engine createEngine();\n&#125;\n\n然后是具体的大众汽车工厂类（Volkswagen Car Factory）和奥迪汽车工厂类（Audi Car Factory）：\n// 定义大众汽车工厂\nclass VolkswagenFactory extends CarFactory &#123;\n    @Override\n    public Wheel createWheel() &#123;\n        return new VolkswagenWheel();\n    &#125;\n    @Override\n    public Body createBody() &#123;\n        return new VolkswagenBody();\n    &#125;\n    @Override\n    public Engine createEngine() &#123;\n        return new VolkswagenEngine();\n    &#125;\n&#125;\n\n// 定义奥迪汽车工厂\nclass AudiFactory extends CarFactory &#123;\n    @Override\n    public Wheel createWheel() &#123;\n        return new AudiWheel();\n    &#125;\n    @Override\n    public Body createBody() &#123;\n        return new AudiBody();\n    &#125;\n    @Override\n    public Engine createEngine() &#123;\n        return new AudiEngine();\n    &#125;\n&#125;\n\n然后是汽车组件的抽象类（Abstract Product）：\n// 定义汽车的组件抽象类\nabstract class Wheel &#123;\n    public abstract void create();\n&#125;\nabstract class Body &#123;\n    public abstract void create();\n&#125;\nabstract class Engine &#123;\n    public abstract void create();\n&#125;\n\n具体的大众和奥迪汽车组件的类：\n// 定义大众汽车组件\nclass VolkswagenWheel extends Wheel &#123;\n    @Override\n    public void create() &#123;\n        System.out.println(\"生产大众车轮\");\n    &#125;\n&#125;\nclass VolkswagenBody extends Body &#123;\n    @Override\n    public void create() &#123;\n        System.out.println(\"生产大众车身\");\n    &#125;\n&#125;\nclass VolkswagenEngine extends Engine &#123;\n    @Override\n    public void create() &#123;\n        System.out.println(\"生产大众发动机\");\n    &#125;\n&#125;\n\n// 定义奥迪汽车组件\nclass AudiWheel extends Wheel &#123;\n    @Override\n    public void create() &#123;\n        System.out.println(\"生产奥迪车轮\");\n    &#125;\n&#125;\nclass AudiBody extends Body &#123;\n    @Override\n    public void create() &#123;\n        System.out.println(\"生产奥迪车身\");\n    &#125;\n&#125;\nclass AudiEngine extends Engine &#123;\n    @Override\n    public void create() &#123;\n        System.out.println(\"生产奥迪发动机\");\n    &#125;\n&#125;\n\n最后，我们可以使用这些类来创建不同品牌的汽车：\npublic class Main &#123;\n    public static void main(String[] args) &#123;\n        // 创建大众汽车\n        CarFactory vwFactory = new VolkswagenFactory();\n        vwFactory.createWheel().create();\n        vwFactory.createBody().create();\n        vwFactory.createEngine().create();\n        \n        // 创建奥迪汽车\n        CarFactory audiFactory = new AudiFactory();\n        audiFactory.createWheel().create();\n        audiFactory.createBody().create();\n        audiFactory.createEngine().create();\n    &#125;\n&#125;\n\n","slug":"工厂模式","date":"2023-05-09T05:19:32.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"b00cac0eae3b04411014ea36945005af","title":"建造者模式","content":"Java建造者模式Java中的建造者模式(Builder Pattern)\n主要用于创建一个复杂对象，它通过一步一步地构建，可以创建出不同的对象表示。\n该模式包含以下几个角色：\n\nBuilder(建造者)：抽象建造者，定义创建对象的接口，并包含返回建造产品的方法。\n\nConcreteBuilder(具体建造者)：实现Builder接口，实现具体产品的创建过程，以及返回具体产品的方法。\n\nDirector(导演者)：负责调用建造者生成产品。\n\nProduct(产品)：表示被生成的复杂对象，包含多个部件。\n\n\n以下是Java中的建造者模式示例代码：\n// Product\nclass Car &#123;\n    private String engine;\n    private String chassis;\n    private String body;\n\n    public void setEngine(String engine) &#123;\n        this.engine = engine;\n    &#125;\n\n    public void setChassis(String chassis) &#123;\n        this.chassis = chassis;\n    &#125;\n\n    public void setBody(String body) &#123;\n        this.body = body;\n    &#125;\n\n    @Override\n    public String toString() &#123;\n        return \"Car &#123;engine='\" + engine + \"', chassis='\" + chassis + \"', body='\" + body + \"'&#125;\";\n    &#125;\n&#125;\n\n// Builder\ninterface CarBuilder &#123;\n    void buildEngine();\n    void buildChassis();\n    void buildBody();\n    Car getCar();\n&#125;\n\n// ConcreteBuilders\nclass SportsCarBuilder implements CarBuilder &#123;\n    private Car car;\n\n    public SportsCarBuilder() &#123;\n        this.car = new Car();\n    &#125;\n\n    @Override\n    public void buildEngine() &#123;\n        car.setEngine(\"3.0L V6\");\n    &#125;\n\n    @Override\n    public void buildChassis() &#123;\n        car.setChassis(\"Aluminum\");\n    &#125;\n\n    @Override\n    public void buildBody() &#123;\n        car.setBody(\"Carbon Fiber\");\n    &#125;\n\n    @Override\n    public Car getCar() &#123;\n        return this.car;\n    &#125;\n&#125;\n\nclass SedanCarBuilder implements CarBuilder &#123;\n    private Car car;\n\n    public SedanCarBuilder() &#123;\n        this.car = new Car();\n    &#125;\n\n    @Override\n    public void buildEngine() &#123;\n        car.setEngine(\"2.4L 4-cylinder\");\n    &#125;\n\n    @Override\n    public void buildChassis() &#123;\n        car.setChassis(\"Steel\");\n    &#125;\n\n    @Override\n    public void buildBody() &#123;\n        car.setBody(\"Metal\");\n    &#125;\n\n    @Override\n    public Car getCar() &#123;\n        return this.car;\n    &#125;\n&#125;\n\n\n// Director\nclass AutomotiveEngineer &#123;\n    public void build (CarBuilder builder) &#123;\n        builder.buildEngine();\n        builder.buildChassis();\n        builder.buildBody();\n    &#125;\n&#125;\n\npublic class BuilderPatternExample &#123;\n    public static void main(String[] args) &#123;\n        AutomotiveEngineer engineer = new AutomotiveEngineer();\n\n        CarBuilder sportsCarBuilder = new SportsCarBuilder();\n        engineer.build (sportsCarBuilder);\n        System.out.println(sportsCarBuilder.getCar()); \n        // Output: Car &#123;engine='3.0L V6', chassis='Aluminum', body='Carbon Fiber'&#125;\n\n        CarBuilder sedanCarBuilder = new SedanCarBuilder();\n        engineer.build (sedanCarBuilder);\n        System.out.println(sedanCarBuilder.getCar()); \n        // Output: Car &#123;engine='2.4L 4-cylinder', chassis='Steel', body='Metal'&#125;\n    &#125;\n&#125;\n\n在这个示例中，CarBuilder表示抽象建造者，SportsCarBuilder和SedanCarBuilder分别表示具体建造者，AutomotiveEngineer表示导演者，Car表示产品。通过导演者调用具体建造者的构建方法，即可建造出不同的Car实例。\n它的主要亮点有三处：\n\n较为灵活的构建产品对象\n\n在不执行最后 build 方法前，产品对象都不可用\n\n构建过程采用链式调用，看起来比较爽\n\n\n","slug":"建造者模式","date":"2023-05-09T04:38:15.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"0c6e8405652937a86cc3782f838cf782","title":"spring中常见的设计模式","content":"Spring中常见的设计模式1.Spring中的单例模式单例模式 与 单例bean 的区别\n\n根据单例模式的目的 Ensure a class only has one instance, and provide a global point of access to it\n\n就是确保只有一个实例提供给全局使用\n\n显然Spring中的单例bean并非实现了单例模式，单例bean只能保证每个容器内，相同id的bean单实例\n Spring中也有用到单例模式\n\norg.springframework.transaction.TransactionDefinition#withDefaults\n\norg.springframework.aop.TruePointcut#INSTANCE\n\norg.springframework.aop.interceptor.ExposeInvocationInterceptor#ADVISOR\n\norg.springframework.core.annotation.AnnotationAwareOrderComparator#INSTANCE\n\norg.springframework.core.OrderComparator#INSTANCE\n\n\n2.Spring中的Builder（建造者模式）定义 Separate the construction of a complex object from its representation so that the same construction process can create different representations 即将复杂对象的构建与其表示分离，以便同样的构建过程可以创建不同的表示形式。\n优点：\n\n较为灵活的构建产品对象\n在不执行最后 build 方法前，产品对象都不可用\n构建过程采用链式调用，看起来比较爽\n\nSpring 中体现 Builder 模式的地方：\n\norg.springframework.beans.factory.support.BeanDefinitionBuilder\norg.springframework.web.util.UriComponentsBuilder\norg.springframework.http.ResponseEntity.HeadersBuilder\norg.springframework.http.ResponseEntity.BodyBuilder\n\n3.Spring中的Factory Method(工厂方法模式)定义Define an interface for creating an object, but let subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses  即一个创建对象的接口，但是让子类来决定实例化哪个类。工厂方法让一个类的实例化延迟到其子类中进行。\nSpring 中的 ApplicationContext 与 BeanFactory 中的 getBean 都可以视为工厂方法，它隐藏了 bean （产品）的创建过程和具体实现\nSpring 中其它工厂：\n\norg.springframework.beans.factory.FactoryBean\n\n@Bean 标注的静态方法及实例方法\n\nObjectFactory 及 ObjectProvider\n\n\n前两种工厂主要封装第三方的 bean 的创建过程，后两种工厂可以推迟 bean 创建，解决循环依赖及单例注入多例等问题\n4.Spring中的Adapter(适配器模式)定义 Convert the interface of a class into another interface clients expect. Adapter lets classes work together that couldn’t otherwise because of incompatible interfaces 即 将一个类的接口转换为另一个客户端所期望的接口。适配器模式可以让原本由于接口不兼容而无法一起工作的类能够协同工作。\n典型的实现有两处:\n\norg.springframework.web.servlet.HandlerAdapter – 因为控制器实现有各种各样，比如有\n@RequestMapping 标注的控制器实现\n传统的基于 Controller 接口（不是 @Controller注解啊）的实现\n较新的基于 RouterFunction 接口的实现\n它们的处理方法都不一样，为了统一调用，必须适配为 HandlerAdapter 接口\n\n\norg.springframework.beans.factory.support.DisposableBeanAdapter – 因为销毁方法多种多样，因此都要适配为 DisposableBean 来统一调用销毁方法\n\n5.Spring中的Composite(组合模式)定义 Compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly  即将对象组合成树形结构来表示部分-整体层次结构。组合模式可以让客户端统一地对待单个对象和对象组合。\n典型实现有：\n\norg.springframework.web.method.support.HandlerMethodArgumentResolverComposite\norg.springframework.web.method.support.HandlerMethodReturnValueHandlerComposite\norg.springframework.web.servlet.handler.HandlerExceptionResolverComposite\norg.springframework.web.servlet.view.ViewResolverComposite\n\ncomposite 对象的作用是，将分散的调用集中起来，统一调用入口，它的特征是：与具体干活的类实现同一个接口，当调用 composite 对象的接口方法时，其实是委托具体干活的类实现来完成                                                                     \n6.Spring中的Decorator(装饰器模式)定义 Attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality 即 向对象动态地附加职责。 装饰器提供了一种灵活的替代继承的方式，以扩展功能。\n典型实现：\n\norg.springframework.web.util.ContentCachingRequestWrapper\n\nContentCachingRequestWrapper 通过包装 HttpServletRequest 对象，提供了对输入流进行重复读取和缓存的功能。其构造函数需要传入一个原始的 HttpServletRequest 对象，然后使用自己的 ByteArrayOutputStream 缓存请求体，同时提供了多个方法来获取请求信息。例如：\n\ngetContentAsByteArray() : 获取请求的内容（byte 数组），如果请求内容被缓存了，则返回缓存的内容，否则读取并返回原始内容。\ngetContentAsStream() : 返回请求的内容流（ServletInputStream），如果请求内容被缓存了，则返回缓存的内容，否则读取并返回原始内容。\ngetReader() : 返回一个读取请求内容的 BufferedReader 实例，如果请求内容被缓存了，则返回缓存的内容，否则读取并返回原始内容。\n\n总的来说，ContentCachingRequestWrapper 的实现非常好地体现了装饰器模式的思想，通过包装原始请求对象，提供了额外的功能，并且不改变原始对象的行为，从而实现了请求体的缓存和多次读取。\n7.Spring中的Proxy(代理模式)定义 Provide a surrogate or placeholder for another object to control access to it 即，为另一个对象提供一个代理或占位符来控制对它的访问。\n代理模式的主要目的是控制对目标对象的访问，在不改变原有代码的情况下，为对象提供一种间接访问的方式。代理模式在客户端和目标对象之间创建了一个代理对象，客户端通过代理对象来访问目标对象，从而可以对访问进行控制。代理模式可以用于保护目标对象的访问性、缓存对象等。\n与装饰器模式的区别\n装饰器模式注重的是功能增强，避免子类继承方式进行功能扩展，而代理模式更注重控制目标的访问\n典型实现：\n\norg.springframework.aop.framework.JdkDynamicAopProxy\norg.springframework.aop.framework.ObjenesisCglibAopProxy\n\n8.Spring中的Chain of Responsibility(责任链模式)定义 Avoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it 即 避免使用一个对象直接调用另一个对象，而是通过将请求传递给多个对象中的一个来处理请求。将接收对象链接在一起，并将请求沿着链传递，直到一个对象能够处理它。\n典型实现：\n\norg.springframework.web.servlet.HandlerInterceptor\n\nHandlerInterceptor支持责任链模式，通过链式调用多个HandlerInterceptor，可以处理同一个请求的多个这样的拦截器。\n当一个请求到达后端控制器前，可以通过多个HandlerInterceptor按照特定的顺序进行处理。每个HandlerInterceptor可以处理请求，然后将处理结果传递给下一个HandlerInterceptor。这样，就形成了一个责任链。\n在责任链模式中，我们可以动态添加或删除HandlerInterceptor，并且可以定义多个拦截器链。这样，我们可以实现一些复杂的处理逻辑，将请求按照不同的方式进行处理，以达到更好的效果。\n9.Spring 中的 Observer(观察者模式)定义 Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically 即 定义一种一对多的对象依赖关系，这样当一个对象改变状态时，所有其依赖者将自动得到通知并更新。\n典型实现：\n\norg.springframework.context.ApplicationListener\norg.springframework.context.event.ApplicationEventMulticaster\norg.springframework.context.ApplicationEvent\n\n一个事件（ApplicationEvent）可以被ApplicationContext中的一个或多个ApplicationListener所监听。ApplicationContext会在事件发生时通知已注册的ApplicationListener，并由ApplicationListener来负责对该事件做出响应。\n为了实现这种模式，Spring提供了以下几个核心组件：\n\nApplicationEvent：代表了一个应用程序中发生的事件，包含了事件的信息。\nApplicationListener：负责处理事件，并进行响应的组件。\nApplicationEventMulticaster：用于管理事件监听器，负责将事件分发给对应的监听器进行处理。\n\n当一个事件被触发时，ApplicationEventMulticaster会调用所有已注册的ApplicationListener实例的onApplicationEvent()方法来进行处理。当事件被多个ApplicationListener监听时，ApplicationEventMulticaster会按顺序调用它们的onApplicationEvent()方法。\n因此，Spring的事件驱动模型就体现了观察者模式的特征，其中ApplicationListener充当了观察者的角色，ApplicationEventMulticaster充当了被观察者的角色。\n10. Spring 中的 Strategy(策略模式)定义 Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it 即，定义一个算法族，将每个算法封装起来，使它们可以互换。策略模式能够让算法的变化独立于使用它们的客户端。\n典型实现：\n\norg.springframework.beans.factory.support.InstantiationStrategy\norg.springframework.core.annotation.MergedAnnotations.SearchStrategy\norg.springframework.boot.autoconfigure.condition.SearchStrategy\n\norg.springframework.beans.factory.support.InstantiationStrategy：存在多个实现InstantiationStrategy接口的类，这些类都实现了不同的实例化策略，比如：CglibSubclassingInstantiationStrategy、SimpleInstantiationStrategy和SmartInstantiationStrategy等。这些实现类可以根据实际情况取舍，通过策略模式实现了实例化策略的动态切换。\norg.springframework.core.annotation.MergedAnnotations.SearchStrategy：存在多个实现SearchStrategy接口的类，这些类都实现了不同的注解查找策略，比如：TypeMappedAnnotationChecker、RepeatableContainers、DirectlyDeclaredAnnotations和SynthesizedAnnotationDetection等。这些实现类可以根据实际情况取舍，通过策略模式实现了注解查找策略的动态切换。\norg.springframework.boot.autoconfigure.condition.SearchStrategy：存在多个实现SearchStrategy接口的类，这些类都实现了不同的条件判断策略，比如：OnClassCondition、ConditionalOnWebApplication和ConditionalOnProperty等。这些实现类可以根据实际情况取舍，通过策略模式实现了条件判断策略的动态切换。\n11. Spring 中的 Template Method(模板方法)定义 Define the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm’s structure  在一个操作中定义算法的框架，将某些步骤延迟到子类中。模板方法让子类重新定义算法的某些步骤，而不改变算法的结构。\n典型实现：\n\n大部分以 Template 命名的类，如 JdbcTemplate，TransactionTemplate\n很多以 Abstract 命名的类，如 AbstractApplicationContext\n\n在JdbcTemplate中，模板方法是execute()方法，它封装了执行SQL语句的流程，具体的SQL语句可以通过传入不同的参数来实现。execute()方法中包含了一些固定的步骤，如获取连接、创建Statement、执行SQL语句、关闭资源等，这些步骤是不变的，但是具体的SQL语句和参数是可以变化的。通过这种方式，JdbcTemplate将重复的、泛化的操作封装到模板方法中，使得使用者只需要关注具体的SQL语句和参数。\nTransactionTemplate也是类似的，在Spring事务管理中，它封装了执行事务的流程，具体的事务操作可以通过传入不同的参数来实现。TransactionTemplate中的模板方法是execute()方法，它包含了获取事务、执行方法、提交&#x2F;回滚事务等固定步骤，这些步骤是每个事务操作都必须要执行的，但是具体的事务操作可以改变。通过这种方式，TransactionTemplate将事务管理的复杂性封装到模板方法中，使用者只需要关注自己的业务逻辑，而不需要关心事务的管理。\n在AbstractApplicationContext中，模板方法是refresh()方法，该方法包含了Spring容器的初始化流程，具体的初始化过程可以通过继承AbstractApplicationContext的子类来实现。refresh()方法中包含了一些固定的步骤，比如读取配置文件、创建&#x2F;初始化BeanFactory、加载Bean定义、注册BeanPostProcessor等，这些步骤是不变的，但是具体的实现可以改变。子类根据自己的需要，可以通过重写refresh()方法中的一些步骤来实现自己的初始化过程，比如读取不同的配置文件、使用不同的BeanFactory等。\n通过这种方式，AbstractApplicationContext将Spring容器的初始化过程封装到模板方法中，使得使用者只需要关注具体的Bean的配置和使用，而不需要关心Spring容器的初始化过程。这样可以大大简化代码的编写和维护，提高开发效率。\n","slug":"Spring中的设计模式","date":"2023-05-09T04:00:50.000Z","categories_index":"","tags_index":"Java,设计模式,Spring,面试题","author_index":"大宝贝的程序员"},{"id":"5e422a9b2c69bd7294ec71b84cbbb53b","title":"单例模式","content":"什么是单例模式？单例模式的特点是什么？单例模式属于创建型模式，⼀个单例类在任何情况下都只存在⼀个实例，\n构造⽅法必须是私有的、由⾃⼰创建⼀个静态变量存储实例，对外提供⼀\n个静态公有⽅法获取实例。\n优点是内存中只有⼀个实例，减少了开销，尤其是频繁创建和销毁实例的\n情况下并且可以避免对资源的多重占⽤。缺点是没有抽象层，难以扩展，\n与单⼀职责原则冲突。\n单例模式的常⻅写法有哪些？饿汉式顾名思义，类⼀加载就创建对象，这种⽅式⽐较常⽤，但容易产⽣垃圾对象，浪费内存空间。\n\n优点：线程安全，没有加锁，执⾏效率较⾼\n\n缺点：不是懒加载，类加载时就初始化，浪费内存空间\n\n\n\n线程安全：饿汉式单例是如何保证线程安全的呢？它是基于类加载机制避免了多线程\n的同步问题，但是如果类被不同的类加载器加载就会创建不同的实例。\n\n\npublic class Singleton &#123;\n // 1、私有化构造⽅法\n private Singleton()&#123;&#125;\n // 2、定义⼀个静态变量指向⾃⼰类型\n private final static Singleton instance = new\nSingleton();\n // 3、对外提供⼀个公共的⽅法获取实例\n public static Singleton getInstance() &#123;\n return instance;\n &#125;\n&#125;\n\n使⽤反射破坏单例public class Test &#123;\n public static void main(String[] args) throws\nException&#123;\n // 使⽤反射破坏单例\n // 获取空参构造⽅法\n Constructor&lt;Singleton> declaredConstructor =\nSingleton.class.getDeclaredConstructor(null);\n // 设置强制访问\n declaredConstructor.setAccessible(true);\n // 创建实例\n Singleton singleton =\ndeclaredConstructor.newInstance();\n System.out.println(\"反射创建的实例\" + singleton);\n System.out.println(\"正常创建的实例\" +\nSingleton.getInstance());\n System.out.println(\"正常创建的实例\" +\nSingleton.getInstance());\n &#125;\n&#125;\n\n输出结果如下反射创建的实例\ncom.example.spring.demo.single.Singleton@6267c3bb\n正常创建的实例\ncom.example.spring.demo.single.Singleton@533ddba\n正常创建的实例\ncom.example.spring.demo.single.Singleton@533ddba\n\n线程不安全的懒汉式这种⽅式在单线程下使⽤没有问题，对于多线程是⽆法保证单例的，这⾥列出来是为了和后⾯使⽤锁保证线程安全的单例做对⽐\n\n优点：懒加载\n缺点：线程不安全\n\n//线程不安全的懒汉式单例\npublic class Singleton &#123;\n // 1、私有化构造⽅法\n private Singleton()&#123; &#125;\n // 2、定义⼀个静态变量指向⾃⼰类型\n private static Singleton instance;\n // 3、对外提供⼀个公共的⽅法获取实例\n public static Singleton getInstance() &#123;\n // 判断为 null 的时候再创建对象\n if (instance == null) &#123;\n instance = new Singleton();\n &#125;\n return instance;\n &#125;\n&#125;\n\n线程安全的懒汉式懒汉式单例如何保证线程安全呢？通过 synchronized 关键字加锁保证线程\n安全， synchronized 可以添加在⽅法上⾯，也可以添加在代码块上⾯，这\n⾥演示添加在⽅法上⾯，存在的问题是 每⼀次调⽤ getInstance 获取实例时\n都需要加锁和释放锁，这样是⾮常影响性能的。\n\n优点：懒加载，线程安全\n\n缺点：效率较低\n\n\n//懒汉式单例，⽅法上⾯添加 synchronized 保证线程安全\npublic class Singleton &#123;\n // 1、私有化构造⽅法\n private Singleton()&#123; &#125;\n // 2、定义⼀个静态变量指向⾃⼰类型\n private static Singleton instance;\n // 3、对外提供⼀个公共的⽅法获取实例\n public synchronized static Singleton getInstance() &#123;\n if (instance == null) &#123;\n instance = new Singleton();\n &#125;\n return instance;\n    &#125;\n&#125;\n\n双重检查锁(DCL)这⾥的双重检查是指两次⾮空判断，锁指的是 synchronized 加锁，为什么\n要进⾏双重判断，其实很简单，第⼀重判断，如果实例已经存在，那么就\n不再需要进⾏同步操作，⽽是直接返回这个实例，如果没有创建，才会进\n⼊同步块，同步块的⽬的与之前相同，⽬的是为了防⽌有多个线程同时调\n⽤时，导致⽣成多个实例，有了同步块，每次只能有⼀个线程调⽤访问同\n步块内容，当第⼀个抢到锁的调⽤获取了实例之后，这个实例就会被创\n建，之后的所有调⽤都不会进⼊同步块，直接在第⼀重判断就返回了单\n例。\n关于内部的第⼆重空判断的作⽤，当多个线程⼀起到达锁位置时，进⾏锁\n竞争，其中⼀个线程获取锁，如果是第⼀次进⼊则为 null，会进⾏单例对\n象的创建，完成后释放锁，其他线程获取锁后就会被空判断拦截，直接返\n回已创建的单例对象。\n其中最关键的⼀个点就是 volatile 关键字的使⽤，关于 volatile 的详细介\n绍可以直接搜索 volatile 关键字即可，有很多写的⾮常好的⽂章，这⾥不做\n详细介绍，简单说明⼀下，双重检查锁中使⽤ volatile 的两个重要特性：\n可⻅性、禁⽌指令重排序\n当我们在引⽤变量上⾯添加 volatile 关键字以后，会通过在创建对象指令\n的前后添加内存屏障来禁⽌指令重排序，就可以避免这个问题，⽽且对\nvolatile 修饰的变量的修改对其他任何线程都是可⻅的\npublic class Singleton &#123;\n // 1、私有化构造⽅法\n private Singleton() &#123;\n &#125;\n // 2、定义⼀个静态变量指向⾃⼰类型\n private volatile static Singleton instance;\n // 3、对外提供⼀个公共的⽅法获取实例\n public static Singleton getInstance() &#123;\n // 第⼀重检查是否为 null\n if (instance == null) &#123;\n // 使⽤ synchronized 加锁\n synchronized (Singleton.class) &#123;\n // 第⼆重检查是否为 null\n     if (instance == null) &#123;\n // new 关键字创建对象不是原⼦操作\n instance = new Singleton();\n\t \t\t&#125;\n \t\t&#125;\n \t&#125;\n return instance;\n \t&#125;\n&#125;\n\n\n优点：懒加载，线程安全，效率较⾼\n\n缺点：实现较复杂\n\n\n静态内部类//静态内部类实现单例\npublic class Singleton &#123;\n // 1、私有化构造⽅法\n private Singleton() &#123;\n &#125;\n // 2、对外提供获取实例的公共⽅法\n public static Singleton getInstance() &#123;\n return InnerClass.INSTANCE;\n     \n // 定义静态内部类\n private static class InnerClass&#123;\n private final static Singleton INSTANCE = new\nSingleton();\n &#125;\n&#125;\n\n\n优点：懒加载，线程安全，效率较⾼，实现简单\n\n静态内部类单例是如何实现懒加载的呢？⾸先，我们先了解下类的加载时机。虚拟机规范要求有且只有 5 种情况必须⽴即对类进⾏初始化（加载、验证、准备需要在此之前开始）：\n\n遇到 new 、 getstatic 、 putstatic 、 invokestatic 这 4 条字节码指令时。⽣成这 4 条指令最常⻅的 Java 代码场景是：使⽤ new 关键字实例化对象的时候、读取或设置⼀个类的静态字段（final 修饰除外，被final 修饰的静态字段是常量，已在编译期把结果放⼊常量池）的时候，以及调⽤⼀个类的静态⽅法的时候。\n使⽤ java.lang.reflect 包⽅法对类进⾏反射调⽤的时候。\n当初始化⼀个类的时候，如果发现其⽗类还没有进⾏过初始化，则需要先触发其⽗类的初始化。\n当虚拟机启动时，⽤户需要指定⼀个要执⾏的主类（包含 main()的那个类），虚拟机会先初始化这个主类。\n当使⽤ JDK 1.7 的动态语⾔⽀持时，如果⼀个java.lang.invoke.MethodHandle 实例最后的解析结果是REF_getStatic 、 REF_putStatic 、 REF_invokeStatic 的⽅法句柄，则需要先触发这个⽅法句柄所对应的类的初始化。\n\n这 5 种情况被称为是类的主动引⽤，注意，这⾥《虚拟机规范》中使⽤的限定词是 “有且仅有”，那么，除此之外的所有引⽤类都不会对类进⾏初始化，称为被动引⽤。静态内部类就属于被动引⽤的情况。\n当 getInstance()⽅法被调⽤时，InnerClass 才在 Singleton 的运⾏时常量池⾥，把符号引⽤替换为直接引⽤，这时静态对象 INSTANCE 也真正被创建，然后再被 getInstance()⽅法返回出去，这点同饿汉模式。\n那么 INSTANCE 在创建过程中⼜是如何保证线程安全的呢？\n虚拟机会保证⼀个类的 () ⽅法在多线程环境中被正确地加锁、同步，如果多个线程同时去初始化⼀个类，那么只会有⼀个线程去执⾏这个类的 () ⽅法，其他线程都需要阻塞等待，直到活动线程执⾏() ⽅法完毕。如果在⼀个类的 () ⽅法中有耗时很⻓的操作，就可能造成多个进程阻塞(需要注意的是，其他线程虽然会被阻塞，但如果执⾏ () ⽅法后，其他线程唤醒之后不会再次进⼊ () ⽅法。同⼀个加载器下，⼀个类型只会初始化⼀次。)，在实际应⽤中，这种阻塞往往是很隐蔽的。\n所以说静态内部类形式的单例可保证线程安全，也能保证单例的唯⼀性，同时也延迟了单例的实例化。\n枚举单例//枚举实现单例\npublic enum Singleton &#123;\n INSTANCE;\n public void doSomething(String str) &#123;\n System.out.println(str);\n &#125;\n&#125;\n\n\n优点：简单，⾼效，线程安全，可以避免通过反射破坏枚举单例\n\n枚举在 java 中与普通类⼀样，都能拥有字段与⽅法，⽽且枚举实例创建是线程安全的，在任何情况下，它都是⼀个单例，可以直接通过如下⽅式调⽤获取实例：\nSingleton singleton = Singleton.INSTANCE;\n使⽤下⾯的命令反编译枚举类\njavap Singleton.class\n得到如下内容\nCompiled from \"Singleton.java\"\npublic final class com.spring.demo.singleton.Singleton\nextends\njava.lang.Enum&lt;com.spring.demo.singleton.Singleton> &#123;\n public static final\ncom.spring.demo.singleton.Singleton INSTANCE;\n public static com.spring.demo.singleton.Singleton[]\nvalues();\n public static com.spring.demo.singleton.Singleton\nvalueOf(java.lang.String);\n public void doSomething(java.lang.String);\n static &#123;&#125;;\n&#125;\n\n从枚举的反编译结果可以看到，INSTANCE 被 static final 修饰，所以可以通过类名直接调⽤，因为static 类型的属性会在类被加载之后被初始化，当⼀个 Java 类第⼀次被真正使⽤到的时候静态资源被初始化、Java 类的加载和初始化过程都是线程安全的，所以创建⼀个 enum 类型是线程安全的。\n通过反射破坏枚举，实现代码如下：public class Test &#123;\n public static void main(String[] args) throws\nException &#123;\n Singleton singleton = Singleton.INSTANCE;\n singleton.doSomething(\"hello enum\");\n // 尝试使⽤反射破坏单例\n // 枚举类没有空参构造⽅法，反编译后可以看到枚举有⼀个两个\n参数的构造⽅法\n Constructor&lt;Singleton> declaredConstructor =\nSingleton.class.getDeclaredConstructor(String.class,\nint.class);\n // 设置强制访问\n declaredConstructor.setAccessible(true);\n // 创建实例，这⾥会报错，因为⽆法通过反射创建枚举的实例\n Singleton enumSingleton =\ndeclaredConstructor.newInstance();\n System.out.println(enumSingleton);\n &#125;\n&#125;\n\n运⾏结果报如下错误：\nException in thread \"main\"\njava.lang.IllegalArgumentException: Cannot reflectively\ncreate enum objects at\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:492) \nat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\nat com.spring.demo.singleton.Test.main(Test.java:24)\n\n所以⽆法通过反射创建枚举的实例。\n","slug":"单例模式","date":"2023-05-08T13:10:28.000Z","categories_index":"","tags_index":"Java,设计模式","author_index":"大宝贝的程序员"},{"id":"064fd1f961f785b45ddfca5b3ce563ee","title":"spring事务失效的情况以及注意事项","content":"什么是spring事务？​\tSpring Framework提供了一种简单而强大的机制来管理事务，这个机制被称为Spring事务。Spring事务是在Java应用程序中管理数据库事务的一种强大的方法。在Spring事务中，所有的数据库访问是在事务管理器的上下文中进行的。\n​\tSpring事务的核心思想是将事务管理从具体的数据访问代码中抽象出来。Spring提供了一种集中式的方式来管理事务，这使得我们可以在不修改具体数据访问代码的情况下轻松地添加或删除事务。\nspring事务的优点Spring事务具有以下优点：\n简单性 - Spring事务使得事务管理变得非常简单，只需要添加少量的注释就可以实现。\n可扩展性 - Spring事务提供了各种事务管理器，以适应各种不同的事务需求。它还允许我们自行扩展和定制事务管理器。\n独立性 - Spring事务提供了一种与底层数据访问技术无关的事务管理方式，因此我们可以轻松地更改数据库，而不需要更改事务管理方式。\nspring事务的类型Spring事务有以下常见的类型：\n\n声明式事务 - 该类型的事务是在配置文件或注释中声明的。这使得我们可以轻松地添加和删除事务。\n编程式事务 - 该类型的事务是通过编写代码来实现的。它提供了更细粒度的控制和更高的自定义程度。但是，编写代码会使得代码复杂化。\n\nspring事务失效的情况事务失效的八种场景1.抛出检查异常导致事务不能正确回滚补充什么是检查异常：\nJava中的异常分为两种类型：检查异常和非检查异常。\n检查异常需要使用try-catch语句或者throws声明来处理或抛出，否则编译器会报错。这种异常通常表示程序运行时必须解决的某些错误或不合法操作。例如：IOException、SQLException、ClassNotFoundException等。\n非检查异常是指RuntimeException及其子类异常，不需要在代码中进行特殊处理，程序在运行期间出现这种异常时，会自动触发JVM的异常处理机制。这种异常通常表示程序出现了逻辑或编程错误。例如：NullPointerException、ArrayIndexOutOfBoundsException、IllegalArgumentException等。\n@Service\npublic class Service1 &#123;\n\n    @Autowired\n    private AccountMapper accountMapper;\n\n    @Transactional\n    public void transfer(int from, int to, int amount) throws FileNotFoundException &#123;\n        int fromBalance = accountMapper.findBalanceBy(from);\n        if (fromBalance - amount >= 0) &#123;\n            accountMapper.update(from, -1 * amount);\n            new FileInputStream(\"hj\");\n            accountMapper.update(to, amount);\n        &#125;\n    &#125;\n&#125;\n\n\n无法事务回滚：Spring默认只会回滚非检查异常\n\n解法：\n配置rollbackFor属性 (什么错误需要回滚)：\n @Transactional(rollbackFor = Exception.class)\n\n\n2.方法内自己try-catch异常导致事务不能正确回滚@Service\npublic class Service2 &#123;\n\n    @Autowired\n    private AccountMapper accountMapper;\n\n    @Transactional(rollbackFor = Exception.class)\n    public void transfer(int from, int to, int amount)  &#123;\n        try &#123;\n            int fromBalance = accountMapper.findBalanceBy(from);\n            if (fromBalance - amount >= 0) &#123;\n                accountMapper.update(from, -1 * amount);\n                new FileInputStream(\"hj\");\n                accountMapper.update(to, amount);\n            &#125;\n        &#125; catch (FileNotFoundException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n&#125;\n\n\n原因：事务通知只有捕捉到了目标方法抛出的异常，才能进行后续的回滚处理，如果目标自己处理掉异常，事务通知无法知悉\n\n方法1：将catch到的异常，包装成运行时异常抛出\n\n在catch块里throw new RuntimeException(e);\n\n\n方法2：手动设置回滚\n\n在catch块添加\nTransactionInterceptor.currentTransactionStatus().setRollbackOnly();\n\n\n\n\n3.aop切面顺序异常导致事务不能正确回滚@Service\npublic class Service3 &#123;\n\n    @Autowired\n    private AccountMapper accountMapper;\n\n    @Transactional(rollbackFor = Exception.class)\n    public void transfer(int from, int to, int amount) throws FileNotFoundException &#123;\n        int fromBalance = accountMapper.findBalanceBy(from);\n        if (fromBalance - amount >= 0) &#123;\n            accountMapper.update(from, -1 * amount);\n            new FileInputStream(\"hj\");\n            accountMapper.update(to, amount);\n        &#125;\n    &#125;\n&#125;\n\n@Aspect\npublic class MyAspect &#123;\n    @Around(\"execution(* transfer(..))\")\n    public Object around(ProceedingJoinPoint pjp) throws Throwable &#123;\n        LoggerUtils.get().debug(\"log:&#123;&#125;\", pjp.getTarget());\n        try &#123;\n            return pjp.proceed();\n        &#125; catch (Throwable e) &#123;\n            e.printStackTrace();\n            return null;\n        &#125;\n    &#125;\n&#125;\n\n\n\n4.非public方法导致事务失效@Service\npublic class Service4 &#123;\n\n    @Autowired\n    private AccountMapper accountMapper;\n\n    @Transactional\n    void transfer(int from, int to, int amount) throws FileNotFoundException &#123;\n        int fromBalance = accountMapper.findBalanceBy(from);\n        if (fromBalance - amount >= 0) &#123;\n            accountMapper.update(from, -1 * amount);\n            accountMapper.update(to, amount);\n        &#125;\n    &#125;\n&#125;\n\n\n原因：Spring为方法创建代理，添加事务通知，前提条件都是方法为public\n\n方法1：方法改为public\n\n方法2：添加bean配置，不推荐使用\n\n@Bean\npublic TransactionAttributeSource transactionAttributeSource() &#123;\n    return new AnnotationTransactionAttributeSource(false);\n&#125;\n\n\n\n\n\n​\t拓展：AnnotationTransactionAttributeSource\n​\t\tAnnotationTransactionAttributeSource是Spring事务框架中的一个类，用于解析@Transactional注解并以此为基础生成TransactionAttribute对象。TransactionAttribute对象描述了一段方法执行时应该使用的事务属性，包括事务的名字、传播行为、隔离级别、超时时间、只读属性等。\n​\t\t在Spring的事务管理中，事务切面对象会调用AnnotationTransactionAttributeSource对象的getTransactionAttribute()方法，从而获取方法或类级别的事务属性。这种方式可以将事务属性与具体的业务代码解耦，使得业务代码更加简洁可读。\n​\t\tAnnotationTransactionAttributeSource对象有一个布尔类型的构造函数参数，它用于控制@Transactional注解的解析方式。如果这个参数为false，则AnnotationTransactionAttributeSource只会解析@Transactional注解，而不会考虑继承和重载关系，即只解析被注解类或方法上的@Transactional注解，而不考虑其基类和子类以及重载方法中的@Transactional注解。\n​\t\t默认情况下AnnotationTransactionAttributeSource对象的构造函数参数是true，即支持继承和重载的注解解析。但是，在某些特殊情况下，这种解析方式可能会导致一些问题，如事务传播、超时等不被正确解析。此时，可以通过设置构造函数参数为false来禁用这种解析机制，以确保正确地解析@Transactional注解。\n5.父子容器导致的事务失效6.调用本类方法导致传播行为失效7.@Transactional没有保证原子行为8.@Transactional方法导致的synchronized失效","slug":"Spring事务失效的情况以及注意事项","date":"2023-05-07T12:31:38.000Z","categories_index":"","tags_index":"Java,Spring,面试题","author_index":"大宝贝的程序员"}]